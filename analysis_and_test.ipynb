{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d03cf8aa",
      "metadata": {
        "id": "d03cf8aa"
      },
      "source": [
        "# Data\n",
        "\n",
        "The provided dataset contains multiple spots in the histology images of several tissues and the labels of these spots.\n",
        "\n",
        "The code below shows how you can load and use the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9puxY4BWouP",
        "outputId": "2eeaeae6-e6db-41ff-9992-ef2ee6b2000b"
      },
      "id": "J9puxY4BWouP",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/her2_analysis')"
      ],
      "metadata": {
        "id": "OWegJx9CXelc"
      },
      "id": "OWegJx9CXelc",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f1ac9ba9",
      "metadata": {
        "id": "f1ac9ba9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "\n",
        "def load_her2():\n",
        "    X = []\n",
        "    for i in range(7):\n",
        "        X.append(pkl.load(open('data/X%d.pkl' % (i), 'rb')))\n",
        "        y = pkl.load(open('data/y%d.pkl' % (i), 'rb'))\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7813b1c6",
      "metadata": {
        "id": "7813b1c6"
      },
      "outputs": [],
      "source": [
        "X, y = load_her2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fc02c63b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fc02c63b",
        "outputId": "ae4be6db-7f87-44f7-8ffe-efe4ed4b7bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 tissues\n",
            "346 spots in the first tissue\n",
            "Label for the first spot in the first tissue: 0\n",
            "Image for the first spot in the first tissue\n",
            "[[[227 202 224]\n",
            "  [225 199 224]\n",
            "  [225 201 225]\n",
            "  ...\n",
            "  [171 153 195]\n",
            "  [178 160 202]\n",
            "  [186 161 200]]\n",
            "\n",
            " [[227 202 224]\n",
            "  [226 200 225]\n",
            "  [225 200 222]\n",
            "  ...\n",
            "  [180 162 204]\n",
            "  [185 163 201]\n",
            "  [190 168 204]]\n",
            "\n",
            " [[227 202 224]\n",
            "  [225 199 224]\n",
            "  [227 199 224]\n",
            "  ...\n",
            "  [186 160 207]\n",
            "  [192 171 204]\n",
            "  [194 169 199]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[226 200 225]\n",
            "  [225 202 228]\n",
            "  [227 205 228]\n",
            "  ...\n",
            "  [111 112 166]\n",
            "  [117 117 171]\n",
            "  [130 127 184]]\n",
            "\n",
            " [[225 199 224]\n",
            "  [224 198 225]\n",
            "  [225 202 228]\n",
            "  ...\n",
            "  [103 103 157]\n",
            "  [111 111 165]\n",
            "  [122 122 174]]\n",
            "\n",
            " [[227 198 226]\n",
            "  [224 198 225]\n",
            "  [224 199 228]\n",
            "  ...\n",
            "  [110 103 157]\n",
            "  [111 105 153]\n",
            "  [113 115 164]]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(len(X), 'tissues')\n",
        "print(len(X[0]), 'spots in the first tissue')\n",
        "print('Label for the first spot in the first tissue:', y[0][0])\n",
        "print('Image for the first spot in the first tissue')\n",
        "img = np.transpose(X[0][0], [1, 2, 0])\n",
        "plt.imshow(img)\n",
        "print(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a87bff9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "a87bff9d",
        "outputId": "a798203c-658c-4527-a37f-1312f585bab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7bc9e8ddace0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(y[0][1])\n",
        "img = np.transpose(X[0][1], [1, 2, 0])\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "66b7d925",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "66b7d925",
        "outputId": "e29aacc8-efca-4046-b691-ae5703b4fe23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1668, 58, 229, 41, 448, 183, 241] 2868\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7bc9e64a8b80>]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "count=0\n",
        "dist=[0,0,0,0,0,0,0]\n",
        "for i in y:\n",
        "    for j in i:\n",
        "        dist[j]+=1\n",
        "        count+=1\n",
        "count\n",
        "X\n",
        "print(dist,count)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cba7117a",
      "metadata": {
        "id": "cba7117a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f833a246-e07e-4156-dd63-1f3b73b2815d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/her2_analysis')\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization,  Dropout, RandomCrop\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pltn\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "# import Sklearn_PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ec17cfaf",
      "metadata": {
        "id": "ec17cfaf"
      },
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
        "# # dataset): unsupervised feature extraction / dimensionality reduction\n",
        "\n",
        "# # Center data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f8ae394f",
      "metadata": {
        "id": "f8ae394f"
      },
      "outputs": [],
      "source": [
        "# X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "09eba1b8",
      "metadata": {
        "id": "09eba1b8"
      },
      "outputs": [],
      "source": [
        "# !pwd\n",
        "# print(len(X),len(X[0]),len(X[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "45eaf2bc",
      "metadata": {
        "id": "45eaf2bc"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Assuming X is your array of images\n",
        "# os.chdir('/home/Student/s4737925/Nan/her2_analysis/data/images')\n",
        "# # Iterate through each image in X\n",
        "# for i in range(len(X)):\n",
        "#     print(i)\n",
        "#     for j in range(len(X[i])):\n",
        "#         print(i,j)\n",
        "#         img = np.transpose(X[i][j], [1, 2, 0])\n",
        "#         plt.imshow(img)  # Plot the image\n",
        "#         plt.axis('off')\n",
        "#         k=y[i][j]\n",
        "#         os.chdir(f'/home/Student/s4737925/Nan/her2_analysis/data/images/{k}')\n",
        "#         plt.savefig(f'image_{i}_{j}.png')  # Save the plotted image\n",
        "#         plt.close()\n",
        "# os.chdir('/home/Student/s4737925/Nan/her2_analysis')\n",
        "# # Close any open plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1aab1ae7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aab1ae7",
        "outputId": "fb3fe3ec-d877-495c-8796-5ceb651f844f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in '/content/drive/MyDrive/her2_analysis/data/images1/0': 1167\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def count_files(directory):\n",
        "    if os.path.exists(directory) and os.path.isdir(directory):\n",
        "        files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "        num_files = len(files)\n",
        "        return num_files\n",
        "    else:\n",
        "        return \"Directory does not exist or is not a valid directory.\"\n",
        "\n",
        "# Replace '/path/to/directory' with the path of your directory\n",
        "directory_path = '/content/drive/MyDrive/her2_analysis/data/images1/0'\n",
        "num_files = count_files(directory_path)\n",
        "print(f\"Number of files in '{directory_path}': {num_files}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# import random\n",
        "# from math import ceil\n",
        "\n",
        "# # Source directory\n",
        "# source_dir = '/content/drive/MyDrive/her2_analysis/data/images1/'\n",
        "\n",
        "# # Destination directory for test data\n",
        "# test_dir = '/content/drive/MyDrive/her2_analysis/data/test1/'\n",
        "\n",
        "# # Function to move a percentage of files from source to destination\n",
        "# def move_percentage_files(src, dst, percentage):\n",
        "#     files = os.listdir(src)\n",
        "#     num_files_to_move = ceil(len(files) * percentage)\n",
        "#     files_to_move = random.sample(files, num_files_to_move)\n",
        "#     for file in files_to_move:\n",
        "#         src_file = os.path.join(src, file)\n",
        "#         dst_file = os.path.join(dst, file)\n",
        "#         shutil.move(src_file, dst_file)\n",
        "\n",
        "# # Iterate through subdirectories\n",
        "# for subdir in os.listdir(source_dir):\n",
        "#     subdir_path = os.path.join(source_dir, subdir)\n",
        "#     if os.path.isdir(subdir_path):\n",
        "#         test_subdir = os.path.join(test_dir, subdir)\n",
        "#         os.makedirs(test_subdir, exist_ok=True)\n",
        "#         move_percentage_files(subdir_path, test_subdir, 0.3)\n"
      ],
      "metadata": {
        "id": "SHdvbk1O7--f"
      },
      "id": "SHdvbk1O7--f",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "299e77cc",
      "metadata": {
        "id": "299e77cc"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d9965c60",
      "metadata": {
        "id": "d9965c60"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import autocast\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Warning CUDA not Found. Using CPU\")\n",
        "\n",
        "# High learning rate\n",
        "# High test batch size\n",
        "# Low Epoch - 67703 (34) - N\n",
        "# Batch Size - 67702 (96) - N\n",
        "# Weight decay - 67703 (4e-4) - O\n",
        "# Weight decay - 67787 (6e-4) - O\n",
        "\n",
        "# num_epochs = 36\n",
        "# learning_rate = 0.06\n",
        "num_classes=7\n",
        "num_epochs = 200\n",
        "learning_rate = 0.06\n",
        "size=256\n",
        "\n",
        "\n",
        "# transform_train = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010)),transforms.RandomHorizontalFlip(),transforms.RandomCrop(32, padding=4,padding_mode='reflect')])\n",
        "\n",
        "# transform_test = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))])\n",
        "\n",
        "\n",
        "# trainset=torchvision.datasets.CIFAR10(root=\"/\", train=True, download=True, transform=transform_train)\n",
        "# train_loader=torch.utils.data.DataLoader(trainset,batch_size=128,shuffle=True, num_workers=3, pin_memory=True)\n",
        "\n",
        "# testset=torchvision.datasets.CIFAR10(root=\"/\", train=False, download=True, transform=transform_test)\n",
        "# test_loader=torch.utils.data.DataLoader(testset,batch_size=256,shuffle=False, num_workers=3, pin_memory=True)\n",
        "\n",
        "# Define transformations to apply to the images\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((size, size)),  # Resize images to a consistent size  ############################\n",
        "    transforms.ToTensor()      # Convert images to PyTorch tensors\n",
        "    ,transforms.RandomHorizontalFlip(),transforms.RandomCrop(size,padding_mode='reflect')])\n",
        "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((size, size)),  # Resize images to a consistent size  ############################\n",
        "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
        "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Path to your dataset containing subdirectories for each class\n",
        "data_dir = '/content/drive/MyDrive/her2_analysis/data/images1/'\n",
        "data_dir_test = '/content/drive/MyDrive/her2_analysis/data/test1/'\n",
        "# Load the dataset using ImageFolder and apply transformations\n",
        "dataset_train = datasets.ImageFolder(root=data_dir, transform=transform_train)\n",
        "dataset_test = datasets.ImageFolder(root=data_dir_test, transform=transform)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "total_len = len(dataset_test)\n",
        "#train_len = int(0.7 * total_len)  # 70% for training\n",
        "valid_len = int(0.5 * total_len)  # 15% for validation\n",
        "test_len = total_len - valid_len  # Remaining for testing\n",
        "\n",
        "valid_set, test_set = torch.utils.data.random_split(dataset_test, [valid_len, test_len])\n",
        "train_set=dataset_train\n",
        "# train_set, test_set = torch.utils.data.random_split(\n",
        "#     dataset, [train_len, test_len])\n",
        "\n",
        "# Create data loaders for training, validation, and test sets\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=32)\n",
        "test_loader = DataLoader(test_set, batch_size=32, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_loader),len(dataset_train),len(test_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvCrD9YmbemF",
        "outputId": "76e483d5-40f4-4c6a-82d2-4581bd275f2b"
      },
      "id": "xvCrD9YmbemF",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63 2004 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class BasicBlock(nn.Module):\n",
        "#     expansion = 1\n",
        "\n",
        "#     def __init__(self, in_planes, planes, stride=1):\n",
        "#         super(BasicBlock, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "#         self.bn1 = nn.BatchNorm2d(planes)\n",
        "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "#         self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "#         self.shortcut = nn.Sequential()\n",
        "#         if stride != 1 or in_planes != self.expansion*planes:\n",
        "#             self.shortcut = nn.Sequential(\n",
        "#                 nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "#                 nn.BatchNorm2d(self.expansion*planes)\n",
        "#             )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         ## Forward Propagation is the way to move from the Input layer (left) to the Output layer (right) in the neural network.\n",
        "#         ## The process of moving from the right to left i.e backward from the Output to the Input layer is called the Backward Propagation.\n",
        "#         out = F.relu(self.bn1(self.conv1(x)))\n",
        "#         out = self.bn2(self.conv2(out))\n",
        "#         out += self.shortcut(x)\n",
        "#         out = F.relu(out)\n",
        "#         return out\n",
        "\n",
        "\n",
        "# class ResNet(nn.Module):\n",
        "#     def __init__(self, block, num_blocks, num_classes=7):\n",
        "#         super(ResNet, self).__init__()\n",
        "#         self.in_planes = 64\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "#         self.bn1 = nn.BatchNorm2d(64) ## Normalization helps get data within a range and reduces the skewness which helps learn faster and better.\n",
        "#         ## Normalization can also tackle the diminishing and exploding gradients problems.\n",
        "#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "#         self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "#         self.linear = nn.Linear(512*block.expansion, num_classes) ## Applies a linear transformation to the incoming data: y = x*W^T + b\n",
        "\n",
        "#     def _make_layer(self, block, planes, num_blocks, stride):\n",
        "#         strides = [stride] + [1]*(num_blocks-1)\n",
        "#         layers = []\n",
        "#         for stride in strides:\n",
        "#             layers.append(block(self.in_planes, planes, stride))\n",
        "#             self.in_planes = planes * block.expansion\n",
        "#         return nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = F.relu(self.bn1(self.conv1(x)))\n",
        "#         out = self.layer1(out)\n",
        "#         out = self.layer2(out)\n",
        "#         out = self.layer3(out)\n",
        "#         out = self.layer4(out)\n",
        "#         out = F.avg_pool2d(out, 4)\n",
        "#         out = out.view(out.size(0), -1)\n",
        "#         out = self.linear(out)\n",
        "#         return out\n",
        "\n",
        "# def Resnet18():\n",
        "#     return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "# model = Resnet18()\n",
        "# model=model.to(device)\n",
        "\n",
        "# #model info\n",
        "# print(\"Model No. of Parameters:\", sum([param.nelement() for param in model.parameters()]))\n",
        "# print(model)\n"
      ],
      "metadata": {
        "id": "YmtmbM-4Yl70"
      },
      "id": "YmtmbM-4Yl70",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=7):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, size//8)  # Adjust pooling based on the new input size (64x64)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def Resnet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "model = Resnet18()\n",
        "model = model.to(device)\n",
        "\n",
        "# model info\n",
        "print(\"Model No. of Parameters:\", sum([param.nelement() for param in model.parameters()]))\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lka9oxSk9GKT",
        "outputId": "a571c82f-ff11-4be9-ab84-f427e2b1932d"
      },
      "id": "lka9oxSk9GKT",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model No. of Parameters: 11172423\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=512, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # Define the neural network model\n",
        "# class SimpleNet(nn.Module):\n",
        "#     def __init__(self, num_classes):\n",
        "#         super(SimpleNet, self).__init__()\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         self.fc1 = nn.Linear(786432,32)  # Input size: 512x512, Output size: 128\n",
        "#         self.fc2 = nn.Linear(32, num_classes)  # Output size: num_classes (for classification)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.flatten(x)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return F.softmax(x, dim=1)  # Softmax activation for classification\n",
        "\n",
        "# # Instantiate the model\n",
        "# num_classes = 7  # Replace this with the actual number of classes\n",
        "# model = SimpleNet(num_classes)\n",
        "\n",
        "# # Print the model architecture\n",
        "# model=model.to(device)\n",
        "# print(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "R9THTgYydo4B"
      },
      "id": "R9THTgYydo4B",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f231ff9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f231ff9a",
        "outputId": "cdde84ec-c3a1-4178-adba-dcd2bef4fd1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [49/200], Step [63/63] Loss: 0.65650\n",
            "Valid Accuracy: 59.9537037037037 %\n",
            "Epoch [50/200], Step [1/63] Loss: 0.83803\n",
            "Epoch [50/200], Step [2/63] Loss: 0.73425\n",
            "Epoch [50/200], Step [3/63] Loss: 0.79752\n",
            "Epoch [50/200], Step [4/63] Loss: 0.56100\n",
            "Epoch [50/200], Step [5/63] Loss: 0.64292\n",
            "Epoch [50/200], Step [6/63] Loss: 0.72693\n",
            "Epoch [50/200], Step [7/63] Loss: 0.87911\n",
            "Epoch [50/200], Step [8/63] Loss: 0.53134\n",
            "Epoch [50/200], Step [9/63] Loss: 0.71768\n",
            "Epoch [50/200], Step [10/63] Loss: 0.91788\n",
            "Epoch [50/200], Step [11/63] Loss: 0.65660\n",
            "Epoch [50/200], Step [12/63] Loss: 1.36565\n",
            "Epoch [50/200], Step [13/63] Loss: 0.83509\n",
            "Epoch [50/200], Step [14/63] Loss: 0.72309\n",
            "Epoch [50/200], Step [15/63] Loss: 0.65283\n",
            "Epoch [50/200], Step [16/63] Loss: 0.64244\n",
            "Epoch [50/200], Step [17/63] Loss: 0.69419\n",
            "Epoch [50/200], Step [18/63] Loss: 0.78636\n",
            "Epoch [50/200], Step [19/63] Loss: 0.88811\n",
            "Epoch [50/200], Step [20/63] Loss: 1.00285\n",
            "Epoch [50/200], Step [21/63] Loss: 0.69927\n",
            "Epoch [50/200], Step [22/63] Loss: 0.71940\n",
            "Epoch [50/200], Step [23/63] Loss: 0.99443\n",
            "Epoch [50/200], Step [24/63] Loss: 0.50335\n",
            "Epoch [50/200], Step [25/63] Loss: 1.07615\n",
            "Epoch [50/200], Step [26/63] Loss: 0.55700\n",
            "Epoch [50/200], Step [27/63] Loss: 0.78780\n",
            "Epoch [50/200], Step [28/63] Loss: 0.64865\n",
            "Epoch [50/200], Step [29/63] Loss: 0.81891\n",
            "Epoch [50/200], Step [30/63] Loss: 0.74956\n",
            "Epoch [50/200], Step [31/63] Loss: 0.71517\n",
            "Epoch [50/200], Step [32/63] Loss: 0.62172\n",
            "Epoch [50/200], Step [33/63] Loss: 0.86591\n",
            "Epoch [50/200], Step [34/63] Loss: 0.78117\n",
            "Epoch [50/200], Step [35/63] Loss: 0.68379\n",
            "Epoch [50/200], Step [36/63] Loss: 0.64192\n",
            "Epoch [50/200], Step [37/63] Loss: 0.94202\n",
            "Epoch [50/200], Step [38/63] Loss: 0.88621\n",
            "Epoch [50/200], Step [39/63] Loss: 0.40668\n",
            "Epoch [50/200], Step [40/63] Loss: 0.61886\n",
            "Epoch [50/200], Step [41/63] Loss: 0.58931\n",
            "Epoch [50/200], Step [42/63] Loss: 0.83368\n",
            "Epoch [50/200], Step [43/63] Loss: 0.89276\n",
            "Epoch [50/200], Step [44/63] Loss: 0.53232\n",
            "Epoch [50/200], Step [45/63] Loss: 0.96931\n",
            "Epoch [50/200], Step [46/63] Loss: 1.00086\n",
            "Epoch [50/200], Step [47/63] Loss: 0.88695\n",
            "Epoch [50/200], Step [48/63] Loss: 0.70447\n",
            "Epoch [50/200], Step [49/63] Loss: 0.46199\n",
            "Epoch [50/200], Step [50/63] Loss: 0.52715\n",
            "Epoch [50/200], Step [51/63] Loss: 0.84911\n",
            "Epoch [50/200], Step [52/63] Loss: 0.73665\n",
            "Epoch [50/200], Step [53/63] Loss: 0.67332\n",
            "Epoch [50/200], Step [54/63] Loss: 0.58435\n",
            "Epoch [50/200], Step [55/63] Loss: 0.76081\n",
            "Epoch [50/200], Step [56/63] Loss: 0.55346\n",
            "Epoch [50/200], Step [57/63] Loss: 0.94380\n",
            "Epoch [50/200], Step [58/63] Loss: 0.82553\n",
            "Epoch [50/200], Step [59/63] Loss: 0.66185\n",
            "Epoch [50/200], Step [60/63] Loss: 0.95198\n",
            "Epoch [50/200], Step [61/63] Loss: 0.72445\n",
            "Epoch [50/200], Step [62/63] Loss: 0.44911\n",
            "Epoch [50/200], Step [63/63] Loss: 1.15565\n",
            "Valid Accuracy: 66.89814814814815 %\n",
            "Epoch [51/200], Step [1/63] Loss: 0.73263\n",
            "Epoch [51/200], Step [2/63] Loss: 0.94076\n",
            "Epoch [51/200], Step [3/63] Loss: 0.78111\n",
            "Epoch [51/200], Step [4/63] Loss: 0.73206\n",
            "Epoch [51/200], Step [5/63] Loss: 0.84149\n",
            "Epoch [51/200], Step [6/63] Loss: 0.66684\n",
            "Epoch [51/200], Step [7/63] Loss: 0.59197\n",
            "Epoch [51/200], Step [8/63] Loss: 0.59249\n",
            "Epoch [51/200], Step [9/63] Loss: 0.82482\n",
            "Epoch [51/200], Step [10/63] Loss: 0.59819\n",
            "Epoch [51/200], Step [11/63] Loss: 0.79166\n",
            "Epoch [51/200], Step [12/63] Loss: 0.39623\n",
            "Epoch [51/200], Step [13/63] Loss: 0.68253\n",
            "Epoch [51/200], Step [14/63] Loss: 0.38517\n",
            "Epoch [51/200], Step [15/63] Loss: 0.79566\n",
            "Epoch [51/200], Step [16/63] Loss: 1.02537\n",
            "Epoch [51/200], Step [17/63] Loss: 0.95031\n",
            "Epoch [51/200], Step [18/63] Loss: 1.07468\n",
            "Epoch [51/200], Step [19/63] Loss: 0.64703\n",
            "Epoch [51/200], Step [20/63] Loss: 1.07740\n",
            "Epoch [51/200], Step [21/63] Loss: 0.78229\n",
            "Epoch [51/200], Step [22/63] Loss: 0.46674\n",
            "Epoch [51/200], Step [23/63] Loss: 0.52037\n",
            "Epoch [51/200], Step [24/63] Loss: 0.84530\n",
            "Epoch [51/200], Step [25/63] Loss: 0.70915\n",
            "Epoch [51/200], Step [26/63] Loss: 0.79726\n",
            "Epoch [51/200], Step [27/63] Loss: 0.79171\n",
            "Epoch [51/200], Step [28/63] Loss: 0.64266\n",
            "Epoch [51/200], Step [29/63] Loss: 0.77497\n",
            "Epoch [51/200], Step [30/63] Loss: 0.88692\n",
            "Epoch [51/200], Step [31/63] Loss: 0.75937\n",
            "Epoch [51/200], Step [32/63] Loss: 0.62977\n",
            "Epoch [51/200], Step [33/63] Loss: 0.97478\n",
            "Epoch [51/200], Step [34/63] Loss: 0.70356\n",
            "Epoch [51/200], Step [35/63] Loss: 0.77713\n",
            "Epoch [51/200], Step [36/63] Loss: 0.68913\n",
            "Epoch [51/200], Step [37/63] Loss: 0.49067\n",
            "Epoch [51/200], Step [38/63] Loss: 0.81186\n",
            "Epoch [51/200], Step [39/63] Loss: 0.97993\n",
            "Epoch [51/200], Step [40/63] Loss: 0.33686\n",
            "Epoch [51/200], Step [41/63] Loss: 0.86184\n",
            "Epoch [51/200], Step [42/63] Loss: 0.78665\n",
            "Epoch [51/200], Step [43/63] Loss: 0.71287\n",
            "Epoch [51/200], Step [44/63] Loss: 0.90842\n",
            "Epoch [51/200], Step [45/63] Loss: 0.66370\n",
            "Epoch [51/200], Step [46/63] Loss: 0.65009\n",
            "Epoch [51/200], Step [47/63] Loss: 0.59397\n",
            "Epoch [51/200], Step [48/63] Loss: 0.88261\n",
            "Epoch [51/200], Step [49/63] Loss: 0.80768\n",
            "Epoch [51/200], Step [50/63] Loss: 0.54238\n",
            "Epoch [51/200], Step [51/63] Loss: 0.61841\n",
            "Epoch [51/200], Step [52/63] Loss: 0.62908\n",
            "Epoch [51/200], Step [53/63] Loss: 0.76683\n",
            "Epoch [51/200], Step [54/63] Loss: 1.16416\n",
            "Epoch [51/200], Step [55/63] Loss: 0.90801\n",
            "Epoch [51/200], Step [56/63] Loss: 0.72990\n",
            "Epoch [51/200], Step [57/63] Loss: 0.65829\n",
            "Epoch [51/200], Step [58/63] Loss: 0.74442\n",
            "Epoch [51/200], Step [59/63] Loss: 0.47381\n",
            "Epoch [51/200], Step [60/63] Loss: 0.71819\n",
            "Epoch [51/200], Step [61/63] Loss: 0.58237\n",
            "Epoch [51/200], Step [62/63] Loss: 0.43593\n",
            "Epoch [51/200], Step [63/63] Loss: 1.45356\n",
            "Valid Accuracy: 63.19444444444444 %\n",
            "Epoch [52/200], Step [1/63] Loss: 0.66896\n",
            "Epoch [52/200], Step [2/63] Loss: 0.50856\n",
            "Epoch [52/200], Step [3/63] Loss: 0.62871\n",
            "Epoch [52/200], Step [4/63] Loss: 0.87688\n",
            "Epoch [52/200], Step [5/63] Loss: 0.93879\n",
            "Epoch [52/200], Step [6/63] Loss: 0.49791\n",
            "Epoch [52/200], Step [7/63] Loss: 0.67984\n",
            "Epoch [52/200], Step [8/63] Loss: 0.55746\n",
            "Epoch [52/200], Step [9/63] Loss: 0.86510\n",
            "Epoch [52/200], Step [10/63] Loss: 0.70331\n",
            "Epoch [52/200], Step [11/63] Loss: 0.99229\n",
            "Epoch [52/200], Step [12/63] Loss: 0.70019\n",
            "Epoch [52/200], Step [13/63] Loss: 1.00376\n",
            "Epoch [52/200], Step [14/63] Loss: 0.64575\n",
            "Epoch [52/200], Step [15/63] Loss: 0.69669\n",
            "Epoch [52/200], Step [16/63] Loss: 0.72273\n",
            "Epoch [52/200], Step [17/63] Loss: 0.52567\n",
            "Epoch [52/200], Step [18/63] Loss: 0.84616\n",
            "Epoch [52/200], Step [19/63] Loss: 0.97724\n",
            "Epoch [52/200], Step [20/63] Loss: 0.68048\n",
            "Epoch [52/200], Step [21/63] Loss: 0.93826\n",
            "Epoch [52/200], Step [22/63] Loss: 0.81800\n",
            "Epoch [52/200], Step [23/63] Loss: 0.93503\n",
            "Epoch [52/200], Step [24/63] Loss: 0.85726\n",
            "Epoch [52/200], Step [25/63] Loss: 0.66623\n",
            "Epoch [52/200], Step [26/63] Loss: 0.70955\n",
            "Epoch [52/200], Step [27/63] Loss: 0.65177\n",
            "Epoch [52/200], Step [28/63] Loss: 0.69348\n",
            "Epoch [52/200], Step [29/63] Loss: 0.59889\n",
            "Epoch [52/200], Step [30/63] Loss: 0.84302\n",
            "Epoch [52/200], Step [31/63] Loss: 0.66297\n",
            "Epoch [52/200], Step [32/63] Loss: 0.75562\n",
            "Epoch [52/200], Step [33/63] Loss: 0.56349\n",
            "Epoch [52/200], Step [34/63] Loss: 0.64559\n",
            "Epoch [52/200], Step [35/63] Loss: 0.94978\n",
            "Epoch [52/200], Step [36/63] Loss: 0.72100\n",
            "Epoch [52/200], Step [37/63] Loss: 0.40272\n",
            "Epoch [52/200], Step [38/63] Loss: 0.83891\n",
            "Epoch [52/200], Step [39/63] Loss: 0.44838\n",
            "Epoch [52/200], Step [40/63] Loss: 0.69697\n",
            "Epoch [52/200], Step [41/63] Loss: 0.61523\n",
            "Epoch [52/200], Step [42/63] Loss: 0.62252\n",
            "Epoch [52/200], Step [43/63] Loss: 0.87046\n",
            "Epoch [52/200], Step [44/63] Loss: 0.95223\n",
            "Epoch [52/200], Step [45/63] Loss: 1.07506\n",
            "Epoch [52/200], Step [46/63] Loss: 0.70545\n",
            "Epoch [52/200], Step [47/63] Loss: 1.13670\n",
            "Epoch [52/200], Step [48/63] Loss: 0.83780\n",
            "Epoch [52/200], Step [49/63] Loss: 0.67052\n",
            "Epoch [52/200], Step [50/63] Loss: 0.72980\n",
            "Epoch [52/200], Step [51/63] Loss: 0.75422\n",
            "Epoch [52/200], Step [52/63] Loss: 0.76807\n",
            "Epoch [52/200], Step [53/63] Loss: 1.10120\n",
            "Epoch [52/200], Step [54/63] Loss: 0.85439\n",
            "Epoch [52/200], Step [55/63] Loss: 0.92257\n",
            "Epoch [52/200], Step [56/63] Loss: 0.66015\n",
            "Epoch [52/200], Step [57/63] Loss: 0.83357\n",
            "Epoch [52/200], Step [58/63] Loss: 1.03540\n",
            "Epoch [52/200], Step [59/63] Loss: 0.78781\n",
            "Epoch [52/200], Step [60/63] Loss: 0.90009\n",
            "Epoch [52/200], Step [61/63] Loss: 0.94280\n",
            "Epoch [52/200], Step [62/63] Loss: 0.55009\n",
            "Epoch [52/200], Step [63/63] Loss: 0.51785\n",
            "Valid Accuracy: 59.25925925925926 %\n",
            "Epoch [53/200], Step [1/63] Loss: 0.72431\n",
            "Epoch [53/200], Step [2/63] Loss: 0.95648\n",
            "Epoch [53/200], Step [3/63] Loss: 0.71041\n",
            "Epoch [53/200], Step [4/63] Loss: 0.65521\n",
            "Epoch [53/200], Step [5/63] Loss: 0.71408\n",
            "Epoch [53/200], Step [6/63] Loss: 0.80598\n",
            "Epoch [53/200], Step [7/63] Loss: 0.67511\n",
            "Epoch [53/200], Step [8/63] Loss: 0.59642\n",
            "Epoch [53/200], Step [9/63] Loss: 0.55453\n",
            "Epoch [53/200], Step [10/63] Loss: 0.55910\n",
            "Epoch [53/200], Step [11/63] Loss: 0.56546\n",
            "Epoch [53/200], Step [12/63] Loss: 0.60307\n",
            "Epoch [53/200], Step [13/63] Loss: 0.74891\n",
            "Epoch [53/200], Step [14/63] Loss: 0.62018\n",
            "Epoch [53/200], Step [15/63] Loss: 0.93697\n",
            "Epoch [53/200], Step [16/63] Loss: 0.96505\n",
            "Epoch [53/200], Step [17/63] Loss: 0.58874\n",
            "Epoch [53/200], Step [18/63] Loss: 0.53509\n",
            "Epoch [53/200], Step [19/63] Loss: 1.20794\n",
            "Epoch [53/200], Step [20/63] Loss: 0.55538\n",
            "Epoch [53/200], Step [21/63] Loss: 0.63772\n",
            "Epoch [53/200], Step [22/63] Loss: 0.77191\n",
            "Epoch [53/200], Step [23/63] Loss: 0.92401\n",
            "Epoch [53/200], Step [24/63] Loss: 0.59432\n",
            "Epoch [53/200], Step [25/63] Loss: 0.62145\n",
            "Epoch [53/200], Step [26/63] Loss: 0.84749\n",
            "Epoch [53/200], Step [27/63] Loss: 0.63314\n",
            "Epoch [53/200], Step [28/63] Loss: 0.89538\n",
            "Epoch [53/200], Step [29/63] Loss: 0.83643\n",
            "Epoch [53/200], Step [30/63] Loss: 0.65453\n",
            "Epoch [53/200], Step [31/63] Loss: 0.76579\n",
            "Epoch [53/200], Step [32/63] Loss: 0.74260\n",
            "Epoch [53/200], Step [33/63] Loss: 0.69885\n",
            "Epoch [53/200], Step [34/63] Loss: 0.87731\n",
            "Epoch [53/200], Step [35/63] Loss: 0.62553\n",
            "Epoch [53/200], Step [36/63] Loss: 0.82230\n",
            "Epoch [53/200], Step [37/63] Loss: 0.60811\n",
            "Epoch [53/200], Step [38/63] Loss: 0.61743\n",
            "Epoch [53/200], Step [39/63] Loss: 0.92748\n",
            "Epoch [53/200], Step [40/63] Loss: 0.92158\n",
            "Epoch [53/200], Step [41/63] Loss: 0.75994\n",
            "Epoch [53/200], Step [42/63] Loss: 0.80136\n",
            "Epoch [53/200], Step [43/63] Loss: 1.07637\n",
            "Epoch [53/200], Step [44/63] Loss: 0.88952\n",
            "Epoch [53/200], Step [45/63] Loss: 0.83259\n",
            "Epoch [53/200], Step [46/63] Loss: 0.74608\n",
            "Epoch [53/200], Step [47/63] Loss: 0.80741\n",
            "Epoch [53/200], Step [48/63] Loss: 0.99688\n",
            "Epoch [53/200], Step [49/63] Loss: 0.96567\n",
            "Epoch [53/200], Step [50/63] Loss: 0.73670\n",
            "Epoch [53/200], Step [51/63] Loss: 0.65687\n",
            "Epoch [53/200], Step [52/63] Loss: 0.81444\n",
            "Epoch [53/200], Step [53/63] Loss: 0.31445\n",
            "Epoch [53/200], Step [54/63] Loss: 0.95184\n",
            "Epoch [53/200], Step [55/63] Loss: 0.75224\n",
            "Epoch [53/200], Step [56/63] Loss: 1.28945\n",
            "Epoch [53/200], Step [57/63] Loss: 0.55418\n",
            "Epoch [53/200], Step [58/63] Loss: 0.55070\n",
            "Epoch [53/200], Step [59/63] Loss: 0.51902\n",
            "Epoch [53/200], Step [60/63] Loss: 0.59148\n",
            "Epoch [53/200], Step [61/63] Loss: 0.60628\n",
            "Epoch [53/200], Step [62/63] Loss: 1.07096\n",
            "Epoch [53/200], Step [63/63] Loss: 0.61520\n",
            "Valid Accuracy: 68.51851851851852 %\n",
            "Epoch [54/200], Step [1/63] Loss: 0.68535\n",
            "Epoch [54/200], Step [2/63] Loss: 0.90064\n",
            "Epoch [54/200], Step [3/63] Loss: 0.57082\n",
            "Epoch [54/200], Step [4/63] Loss: 0.52700\n",
            "Epoch [54/200], Step [5/63] Loss: 0.65344\n",
            "Epoch [54/200], Step [6/63] Loss: 0.54352\n",
            "Epoch [54/200], Step [7/63] Loss: 0.56195\n",
            "Epoch [54/200], Step [8/63] Loss: 0.77901\n",
            "Epoch [54/200], Step [9/63] Loss: 1.10490\n",
            "Epoch [54/200], Step [10/63] Loss: 0.67448\n",
            "Epoch [54/200], Step [11/63] Loss: 1.49042\n",
            "Epoch [54/200], Step [12/63] Loss: 0.74847\n",
            "Epoch [54/200], Step [13/63] Loss: 0.85318\n",
            "Epoch [54/200], Step [14/63] Loss: 0.58208\n",
            "Epoch [54/200], Step [15/63] Loss: 0.68550\n",
            "Epoch [54/200], Step [16/63] Loss: 0.80814\n",
            "Epoch [54/200], Step [17/63] Loss: 0.61918\n",
            "Epoch [54/200], Step [18/63] Loss: 0.91036\n",
            "Epoch [54/200], Step [19/63] Loss: 0.89721\n",
            "Epoch [54/200], Step [20/63] Loss: 0.56775\n",
            "Epoch [54/200], Step [21/63] Loss: 0.72460\n",
            "Epoch [54/200], Step [22/63] Loss: 0.96928\n",
            "Epoch [54/200], Step [23/63] Loss: 0.71095\n",
            "Epoch [54/200], Step [24/63] Loss: 0.34690\n",
            "Epoch [54/200], Step [25/63] Loss: 0.74647\n",
            "Epoch [54/200], Step [26/63] Loss: 1.06763\n",
            "Epoch [54/200], Step [27/63] Loss: 1.01550\n",
            "Epoch [54/200], Step [28/63] Loss: 0.77936\n",
            "Epoch [54/200], Step [29/63] Loss: 0.91134\n",
            "Epoch [54/200], Step [30/63] Loss: 0.80177\n",
            "Epoch [54/200], Step [31/63] Loss: 0.84331\n",
            "Epoch [54/200], Step [32/63] Loss: 1.14500\n",
            "Epoch [54/200], Step [33/63] Loss: 0.97719\n",
            "Epoch [54/200], Step [34/63] Loss: 0.71440\n",
            "Epoch [54/200], Step [35/63] Loss: 0.64509\n",
            "Epoch [54/200], Step [36/63] Loss: 0.62478\n",
            "Epoch [54/200], Step [37/63] Loss: 0.61489\n",
            "Epoch [54/200], Step [38/63] Loss: 0.69933\n",
            "Epoch [54/200], Step [39/63] Loss: 0.56673\n",
            "Epoch [54/200], Step [40/63] Loss: 0.53035\n",
            "Epoch [54/200], Step [41/63] Loss: 1.15912\n",
            "Epoch [54/200], Step [42/63] Loss: 0.94051\n",
            "Epoch [54/200], Step [43/63] Loss: 0.69754\n",
            "Epoch [54/200], Step [44/63] Loss: 0.53166\n",
            "Epoch [54/200], Step [45/63] Loss: 0.66491\n",
            "Epoch [54/200], Step [46/63] Loss: 0.71289\n",
            "Epoch [54/200], Step [47/63] Loss: 0.73789\n",
            "Epoch [54/200], Step [48/63] Loss: 0.75492\n",
            "Epoch [54/200], Step [49/63] Loss: 0.69420\n",
            "Epoch [54/200], Step [50/63] Loss: 0.64315\n",
            "Epoch [54/200], Step [51/63] Loss: 0.73881\n",
            "Epoch [54/200], Step [52/63] Loss: 0.52651\n",
            "Epoch [54/200], Step [53/63] Loss: 0.63791\n",
            "Epoch [54/200], Step [54/63] Loss: 0.63832\n",
            "Epoch [54/200], Step [55/63] Loss: 0.86879\n",
            "Epoch [54/200], Step [56/63] Loss: 0.50994\n",
            "Epoch [54/200], Step [57/63] Loss: 0.62244\n",
            "Epoch [54/200], Step [58/63] Loss: 0.91757\n",
            "Epoch [54/200], Step [59/63] Loss: 0.53442\n",
            "Epoch [54/200], Step [60/63] Loss: 0.49687\n",
            "Epoch [54/200], Step [61/63] Loss: 0.76622\n",
            "Epoch [54/200], Step [62/63] Loss: 0.70074\n",
            "Epoch [54/200], Step [63/63] Loss: 0.48238\n",
            "Valid Accuracy: 65.04629629629629 %\n",
            "Epoch [55/200], Step [1/63] Loss: 0.55334\n",
            "Epoch [55/200], Step [2/63] Loss: 0.95823\n",
            "Epoch [55/200], Step [3/63] Loss: 0.95001\n",
            "Epoch [55/200], Step [4/63] Loss: 1.06334\n",
            "Epoch [55/200], Step [5/63] Loss: 0.94154\n",
            "Epoch [55/200], Step [6/63] Loss: 1.12083\n",
            "Epoch [55/200], Step [7/63] Loss: 0.64967\n",
            "Epoch [55/200], Step [8/63] Loss: 0.61173\n",
            "Epoch [55/200], Step [9/63] Loss: 0.79735\n",
            "Epoch [55/200], Step [10/63] Loss: 0.56389\n",
            "Epoch [55/200], Step [11/63] Loss: 0.48661\n",
            "Epoch [55/200], Step [12/63] Loss: 0.80852\n",
            "Epoch [55/200], Step [13/63] Loss: 0.75706\n",
            "Epoch [55/200], Step [14/63] Loss: 0.52149\n",
            "Epoch [55/200], Step [15/63] Loss: 0.53972\n",
            "Epoch [55/200], Step [16/63] Loss: 1.03323\n",
            "Epoch [55/200], Step [17/63] Loss: 0.57943\n",
            "Epoch [55/200], Step [18/63] Loss: 0.78697\n",
            "Epoch [55/200], Step [19/63] Loss: 0.74882\n",
            "Epoch [55/200], Step [20/63] Loss: 0.37622\n",
            "Epoch [55/200], Step [21/63] Loss: 0.79920\n",
            "Epoch [55/200], Step [22/63] Loss: 0.79649\n",
            "Epoch [55/200], Step [23/63] Loss: 0.68439\n",
            "Epoch [55/200], Step [24/63] Loss: 0.62372\n",
            "Epoch [55/200], Step [25/63] Loss: 0.67458\n",
            "Epoch [55/200], Step [26/63] Loss: 1.14026\n",
            "Epoch [55/200], Step [27/63] Loss: 0.77003\n",
            "Epoch [55/200], Step [28/63] Loss: 0.70715\n",
            "Epoch [55/200], Step [29/63] Loss: 0.98601\n",
            "Epoch [55/200], Step [30/63] Loss: 0.80465\n",
            "Epoch [55/200], Step [31/63] Loss: 0.78931\n",
            "Epoch [55/200], Step [32/63] Loss: 0.88277\n",
            "Epoch [55/200], Step [33/63] Loss: 0.74573\n",
            "Epoch [55/200], Step [34/63] Loss: 0.78828\n",
            "Epoch [55/200], Step [35/63] Loss: 0.85640\n",
            "Epoch [55/200], Step [36/63] Loss: 0.95869\n",
            "Epoch [55/200], Step [37/63] Loss: 0.38279\n",
            "Epoch [55/200], Step [38/63] Loss: 0.51451\n",
            "Epoch [55/200], Step [39/63] Loss: 0.73356\n",
            "Epoch [55/200], Step [40/63] Loss: 0.61639\n",
            "Epoch [55/200], Step [41/63] Loss: 0.60826\n",
            "Epoch [55/200], Step [42/63] Loss: 0.80037\n",
            "Epoch [55/200], Step [43/63] Loss: 0.73458\n",
            "Epoch [55/200], Step [44/63] Loss: 0.64878\n",
            "Epoch [55/200], Step [45/63] Loss: 0.56852\n",
            "Epoch [55/200], Step [46/63] Loss: 0.54086\n",
            "Epoch [55/200], Step [47/63] Loss: 0.76316\n",
            "Epoch [55/200], Step [48/63] Loss: 0.49484\n",
            "Epoch [55/200], Step [49/63] Loss: 0.62584\n",
            "Epoch [55/200], Step [50/63] Loss: 0.67033\n",
            "Epoch [55/200], Step [51/63] Loss: 0.50008\n",
            "Epoch [55/200], Step [52/63] Loss: 0.76908\n",
            "Epoch [55/200], Step [53/63] Loss: 0.92530\n",
            "Epoch [55/200], Step [54/63] Loss: 1.10025\n",
            "Epoch [55/200], Step [55/63] Loss: 0.68388\n",
            "Epoch [55/200], Step [56/63] Loss: 0.64279\n",
            "Epoch [55/200], Step [57/63] Loss: 0.82659\n",
            "Epoch [55/200], Step [58/63] Loss: 0.63224\n",
            "Epoch [55/200], Step [59/63] Loss: 0.87381\n",
            "Epoch [55/200], Step [60/63] Loss: 0.90318\n",
            "Epoch [55/200], Step [61/63] Loss: 0.61736\n",
            "Epoch [55/200], Step [62/63] Loss: 0.76589\n",
            "Epoch [55/200], Step [63/63] Loss: 0.55742\n",
            "Valid Accuracy: 63.657407407407405 %\n",
            "Epoch [56/200], Step [1/63] Loss: 0.70455\n",
            "Epoch [56/200], Step [2/63] Loss: 0.63080\n",
            "Epoch [56/200], Step [3/63] Loss: 0.81713\n",
            "Epoch [56/200], Step [4/63] Loss: 0.83286\n",
            "Epoch [56/200], Step [5/63] Loss: 1.27140\n",
            "Epoch [56/200], Step [6/63] Loss: 0.79900\n",
            "Epoch [56/200], Step [7/63] Loss: 0.73163\n",
            "Epoch [56/200], Step [8/63] Loss: 0.88616\n",
            "Epoch [56/200], Step [9/63] Loss: 0.62815\n",
            "Epoch [56/200], Step [10/63] Loss: 0.60652\n",
            "Epoch [56/200], Step [11/63] Loss: 0.51345\n",
            "Epoch [56/200], Step [12/63] Loss: 0.89614\n",
            "Epoch [56/200], Step [13/63] Loss: 0.78451\n",
            "Epoch [56/200], Step [14/63] Loss: 0.49902\n",
            "Epoch [56/200], Step [15/63] Loss: 0.83411\n",
            "Epoch [56/200], Step [16/63] Loss: 0.48693\n",
            "Epoch [56/200], Step [17/63] Loss: 0.80205\n",
            "Epoch [56/200], Step [18/63] Loss: 0.59923\n",
            "Epoch [56/200], Step [19/63] Loss: 0.69145\n",
            "Epoch [56/200], Step [20/63] Loss: 0.72843\n",
            "Epoch [56/200], Step [21/63] Loss: 0.72502\n",
            "Epoch [56/200], Step [22/63] Loss: 0.88496\n",
            "Epoch [56/200], Step [23/63] Loss: 0.71845\n",
            "Epoch [56/200], Step [24/63] Loss: 0.64219\n",
            "Epoch [56/200], Step [25/63] Loss: 0.90944\n",
            "Epoch [56/200], Step [26/63] Loss: 0.62631\n",
            "Epoch [56/200], Step [27/63] Loss: 0.83840\n",
            "Epoch [56/200], Step [28/63] Loss: 0.72745\n",
            "Epoch [56/200], Step [29/63] Loss: 0.95950\n",
            "Epoch [56/200], Step [30/63] Loss: 0.91501\n",
            "Epoch [56/200], Step [31/63] Loss: 0.78231\n",
            "Epoch [56/200], Step [32/63] Loss: 0.75866\n",
            "Epoch [56/200], Step [33/63] Loss: 0.75153\n",
            "Epoch [56/200], Step [34/63] Loss: 0.56380\n",
            "Epoch [56/200], Step [35/63] Loss: 0.82763\n",
            "Epoch [56/200], Step [36/63] Loss: 0.66580\n",
            "Epoch [56/200], Step [37/63] Loss: 0.66591\n",
            "Epoch [56/200], Step [38/63] Loss: 0.61809\n",
            "Epoch [56/200], Step [39/63] Loss: 0.83697\n",
            "Epoch [56/200], Step [40/63] Loss: 0.50169\n",
            "Epoch [56/200], Step [41/63] Loss: 1.21664\n",
            "Epoch [56/200], Step [42/63] Loss: 0.66340\n",
            "Epoch [56/200], Step [43/63] Loss: 0.90615\n",
            "Epoch [56/200], Step [44/63] Loss: 1.04768\n",
            "Epoch [56/200], Step [45/63] Loss: 0.59715\n",
            "Epoch [56/200], Step [46/63] Loss: 0.77371\n",
            "Epoch [56/200], Step [47/63] Loss: 0.57016\n",
            "Epoch [56/200], Step [48/63] Loss: 0.56946\n",
            "Epoch [56/200], Step [49/63] Loss: 0.94335\n",
            "Epoch [56/200], Step [50/63] Loss: 0.71156\n",
            "Epoch [56/200], Step [51/63] Loss: 0.74123\n",
            "Epoch [56/200], Step [52/63] Loss: 0.63187\n",
            "Epoch [56/200], Step [53/63] Loss: 0.59015\n",
            "Epoch [56/200], Step [54/63] Loss: 0.61845\n",
            "Epoch [56/200], Step [55/63] Loss: 0.51695\n",
            "Epoch [56/200], Step [56/63] Loss: 0.72658\n",
            "Epoch [56/200], Step [57/63] Loss: 0.53263\n",
            "Epoch [56/200], Step [58/63] Loss: 0.55836\n",
            "Epoch [56/200], Step [59/63] Loss: 0.66026\n",
            "Epoch [56/200], Step [60/63] Loss: 0.73819\n",
            "Epoch [56/200], Step [61/63] Loss: 0.81055\n",
            "Epoch [56/200], Step [62/63] Loss: 0.79383\n",
            "Epoch [56/200], Step [63/63] Loss: 0.70707\n",
            "Valid Accuracy: 68.51851851851852 %\n",
            "Epoch [57/200], Step [1/63] Loss: 0.57075\n",
            "Epoch [57/200], Step [2/63] Loss: 0.69315\n",
            "Epoch [57/200], Step [3/63] Loss: 0.63757\n",
            "Epoch [57/200], Step [4/63] Loss: 0.61595\n",
            "Epoch [57/200], Step [5/63] Loss: 0.63126\n",
            "Epoch [57/200], Step [6/63] Loss: 0.59732\n",
            "Epoch [57/200], Step [7/63] Loss: 0.69630\n",
            "Epoch [57/200], Step [8/63] Loss: 0.60318\n",
            "Epoch [57/200], Step [9/63] Loss: 0.77998\n",
            "Epoch [57/200], Step [10/63] Loss: 0.52266\n",
            "Epoch [57/200], Step [11/63] Loss: 0.76723\n",
            "Epoch [57/200], Step [12/63] Loss: 0.50109\n",
            "Epoch [57/200], Step [13/63] Loss: 0.66913\n",
            "Epoch [57/200], Step [14/63] Loss: 0.92319\n",
            "Epoch [57/200], Step [15/63] Loss: 0.53740\n",
            "Epoch [57/200], Step [16/63] Loss: 0.74501\n",
            "Epoch [57/200], Step [17/63] Loss: 0.55161\n",
            "Epoch [57/200], Step [18/63] Loss: 0.82972\n",
            "Epoch [57/200], Step [19/63] Loss: 0.62690\n",
            "Epoch [57/200], Step [20/63] Loss: 0.61790\n",
            "Epoch [57/200], Step [21/63] Loss: 0.67415\n",
            "Epoch [57/200], Step [22/63] Loss: 0.55704\n",
            "Epoch [57/200], Step [23/63] Loss: 0.66779\n",
            "Epoch [57/200], Step [24/63] Loss: 0.68605\n",
            "Epoch [57/200], Step [25/63] Loss: 1.09954\n",
            "Epoch [57/200], Step [26/63] Loss: 1.04562\n",
            "Epoch [57/200], Step [27/63] Loss: 0.84889\n",
            "Epoch [57/200], Step [28/63] Loss: 0.55138\n",
            "Epoch [57/200], Step [29/63] Loss: 0.68241\n",
            "Epoch [57/200], Step [30/63] Loss: 0.52570\n",
            "Epoch [57/200], Step [31/63] Loss: 0.75122\n",
            "Epoch [57/200], Step [32/63] Loss: 1.09288\n",
            "Epoch [57/200], Step [33/63] Loss: 0.69295\n",
            "Epoch [57/200], Step [34/63] Loss: 0.68815\n",
            "Epoch [57/200], Step [35/63] Loss: 0.70561\n",
            "Epoch [57/200], Step [36/63] Loss: 0.83835\n",
            "Epoch [57/200], Step [37/63] Loss: 0.77635\n",
            "Epoch [57/200], Step [38/63] Loss: 0.48926\n",
            "Epoch [57/200], Step [39/63] Loss: 0.94430\n",
            "Epoch [57/200], Step [40/63] Loss: 0.73676\n",
            "Epoch [57/200], Step [41/63] Loss: 0.63492\n",
            "Epoch [57/200], Step [42/63] Loss: 0.55603\n",
            "Epoch [57/200], Step [43/63] Loss: 0.60165\n",
            "Epoch [57/200], Step [44/63] Loss: 0.60464\n",
            "Epoch [57/200], Step [45/63] Loss: 0.42761\n",
            "Epoch [57/200], Step [46/63] Loss: 0.89221\n",
            "Epoch [57/200], Step [47/63] Loss: 0.71858\n",
            "Epoch [57/200], Step [48/63] Loss: 1.05904\n",
            "Epoch [57/200], Step [49/63] Loss: 0.72001\n",
            "Epoch [57/200], Step [50/63] Loss: 0.81602\n",
            "Epoch [57/200], Step [51/63] Loss: 0.89371\n",
            "Epoch [57/200], Step [52/63] Loss: 0.55345\n",
            "Epoch [57/200], Step [53/63] Loss: 0.64743\n",
            "Epoch [57/200], Step [54/63] Loss: 0.89731\n",
            "Epoch [57/200], Step [55/63] Loss: 0.75946\n",
            "Epoch [57/200], Step [56/63] Loss: 0.78298\n",
            "Epoch [57/200], Step [57/63] Loss: 0.76075\n",
            "Epoch [57/200], Step [58/63] Loss: 0.59351\n",
            "Epoch [57/200], Step [59/63] Loss: 0.62904\n",
            "Epoch [57/200], Step [60/63] Loss: 0.77432\n",
            "Epoch [57/200], Step [61/63] Loss: 1.13980\n",
            "Epoch [57/200], Step [62/63] Loss: 0.74175\n",
            "Epoch [57/200], Step [63/63] Loss: 1.15710\n",
            "Valid Accuracy: 68.28703703703704 %\n",
            "Epoch [58/200], Step [1/63] Loss: 0.72669\n",
            "Epoch [58/200], Step [2/63] Loss: 0.60223\n",
            "Epoch [58/200], Step [3/63] Loss: 0.75878\n",
            "Epoch [58/200], Step [4/63] Loss: 0.71745\n",
            "Epoch [58/200], Step [5/63] Loss: 0.59387\n",
            "Epoch [58/200], Step [6/63] Loss: 0.72240\n",
            "Epoch [58/200], Step [7/63] Loss: 0.65819\n",
            "Epoch [58/200], Step [8/63] Loss: 0.75201\n",
            "Epoch [58/200], Step [9/63] Loss: 0.69057\n",
            "Epoch [58/200], Step [10/63] Loss: 0.93482\n",
            "Epoch [58/200], Step [11/63] Loss: 0.77133\n",
            "Epoch [58/200], Step [12/63] Loss: 0.73759\n",
            "Epoch [58/200], Step [13/63] Loss: 0.73470\n",
            "Epoch [58/200], Step [14/63] Loss: 0.56954\n",
            "Epoch [58/200], Step [15/63] Loss: 0.81108\n",
            "Epoch [58/200], Step [16/63] Loss: 0.73074\n",
            "Epoch [58/200], Step [17/63] Loss: 0.72821\n",
            "Epoch [58/200], Step [18/63] Loss: 0.65177\n",
            "Epoch [58/200], Step [19/63] Loss: 0.85580\n",
            "Epoch [58/200], Step [20/63] Loss: 1.03664\n",
            "Epoch [58/200], Step [21/63] Loss: 0.53662\n",
            "Epoch [58/200], Step [22/63] Loss: 0.47194\n",
            "Epoch [58/200], Step [23/63] Loss: 0.80391\n",
            "Epoch [58/200], Step [24/63] Loss: 0.63018\n",
            "Epoch [58/200], Step [25/63] Loss: 0.72461\n",
            "Epoch [58/200], Step [26/63] Loss: 0.73348\n",
            "Epoch [58/200], Step [27/63] Loss: 0.84779\n",
            "Epoch [58/200], Step [28/63] Loss: 0.79821\n",
            "Epoch [58/200], Step [29/63] Loss: 0.53135\n",
            "Epoch [58/200], Step [30/63] Loss: 0.69919\n",
            "Epoch [58/200], Step [31/63] Loss: 0.78401\n",
            "Epoch [58/200], Step [32/63] Loss: 0.89797\n",
            "Epoch [58/200], Step [33/63] Loss: 0.80051\n",
            "Epoch [58/200], Step [34/63] Loss: 0.96585\n",
            "Epoch [58/200], Step [35/63] Loss: 0.59584\n",
            "Epoch [58/200], Step [36/63] Loss: 0.81933\n",
            "Epoch [58/200], Step [37/63] Loss: 0.76235\n",
            "Epoch [58/200], Step [38/63] Loss: 0.81952\n",
            "Epoch [58/200], Step [39/63] Loss: 0.90258\n",
            "Epoch [58/200], Step [40/63] Loss: 0.66700\n",
            "Epoch [58/200], Step [41/63] Loss: 0.69840\n",
            "Epoch [58/200], Step [42/63] Loss: 0.95626\n",
            "Epoch [58/200], Step [43/63] Loss: 0.46163\n",
            "Epoch [58/200], Step [44/63] Loss: 0.98136\n",
            "Epoch [58/200], Step [45/63] Loss: 0.54683\n",
            "Epoch [58/200], Step [46/63] Loss: 0.60746\n",
            "Epoch [58/200], Step [47/63] Loss: 0.47548\n",
            "Epoch [58/200], Step [48/63] Loss: 0.85220\n",
            "Epoch [58/200], Step [49/63] Loss: 0.82126\n",
            "Epoch [58/200], Step [50/63] Loss: 0.66291\n",
            "Epoch [58/200], Step [51/63] Loss: 0.55167\n",
            "Epoch [58/200], Step [52/63] Loss: 0.77096\n",
            "Epoch [58/200], Step [53/63] Loss: 0.79624\n",
            "Epoch [58/200], Step [54/63] Loss: 0.53972\n",
            "Epoch [58/200], Step [55/63] Loss: 0.66522\n",
            "Epoch [58/200], Step [56/63] Loss: 0.72418\n",
            "Epoch [58/200], Step [57/63] Loss: 1.28611\n",
            "Epoch [58/200], Step [58/63] Loss: 0.57075\n",
            "Epoch [58/200], Step [59/63] Loss: 0.69283\n",
            "Epoch [58/200], Step [60/63] Loss: 0.88039\n",
            "Epoch [58/200], Step [61/63] Loss: 0.97825\n",
            "Epoch [58/200], Step [62/63] Loss: 0.56268\n",
            "Epoch [58/200], Step [63/63] Loss: 0.71372\n",
            "Valid Accuracy: 67.5925925925926 %\n",
            "Epoch [59/200], Step [1/63] Loss: 0.62242\n",
            "Epoch [59/200], Step [2/63] Loss: 0.67320\n",
            "Epoch [59/200], Step [3/63] Loss: 0.74185\n",
            "Epoch [59/200], Step [4/63] Loss: 0.86096\n",
            "Epoch [59/200], Step [5/63] Loss: 0.86664\n",
            "Epoch [59/200], Step [6/63] Loss: 0.79916\n",
            "Epoch [59/200], Step [7/63] Loss: 0.78022\n",
            "Epoch [59/200], Step [8/63] Loss: 0.64140\n",
            "Epoch [59/200], Step [9/63] Loss: 0.68279\n",
            "Epoch [59/200], Step [10/63] Loss: 0.54145\n",
            "Epoch [59/200], Step [11/63] Loss: 0.67237\n",
            "Epoch [59/200], Step [12/63] Loss: 0.55372\n",
            "Epoch [59/200], Step [13/63] Loss: 0.69835\n",
            "Epoch [59/200], Step [14/63] Loss: 0.58745\n",
            "Epoch [59/200], Step [15/63] Loss: 1.02149\n",
            "Epoch [59/200], Step [16/63] Loss: 0.77501\n",
            "Epoch [59/200], Step [17/63] Loss: 0.64213\n",
            "Epoch [59/200], Step [18/63] Loss: 1.33624\n",
            "Epoch [59/200], Step [19/63] Loss: 0.69718\n",
            "Epoch [59/200], Step [20/63] Loss: 0.81982\n",
            "Epoch [59/200], Step [21/63] Loss: 0.97664\n",
            "Epoch [59/200], Step [22/63] Loss: 0.78353\n",
            "Epoch [59/200], Step [23/63] Loss: 0.89444\n",
            "Epoch [59/200], Step [24/63] Loss: 0.76492\n",
            "Epoch [59/200], Step [25/63] Loss: 0.64905\n",
            "Epoch [59/200], Step [26/63] Loss: 0.78378\n",
            "Epoch [59/200], Step [27/63] Loss: 0.57874\n",
            "Epoch [59/200], Step [28/63] Loss: 0.64359\n",
            "Epoch [59/200], Step [29/63] Loss: 0.70193\n",
            "Epoch [59/200], Step [30/63] Loss: 0.54193\n",
            "Epoch [59/200], Step [31/63] Loss: 0.79319\n",
            "Epoch [59/200], Step [32/63] Loss: 0.77009\n",
            "Epoch [59/200], Step [33/63] Loss: 0.52006\n",
            "Epoch [59/200], Step [34/63] Loss: 0.58132\n",
            "Epoch [59/200], Step [35/63] Loss: 0.61812\n",
            "Epoch [59/200], Step [36/63] Loss: 0.88351\n",
            "Epoch [59/200], Step [37/63] Loss: 0.57455\n",
            "Epoch [59/200], Step [38/63] Loss: 0.51568\n",
            "Epoch [59/200], Step [39/63] Loss: 0.95500\n",
            "Epoch [59/200], Step [40/63] Loss: 0.98506\n",
            "Epoch [59/200], Step [41/63] Loss: 0.98803\n",
            "Epoch [59/200], Step [42/63] Loss: 0.57625\n",
            "Epoch [59/200], Step [43/63] Loss: 0.66340\n",
            "Epoch [59/200], Step [44/63] Loss: 0.86009\n",
            "Epoch [59/200], Step [45/63] Loss: 0.75930\n",
            "Epoch [59/200], Step [46/63] Loss: 0.49120\n",
            "Epoch [59/200], Step [47/63] Loss: 1.04920\n",
            "Epoch [59/200], Step [48/63] Loss: 0.58480\n",
            "Epoch [59/200], Step [49/63] Loss: 0.72673\n",
            "Epoch [59/200], Step [50/63] Loss: 0.79317\n",
            "Epoch [59/200], Step [51/63] Loss: 0.66133\n",
            "Epoch [59/200], Step [52/63] Loss: 0.67753\n",
            "Epoch [59/200], Step [53/63] Loss: 0.84295\n",
            "Epoch [59/200], Step [54/63] Loss: 0.64968\n",
            "Epoch [59/200], Step [55/63] Loss: 0.37437\n",
            "Epoch [59/200], Step [56/63] Loss: 0.62648\n",
            "Epoch [59/200], Step [57/63] Loss: 0.74856\n",
            "Epoch [59/200], Step [58/63] Loss: 0.68450\n",
            "Epoch [59/200], Step [59/63] Loss: 0.89988\n",
            "Epoch [59/200], Step [60/63] Loss: 0.90064\n",
            "Epoch [59/200], Step [61/63] Loss: 0.83081\n",
            "Epoch [59/200], Step [62/63] Loss: 0.79246\n",
            "Epoch [59/200], Step [63/63] Loss: 0.71578\n",
            "Valid Accuracy: 52.31481481481482 %\n",
            "Epoch [60/200], Step [1/63] Loss: 0.92131\n",
            "Epoch [60/200], Step [2/63] Loss: 0.62414\n",
            "Epoch [60/200], Step [3/63] Loss: 0.61585\n",
            "Epoch [60/200], Step [4/63] Loss: 0.83839\n",
            "Epoch [60/200], Step [5/63] Loss: 0.64755\n",
            "Epoch [60/200], Step [6/63] Loss: 0.63169\n",
            "Epoch [60/200], Step [7/63] Loss: 0.72825\n",
            "Epoch [60/200], Step [8/63] Loss: 0.76511\n",
            "Epoch [60/200], Step [9/63] Loss: 0.88031\n",
            "Epoch [60/200], Step [10/63] Loss: 0.74680\n",
            "Epoch [60/200], Step [11/63] Loss: 0.51373\n",
            "Epoch [60/200], Step [12/63] Loss: 0.69128\n",
            "Epoch [60/200], Step [13/63] Loss: 0.73058\n",
            "Epoch [60/200], Step [14/63] Loss: 0.73113\n",
            "Epoch [60/200], Step [15/63] Loss: 0.56256\n",
            "Epoch [60/200], Step [16/63] Loss: 0.59199\n",
            "Epoch [60/200], Step [17/63] Loss: 1.02960\n",
            "Epoch [60/200], Step [18/63] Loss: 0.70946\n",
            "Epoch [60/200], Step [19/63] Loss: 0.86579\n",
            "Epoch [60/200], Step [20/63] Loss: 0.58798\n",
            "Epoch [60/200], Step [21/63] Loss: 0.45266\n",
            "Epoch [60/200], Step [22/63] Loss: 0.63801\n",
            "Epoch [60/200], Step [23/63] Loss: 0.68497\n",
            "Epoch [60/200], Step [24/63] Loss: 0.61911\n",
            "Epoch [60/200], Step [25/63] Loss: 0.82886\n",
            "Epoch [60/200], Step [26/63] Loss: 0.79405\n",
            "Epoch [60/200], Step [27/63] Loss: 0.62584\n",
            "Epoch [60/200], Step [28/63] Loss: 0.63482\n",
            "Epoch [60/200], Step [29/63] Loss: 1.13276\n",
            "Epoch [60/200], Step [30/63] Loss: 0.72622\n",
            "Epoch [60/200], Step [31/63] Loss: 0.94517\n",
            "Epoch [60/200], Step [32/63] Loss: 0.96067\n",
            "Epoch [60/200], Step [33/63] Loss: 0.87341\n",
            "Epoch [60/200], Step [34/63] Loss: 0.65058\n",
            "Epoch [60/200], Step [35/63] Loss: 0.50890\n",
            "Epoch [60/200], Step [36/63] Loss: 0.94926\n",
            "Epoch [60/200], Step [37/63] Loss: 0.57546\n",
            "Epoch [60/200], Step [38/63] Loss: 0.77413\n",
            "Epoch [60/200], Step [39/63] Loss: 0.67833\n",
            "Epoch [60/200], Step [40/63] Loss: 0.63384\n",
            "Epoch [60/200], Step [41/63] Loss: 0.55384\n",
            "Epoch [60/200], Step [42/63] Loss: 0.88220\n",
            "Epoch [60/200], Step [43/63] Loss: 0.84534\n",
            "Epoch [60/200], Step [44/63] Loss: 0.81260\n",
            "Epoch [60/200], Step [45/63] Loss: 0.69274\n",
            "Epoch [60/200], Step [46/63] Loss: 0.74130\n",
            "Epoch [60/200], Step [47/63] Loss: 0.63422\n",
            "Epoch [60/200], Step [48/63] Loss: 1.44280\n",
            "Epoch [60/200], Step [49/63] Loss: 0.88605\n",
            "Epoch [60/200], Step [50/63] Loss: 0.65429\n",
            "Epoch [60/200], Step [51/63] Loss: 1.04535\n",
            "Epoch [60/200], Step [52/63] Loss: 0.76939\n",
            "Epoch [60/200], Step [53/63] Loss: 0.58313\n",
            "Epoch [60/200], Step [54/63] Loss: 0.70421\n",
            "Epoch [60/200], Step [55/63] Loss: 0.65990\n",
            "Epoch [60/200], Step [56/63] Loss: 0.49530\n",
            "Epoch [60/200], Step [57/63] Loss: 0.57279\n",
            "Epoch [60/200], Step [58/63] Loss: 0.69959\n",
            "Epoch [60/200], Step [59/63] Loss: 0.50618\n",
            "Epoch [60/200], Step [60/63] Loss: 0.68938\n",
            "Epoch [60/200], Step [61/63] Loss: 0.79228\n",
            "Epoch [60/200], Step [62/63] Loss: 0.83160\n",
            "Epoch [60/200], Step [63/63] Loss: 0.56026\n",
            "Valid Accuracy: 68.05555555555556 %\n",
            "Epoch [61/200], Step [1/63] Loss: 0.60577\n",
            "Epoch [61/200], Step [2/63] Loss: 0.60974\n",
            "Epoch [61/200], Step [3/63] Loss: 0.73151\n",
            "Epoch [61/200], Step [4/63] Loss: 0.77265\n",
            "Epoch [61/200], Step [5/63] Loss: 0.69723\n",
            "Epoch [61/200], Step [6/63] Loss: 0.52669\n",
            "Epoch [61/200], Step [7/63] Loss: 0.57396\n",
            "Epoch [61/200], Step [8/63] Loss: 0.66523\n",
            "Epoch [61/200], Step [9/63] Loss: 0.58139\n",
            "Epoch [61/200], Step [10/63] Loss: 0.54894\n",
            "Epoch [61/200], Step [11/63] Loss: 0.44771\n",
            "Epoch [61/200], Step [12/63] Loss: 0.80754\n",
            "Epoch [61/200], Step [13/63] Loss: 0.44221\n",
            "Epoch [61/200], Step [14/63] Loss: 0.88911\n",
            "Epoch [61/200], Step [15/63] Loss: 0.54821\n",
            "Epoch [61/200], Step [16/63] Loss: 0.63170\n",
            "Epoch [61/200], Step [17/63] Loss: 0.62894\n",
            "Epoch [61/200], Step [18/63] Loss: 0.60346\n",
            "Epoch [61/200], Step [19/63] Loss: 0.96524\n",
            "Epoch [61/200], Step [20/63] Loss: 0.67294\n",
            "Epoch [61/200], Step [21/63] Loss: 0.65041\n",
            "Epoch [61/200], Step [22/63] Loss: 0.63130\n",
            "Epoch [61/200], Step [23/63] Loss: 0.76024\n",
            "Epoch [61/200], Step [24/63] Loss: 0.79981\n",
            "Epoch [61/200], Step [25/63] Loss: 1.02717\n",
            "Epoch [61/200], Step [26/63] Loss: 0.78196\n",
            "Epoch [61/200], Step [27/63] Loss: 0.68209\n",
            "Epoch [61/200], Step [28/63] Loss: 0.59878\n",
            "Epoch [61/200], Step [29/63] Loss: 0.83678\n",
            "Epoch [61/200], Step [30/63] Loss: 0.84328\n",
            "Epoch [61/200], Step [31/63] Loss: 1.05524\n",
            "Epoch [61/200], Step [32/63] Loss: 0.72845\n",
            "Epoch [61/200], Step [33/63] Loss: 0.76820\n",
            "Epoch [61/200], Step [34/63] Loss: 0.79166\n",
            "Epoch [61/200], Step [35/63] Loss: 0.51862\n",
            "Epoch [61/200], Step [36/63] Loss: 0.87868\n",
            "Epoch [61/200], Step [37/63] Loss: 0.75608\n",
            "Epoch [61/200], Step [38/63] Loss: 0.49648\n",
            "Epoch [61/200], Step [39/63] Loss: 0.39735\n",
            "Epoch [61/200], Step [40/63] Loss: 0.48384\n",
            "Epoch [61/200], Step [41/63] Loss: 1.45477\n",
            "Epoch [61/200], Step [42/63] Loss: 0.36748\n",
            "Epoch [61/200], Step [43/63] Loss: 0.63785\n",
            "Epoch [61/200], Step [44/63] Loss: 0.59134\n",
            "Epoch [61/200], Step [45/63] Loss: 0.78606\n",
            "Epoch [61/200], Step [46/63] Loss: 1.12857\n",
            "Epoch [61/200], Step [47/63] Loss: 0.75674\n",
            "Epoch [61/200], Step [48/63] Loss: 0.57204\n",
            "Epoch [61/200], Step [49/63] Loss: 0.92419\n",
            "Epoch [61/200], Step [50/63] Loss: 0.70226\n",
            "Epoch [61/200], Step [51/63] Loss: 0.45093\n",
            "Epoch [61/200], Step [52/63] Loss: 0.76996\n",
            "Epoch [61/200], Step [53/63] Loss: 0.82814\n",
            "Epoch [61/200], Step [54/63] Loss: 1.02066\n",
            "Epoch [61/200], Step [55/63] Loss: 0.73770\n",
            "Epoch [61/200], Step [56/63] Loss: 0.80986\n",
            "Epoch [61/200], Step [57/63] Loss: 0.68098\n",
            "Epoch [61/200], Step [58/63] Loss: 0.59132\n",
            "Epoch [61/200], Step [59/63] Loss: 0.63194\n",
            "Epoch [61/200], Step [60/63] Loss: 0.70051\n",
            "Epoch [61/200], Step [61/63] Loss: 1.13011\n",
            "Epoch [61/200], Step [62/63] Loss: 0.72624\n",
            "Epoch [61/200], Step [63/63] Loss: 0.99765\n",
            "Valid Accuracy: 63.19444444444444 %\n",
            "Epoch [62/200], Step [1/63] Loss: 0.36995\n",
            "Epoch [62/200], Step [2/63] Loss: 0.59476\n",
            "Epoch [62/200], Step [3/63] Loss: 1.01614\n",
            "Epoch [62/200], Step [4/63] Loss: 0.66627\n",
            "Epoch [62/200], Step [5/63] Loss: 0.43642\n",
            "Epoch [62/200], Step [6/63] Loss: 0.54858\n",
            "Epoch [62/200], Step [7/63] Loss: 0.77300\n",
            "Epoch [62/200], Step [8/63] Loss: 0.52995\n",
            "Epoch [62/200], Step [9/63] Loss: 0.71026\n",
            "Epoch [62/200], Step [10/63] Loss: 0.91124\n",
            "Epoch [62/200], Step [11/63] Loss: 0.56368\n",
            "Epoch [62/200], Step [12/63] Loss: 0.75265\n",
            "Epoch [62/200], Step [13/63] Loss: 0.68542\n",
            "Epoch [62/200], Step [14/63] Loss: 0.63919\n",
            "Epoch [62/200], Step [15/63] Loss: 0.65720\n",
            "Epoch [62/200], Step [16/63] Loss: 0.58404\n",
            "Epoch [62/200], Step [17/63] Loss: 0.44365\n",
            "Epoch [62/200], Step [18/63] Loss: 0.58420\n",
            "Epoch [62/200], Step [19/63] Loss: 0.55203\n",
            "Epoch [62/200], Step [20/63] Loss: 0.88749\n",
            "Epoch [62/200], Step [21/63] Loss: 0.86920\n",
            "Epoch [62/200], Step [22/63] Loss: 1.17154\n",
            "Epoch [62/200], Step [23/63] Loss: 0.37830\n",
            "Epoch [62/200], Step [24/63] Loss: 0.74920\n",
            "Epoch [62/200], Step [25/63] Loss: 0.71703\n",
            "Epoch [62/200], Step [26/63] Loss: 0.72169\n",
            "Epoch [62/200], Step [27/63] Loss: 0.97694\n",
            "Epoch [62/200], Step [28/63] Loss: 0.70470\n",
            "Epoch [62/200], Step [29/63] Loss: 0.56945\n",
            "Epoch [62/200], Step [30/63] Loss: 1.12625\n",
            "Epoch [62/200], Step [31/63] Loss: 0.89210\n",
            "Epoch [62/200], Step [32/63] Loss: 0.67015\n",
            "Epoch [62/200], Step [33/63] Loss: 0.83467\n",
            "Epoch [62/200], Step [34/63] Loss: 0.68232\n",
            "Epoch [62/200], Step [35/63] Loss: 1.23124\n",
            "Epoch [62/200], Step [36/63] Loss: 0.52732\n",
            "Epoch [62/200], Step [37/63] Loss: 0.90017\n",
            "Epoch [62/200], Step [38/63] Loss: 0.69084\n",
            "Epoch [62/200], Step [39/63] Loss: 1.08406\n",
            "Epoch [62/200], Step [40/63] Loss: 0.50223\n",
            "Epoch [62/200], Step [41/63] Loss: 0.50600\n",
            "Epoch [62/200], Step [42/63] Loss: 0.46931\n",
            "Epoch [62/200], Step [43/63] Loss: 0.91634\n",
            "Epoch [62/200], Step [44/63] Loss: 0.64463\n",
            "Epoch [62/200], Step [45/63] Loss: 1.10533\n",
            "Epoch [62/200], Step [46/63] Loss: 0.58160\n",
            "Epoch [62/200], Step [47/63] Loss: 0.90085\n",
            "Epoch [62/200], Step [48/63] Loss: 0.85707\n",
            "Epoch [62/200], Step [49/63] Loss: 0.77582\n",
            "Epoch [62/200], Step [50/63] Loss: 0.54477\n",
            "Epoch [62/200], Step [51/63] Loss: 0.88760\n",
            "Epoch [62/200], Step [52/63] Loss: 0.70491\n",
            "Epoch [62/200], Step [53/63] Loss: 0.58229\n",
            "Epoch [62/200], Step [54/63] Loss: 0.77014\n",
            "Epoch [62/200], Step [55/63] Loss: 0.53892\n",
            "Epoch [62/200], Step [56/63] Loss: 0.87357\n",
            "Epoch [62/200], Step [57/63] Loss: 0.77609\n",
            "Epoch [62/200], Step [58/63] Loss: 0.57090\n",
            "Epoch [62/200], Step [59/63] Loss: 0.69226\n",
            "Epoch [62/200], Step [60/63] Loss: 0.63166\n",
            "Epoch [62/200], Step [61/63] Loss: 0.69605\n",
            "Epoch [62/200], Step [62/63] Loss: 0.84254\n",
            "Epoch [62/200], Step [63/63] Loss: 0.76271\n",
            "Valid Accuracy: 66.43518518518519 %\n",
            "Epoch [63/200], Step [1/63] Loss: 0.75279\n",
            "Epoch [63/200], Step [2/63] Loss: 0.78119\n",
            "Epoch [63/200], Step [3/63] Loss: 0.70789\n",
            "Epoch [63/200], Step [4/63] Loss: 0.57488\n",
            "Epoch [63/200], Step [5/63] Loss: 0.88794\n",
            "Epoch [63/200], Step [6/63] Loss: 0.50721\n",
            "Epoch [63/200], Step [7/63] Loss: 0.72784\n",
            "Epoch [63/200], Step [8/63] Loss: 0.82267\n",
            "Epoch [63/200], Step [9/63] Loss: 0.85299\n",
            "Epoch [63/200], Step [10/63] Loss: 0.80542\n",
            "Epoch [63/200], Step [11/63] Loss: 0.68916\n",
            "Epoch [63/200], Step [12/63] Loss: 0.56338\n",
            "Epoch [63/200], Step [13/63] Loss: 0.83318\n",
            "Epoch [63/200], Step [14/63] Loss: 0.66721\n",
            "Epoch [63/200], Step [15/63] Loss: 0.89239\n",
            "Epoch [63/200], Step [16/63] Loss: 0.66617\n",
            "Epoch [63/200], Step [17/63] Loss: 0.72015\n",
            "Epoch [63/200], Step [18/63] Loss: 0.72630\n",
            "Epoch [63/200], Step [19/63] Loss: 0.64829\n",
            "Epoch [63/200], Step [20/63] Loss: 1.00272\n",
            "Epoch [63/200], Step [21/63] Loss: 0.66948\n",
            "Epoch [63/200], Step [22/63] Loss: 0.80240\n",
            "Epoch [63/200], Step [23/63] Loss: 0.80638\n",
            "Epoch [63/200], Step [24/63] Loss: 0.58402\n",
            "Epoch [63/200], Step [25/63] Loss: 0.62491\n",
            "Epoch [63/200], Step [26/63] Loss: 0.94531\n",
            "Epoch [63/200], Step [27/63] Loss: 0.57991\n",
            "Epoch [63/200], Step [28/63] Loss: 0.62951\n",
            "Epoch [63/200], Step [29/63] Loss: 0.76190\n",
            "Epoch [63/200], Step [30/63] Loss: 0.88358\n",
            "Epoch [63/200], Step [31/63] Loss: 0.66543\n",
            "Epoch [63/200], Step [32/63] Loss: 0.76651\n",
            "Epoch [63/200], Step [33/63] Loss: 0.83489\n",
            "Epoch [63/200], Step [34/63] Loss: 0.80627\n",
            "Epoch [63/200], Step [35/63] Loss: 0.46414\n",
            "Epoch [63/200], Step [36/63] Loss: 0.64416\n",
            "Epoch [63/200], Step [37/63] Loss: 0.52538\n",
            "Epoch [63/200], Step [38/63] Loss: 0.60554\n",
            "Epoch [63/200], Step [39/63] Loss: 0.74513\n",
            "Epoch [63/200], Step [40/63] Loss: 0.67324\n",
            "Epoch [63/200], Step [41/63] Loss: 0.77556\n",
            "Epoch [63/200], Step [42/63] Loss: 0.59973\n",
            "Epoch [63/200], Step [43/63] Loss: 0.47328\n",
            "Epoch [63/200], Step [44/63] Loss: 0.88146\n",
            "Epoch [63/200], Step [45/63] Loss: 0.56701\n",
            "Epoch [63/200], Step [46/63] Loss: 0.77719\n",
            "Epoch [63/200], Step [47/63] Loss: 0.68671\n",
            "Epoch [63/200], Step [48/63] Loss: 0.61777\n",
            "Epoch [63/200], Step [49/63] Loss: 1.46335\n",
            "Epoch [63/200], Step [50/63] Loss: 1.04618\n",
            "Epoch [63/200], Step [51/63] Loss: 0.87527\n",
            "Epoch [63/200], Step [52/63] Loss: 0.70969\n",
            "Epoch [63/200], Step [53/63] Loss: 0.71869\n",
            "Epoch [63/200], Step [54/63] Loss: 0.59915\n",
            "Epoch [63/200], Step [55/63] Loss: 0.64392\n",
            "Epoch [63/200], Step [56/63] Loss: 0.72380\n",
            "Epoch [63/200], Step [57/63] Loss: 0.82205\n",
            "Epoch [63/200], Step [58/63] Loss: 0.80164\n",
            "Epoch [63/200], Step [59/63] Loss: 0.66438\n",
            "Epoch [63/200], Step [60/63] Loss: 0.47734\n",
            "Epoch [63/200], Step [61/63] Loss: 0.71488\n",
            "Epoch [63/200], Step [62/63] Loss: 0.62998\n",
            "Epoch [63/200], Step [63/63] Loss: 0.49060\n",
            "Valid Accuracy: 66.89814814814815 %\n",
            "Epoch [64/200], Step [1/63] Loss: 0.97263\n",
            "Epoch [64/200], Step [2/63] Loss: 0.69736\n",
            "Epoch [64/200], Step [3/63] Loss: 0.69184\n",
            "Epoch [64/200], Step [4/63] Loss: 0.69578\n",
            "Epoch [64/200], Step [5/63] Loss: 1.64411\n",
            "Epoch [64/200], Step [6/63] Loss: 0.62834\n",
            "Epoch [64/200], Step [7/63] Loss: 0.70331\n",
            "Epoch [64/200], Step [8/63] Loss: 0.87096\n",
            "Epoch [64/200], Step [9/63] Loss: 0.72205\n",
            "Epoch [64/200], Step [10/63] Loss: 0.73728\n",
            "Epoch [64/200], Step [11/63] Loss: 0.73202\n",
            "Epoch [64/200], Step [12/63] Loss: 0.61774\n",
            "Epoch [64/200], Step [13/63] Loss: 0.62350\n",
            "Epoch [64/200], Step [14/63] Loss: 0.59231\n",
            "Epoch [64/200], Step [15/63] Loss: 0.52678\n",
            "Epoch [64/200], Step [16/63] Loss: 0.63262\n",
            "Epoch [64/200], Step [17/63] Loss: 0.69399\n",
            "Epoch [64/200], Step [18/63] Loss: 0.55734\n",
            "Epoch [64/200], Step [19/63] Loss: 0.49452\n",
            "Epoch [64/200], Step [20/63] Loss: 0.57884\n",
            "Epoch [64/200], Step [21/63] Loss: 0.52621\n",
            "Epoch [64/200], Step [22/63] Loss: 0.80666\n",
            "Epoch [64/200], Step [23/63] Loss: 0.58565\n",
            "Epoch [64/200], Step [24/63] Loss: 0.59323\n",
            "Epoch [64/200], Step [25/63] Loss: 1.17793\n",
            "Epoch [64/200], Step [26/63] Loss: 0.94903\n",
            "Epoch [64/200], Step [27/63] Loss: 0.77036\n",
            "Epoch [64/200], Step [28/63] Loss: 0.76542\n",
            "Epoch [64/200], Step [29/63] Loss: 0.76184\n",
            "Epoch [64/200], Step [30/63] Loss: 0.94095\n",
            "Epoch [64/200], Step [31/63] Loss: 0.55829\n",
            "Epoch [64/200], Step [32/63] Loss: 0.73184\n",
            "Epoch [64/200], Step [33/63] Loss: 0.81929\n",
            "Epoch [64/200], Step [34/63] Loss: 0.61730\n",
            "Epoch [64/200], Step [35/63] Loss: 0.72495\n",
            "Epoch [64/200], Step [36/63] Loss: 0.69342\n",
            "Epoch [64/200], Step [37/63] Loss: 0.49028\n",
            "Epoch [64/200], Step [38/63] Loss: 0.80045\n",
            "Epoch [64/200], Step [39/63] Loss: 0.78256\n",
            "Epoch [64/200], Step [40/63] Loss: 0.53617\n",
            "Epoch [64/200], Step [41/63] Loss: 0.38954\n",
            "Epoch [64/200], Step [42/63] Loss: 0.74124\n",
            "Epoch [64/200], Step [43/63] Loss: 0.80936\n",
            "Epoch [64/200], Step [44/63] Loss: 0.65269\n",
            "Epoch [64/200], Step [45/63] Loss: 0.57959\n",
            "Epoch [64/200], Step [46/63] Loss: 0.65065\n",
            "Epoch [64/200], Step [47/63] Loss: 0.45020\n",
            "Epoch [64/200], Step [48/63] Loss: 0.43706\n",
            "Epoch [64/200], Step [49/63] Loss: 0.57713\n",
            "Epoch [64/200], Step [50/63] Loss: 0.69082\n",
            "Epoch [64/200], Step [51/63] Loss: 1.03677\n",
            "Epoch [64/200], Step [52/63] Loss: 1.25223\n",
            "Epoch [64/200], Step [53/63] Loss: 0.82348\n",
            "Epoch [64/200], Step [54/63] Loss: 0.54722\n",
            "Epoch [64/200], Step [55/63] Loss: 1.05953\n",
            "Epoch [64/200], Step [56/63] Loss: 0.82849\n",
            "Epoch [64/200], Step [57/63] Loss: 0.84468\n",
            "Epoch [64/200], Step [58/63] Loss: 0.72147\n",
            "Epoch [64/200], Step [59/63] Loss: 0.90557\n",
            "Epoch [64/200], Step [60/63] Loss: 0.71084\n",
            "Epoch [64/200], Step [61/63] Loss: 0.89262\n",
            "Epoch [64/200], Step [62/63] Loss: 0.46866\n",
            "Epoch [64/200], Step [63/63] Loss: 0.65179\n",
            "Valid Accuracy: 66.89814814814815 %\n",
            "Epoch [65/200], Step [1/63] Loss: 0.53934\n",
            "Epoch [65/200], Step [2/63] Loss: 0.58894\n",
            "Epoch [65/200], Step [3/63] Loss: 0.41327\n",
            "Epoch [65/200], Step [4/63] Loss: 1.33742\n",
            "Epoch [65/200], Step [5/63] Loss: 0.54697\n",
            "Epoch [65/200], Step [6/63] Loss: 0.71077\n",
            "Epoch [65/200], Step [7/63] Loss: 0.57256\n",
            "Epoch [65/200], Step [8/63] Loss: 0.55938\n",
            "Epoch [65/200], Step [9/63] Loss: 0.60316\n",
            "Epoch [65/200], Step [10/63] Loss: 0.74942\n",
            "Epoch [65/200], Step [11/63] Loss: 0.69183\n",
            "Epoch [65/200], Step [12/63] Loss: 0.55956\n",
            "Epoch [65/200], Step [13/63] Loss: 0.61861\n",
            "Epoch [65/200], Step [14/63] Loss: 0.54670\n",
            "Epoch [65/200], Step [15/63] Loss: 1.07213\n",
            "Epoch [65/200], Step [16/63] Loss: 0.51181\n",
            "Epoch [65/200], Step [17/63] Loss: 0.53315\n",
            "Epoch [65/200], Step [18/63] Loss: 0.93241\n",
            "Epoch [65/200], Step [19/63] Loss: 0.62817\n",
            "Epoch [65/200], Step [20/63] Loss: 0.55944\n",
            "Epoch [65/200], Step [21/63] Loss: 0.92145\n",
            "Epoch [65/200], Step [22/63] Loss: 0.61059\n",
            "Epoch [65/200], Step [23/63] Loss: 0.97285\n",
            "Epoch [65/200], Step [24/63] Loss: 0.90677\n",
            "Epoch [65/200], Step [25/63] Loss: 0.57147\n",
            "Epoch [65/200], Step [26/63] Loss: 0.54212\n",
            "Epoch [65/200], Step [27/63] Loss: 0.59139\n",
            "Epoch [65/200], Step [28/63] Loss: 0.93482\n",
            "Epoch [65/200], Step [29/63] Loss: 0.57518\n",
            "Epoch [65/200], Step [30/63] Loss: 0.99093\n",
            "Epoch [65/200], Step [31/63] Loss: 0.62849\n",
            "Epoch [65/200], Step [32/63] Loss: 0.52135\n",
            "Epoch [65/200], Step [33/63] Loss: 0.83345\n",
            "Epoch [65/200], Step [34/63] Loss: 0.70790\n",
            "Epoch [65/200], Step [35/63] Loss: 0.64353\n",
            "Epoch [65/200], Step [36/63] Loss: 0.82640\n",
            "Epoch [65/200], Step [37/63] Loss: 0.65145\n",
            "Epoch [65/200], Step [38/63] Loss: 0.59799\n",
            "Epoch [65/200], Step [39/63] Loss: 0.82963\n",
            "Epoch [65/200], Step [40/63] Loss: 0.65416\n",
            "Epoch [65/200], Step [41/63] Loss: 0.93224\n",
            "Epoch [65/200], Step [42/63] Loss: 0.65627\n",
            "Epoch [65/200], Step [43/63] Loss: 0.79056\n",
            "Epoch [65/200], Step [44/63] Loss: 0.75414\n",
            "Epoch [65/200], Step [45/63] Loss: 0.84638\n",
            "Epoch [65/200], Step [46/63] Loss: 0.84145\n",
            "Epoch [65/200], Step [47/63] Loss: 0.65425\n",
            "Epoch [65/200], Step [48/63] Loss: 0.67355\n",
            "Epoch [65/200], Step [49/63] Loss: 0.70259\n",
            "Epoch [65/200], Step [50/63] Loss: 0.76311\n",
            "Epoch [65/200], Step [51/63] Loss: 0.66048\n",
            "Epoch [65/200], Step [52/63] Loss: 0.44559\n",
            "Epoch [65/200], Step [53/63] Loss: 0.62860\n",
            "Epoch [65/200], Step [54/63] Loss: 0.80007\n",
            "Epoch [65/200], Step [55/63] Loss: 0.66979\n",
            "Epoch [65/200], Step [56/63] Loss: 0.87147\n",
            "Epoch [65/200], Step [57/63] Loss: 0.59956\n",
            "Epoch [65/200], Step [58/63] Loss: 0.81481\n",
            "Epoch [65/200], Step [59/63] Loss: 0.82273\n",
            "Epoch [65/200], Step [60/63] Loss: 0.62255\n",
            "Epoch [65/200], Step [61/63] Loss: 0.71668\n",
            "Epoch [65/200], Step [62/63] Loss: 0.67985\n",
            "Epoch [65/200], Step [63/63] Loss: 0.87085\n",
            "Valid Accuracy: 60.18518518518518 %\n",
            "Epoch [66/200], Step [1/63] Loss: 0.56555\n",
            "Epoch [66/200], Step [2/63] Loss: 0.62652\n",
            "Epoch [66/200], Step [3/63] Loss: 0.81929\n",
            "Epoch [66/200], Step [4/63] Loss: 0.66147\n",
            "Epoch [66/200], Step [5/63] Loss: 0.39282\n",
            "Epoch [66/200], Step [6/63] Loss: 0.70387\n",
            "Epoch [66/200], Step [7/63] Loss: 0.56014\n",
            "Epoch [66/200], Step [8/63] Loss: 0.97179\n",
            "Epoch [66/200], Step [9/63] Loss: 0.66104\n",
            "Epoch [66/200], Step [10/63] Loss: 0.63244\n",
            "Epoch [66/200], Step [11/63] Loss: 0.59486\n",
            "Epoch [66/200], Step [12/63] Loss: 0.77546\n",
            "Epoch [66/200], Step [13/63] Loss: 1.07265\n",
            "Epoch [66/200], Step [14/63] Loss: 0.56102\n",
            "Epoch [66/200], Step [15/63] Loss: 1.00519\n",
            "Epoch [66/200], Step [16/63] Loss: 0.71733\n",
            "Epoch [66/200], Step [17/63] Loss: 0.68727\n",
            "Epoch [66/200], Step [18/63] Loss: 0.59808\n",
            "Epoch [66/200], Step [19/63] Loss: 0.68156\n",
            "Epoch [66/200], Step [20/63] Loss: 0.96901\n",
            "Epoch [66/200], Step [21/63] Loss: 0.57120\n",
            "Epoch [66/200], Step [22/63] Loss: 0.90806\n",
            "Epoch [66/200], Step [23/63] Loss: 0.88285\n",
            "Epoch [66/200], Step [24/63] Loss: 0.86877\n",
            "Epoch [66/200], Step [25/63] Loss: 0.79695\n",
            "Epoch [66/200], Step [26/63] Loss: 0.67892\n",
            "Epoch [66/200], Step [27/63] Loss: 0.49372\n",
            "Epoch [66/200], Step [28/63] Loss: 0.86891\n",
            "Epoch [66/200], Step [29/63] Loss: 0.74398\n",
            "Epoch [66/200], Step [30/63] Loss: 0.75482\n",
            "Epoch [66/200], Step [31/63] Loss: 0.81914\n",
            "Epoch [66/200], Step [32/63] Loss: 0.75206\n",
            "Epoch [66/200], Step [33/63] Loss: 1.21918\n",
            "Epoch [66/200], Step [34/63] Loss: 0.76312\n",
            "Epoch [66/200], Step [35/63] Loss: 0.62892\n",
            "Epoch [66/200], Step [36/63] Loss: 0.64008\n",
            "Epoch [66/200], Step [37/63] Loss: 0.89797\n",
            "Epoch [66/200], Step [38/63] Loss: 0.67118\n",
            "Epoch [66/200], Step [39/63] Loss: 0.65460\n",
            "Epoch [66/200], Step [40/63] Loss: 0.87034\n",
            "Epoch [66/200], Step [41/63] Loss: 0.61551\n",
            "Epoch [66/200], Step [42/63] Loss: 0.47096\n",
            "Epoch [66/200], Step [43/63] Loss: 0.78503\n",
            "Epoch [66/200], Step [44/63] Loss: 0.62601\n",
            "Epoch [66/200], Step [45/63] Loss: 0.71239\n",
            "Epoch [66/200], Step [46/63] Loss: 0.78355\n",
            "Epoch [66/200], Step [47/63] Loss: 0.82482\n",
            "Epoch [66/200], Step [48/63] Loss: 0.53188\n",
            "Epoch [66/200], Step [49/63] Loss: 0.60575\n",
            "Epoch [66/200], Step [50/63] Loss: 0.64829\n",
            "Epoch [66/200], Step [51/63] Loss: 0.46192\n",
            "Epoch [66/200], Step [52/63] Loss: 0.80830\n",
            "Epoch [66/200], Step [53/63] Loss: 0.67694\n",
            "Epoch [66/200], Step [54/63] Loss: 0.64020\n",
            "Epoch [66/200], Step [55/63] Loss: 0.59644\n",
            "Epoch [66/200], Step [56/63] Loss: 0.77740\n",
            "Epoch [66/200], Step [57/63] Loss: 0.83439\n",
            "Epoch [66/200], Step [58/63] Loss: 0.45711\n",
            "Epoch [66/200], Step [59/63] Loss: 0.79627\n",
            "Epoch [66/200], Step [60/63] Loss: 0.58683\n",
            "Epoch [66/200], Step [61/63] Loss: 0.71156\n",
            "Epoch [66/200], Step [62/63] Loss: 0.48722\n",
            "Epoch [66/200], Step [63/63] Loss: 0.96220\n",
            "Valid Accuracy: 69.67592592592592 %\n",
            "Epoch [67/200], Step [1/63] Loss: 0.70591\n",
            "Epoch [67/200], Step [2/63] Loss: 0.79162\n",
            "Epoch [67/200], Step [3/63] Loss: 0.69695\n",
            "Epoch [67/200], Step [4/63] Loss: 0.68889\n",
            "Epoch [67/200], Step [5/63] Loss: 0.97910\n",
            "Epoch [67/200], Step [6/63] Loss: 0.64056\n",
            "Epoch [67/200], Step [7/63] Loss: 0.58837\n",
            "Epoch [67/200], Step [8/63] Loss: 0.62768\n",
            "Epoch [67/200], Step [9/63] Loss: 0.84911\n",
            "Epoch [67/200], Step [10/63] Loss: 0.76350\n",
            "Epoch [67/200], Step [11/63] Loss: 0.82735\n",
            "Epoch [67/200], Step [12/63] Loss: 0.77580\n",
            "Epoch [67/200], Step [13/63] Loss: 0.82161\n",
            "Epoch [67/200], Step [14/63] Loss: 0.78871\n",
            "Epoch [67/200], Step [15/63] Loss: 0.64261\n",
            "Epoch [67/200], Step [16/63] Loss: 0.50000\n",
            "Epoch [67/200], Step [17/63] Loss: 0.65550\n",
            "Epoch [67/200], Step [18/63] Loss: 0.46405\n",
            "Epoch [67/200], Step [19/63] Loss: 0.96579\n",
            "Epoch [67/200], Step [20/63] Loss: 0.85862\n",
            "Epoch [67/200], Step [21/63] Loss: 0.78045\n",
            "Epoch [67/200], Step [22/63] Loss: 0.94975\n",
            "Epoch [67/200], Step [23/63] Loss: 0.79864\n",
            "Epoch [67/200], Step [24/63] Loss: 0.61002\n",
            "Epoch [67/200], Step [25/63] Loss: 0.87049\n",
            "Epoch [67/200], Step [26/63] Loss: 0.86971\n",
            "Epoch [67/200], Step [27/63] Loss: 0.82465\n",
            "Epoch [67/200], Step [28/63] Loss: 0.62399\n",
            "Epoch [67/200], Step [29/63] Loss: 0.54635\n",
            "Epoch [67/200], Step [30/63] Loss: 0.56513\n",
            "Epoch [67/200], Step [31/63] Loss: 0.63748\n",
            "Epoch [67/200], Step [32/63] Loss: 0.50412\n",
            "Epoch [67/200], Step [33/63] Loss: 0.73688\n",
            "Epoch [67/200], Step [34/63] Loss: 0.70120\n",
            "Epoch [67/200], Step [35/63] Loss: 0.60724\n",
            "Epoch [67/200], Step [36/63] Loss: 0.86149\n",
            "Epoch [67/200], Step [37/63] Loss: 0.76356\n",
            "Epoch [67/200], Step [38/63] Loss: 0.78668\n",
            "Epoch [67/200], Step [39/63] Loss: 0.58369\n",
            "Epoch [67/200], Step [40/63] Loss: 0.49324\n",
            "Epoch [67/200], Step [41/63] Loss: 0.31889\n",
            "Epoch [67/200], Step [42/63] Loss: 0.54133\n",
            "Epoch [67/200], Step [43/63] Loss: 0.88526\n",
            "Epoch [67/200], Step [44/63] Loss: 0.74967\n",
            "Epoch [67/200], Step [45/63] Loss: 0.88159\n",
            "Epoch [67/200], Step [46/63] Loss: 0.61398\n",
            "Epoch [67/200], Step [47/63] Loss: 0.77973\n",
            "Epoch [67/200], Step [48/63] Loss: 0.62303\n",
            "Epoch [67/200], Step [49/63] Loss: 0.94896\n",
            "Epoch [67/200], Step [50/63] Loss: 0.56993\n",
            "Epoch [67/200], Step [51/63] Loss: 0.73387\n",
            "Epoch [67/200], Step [52/63] Loss: 0.54259\n",
            "Epoch [67/200], Step [53/63] Loss: 0.72192\n",
            "Epoch [67/200], Step [54/63] Loss: 0.82931\n",
            "Epoch [67/200], Step [55/63] Loss: 0.72069\n",
            "Epoch [67/200], Step [56/63] Loss: 0.78113\n",
            "Epoch [67/200], Step [57/63] Loss: 0.69146\n",
            "Epoch [67/200], Step [58/63] Loss: 0.91629\n",
            "Epoch [67/200], Step [59/63] Loss: 0.54022\n",
            "Epoch [67/200], Step [60/63] Loss: 0.48537\n",
            "Epoch [67/200], Step [61/63] Loss: 0.70963\n",
            "Epoch [67/200], Step [62/63] Loss: 0.59110\n",
            "Epoch [67/200], Step [63/63] Loss: 0.65032\n",
            "Valid Accuracy: 66.66666666666667 %\n",
            "Epoch [68/200], Step [1/63] Loss: 0.82027\n",
            "Epoch [68/200], Step [2/63] Loss: 0.57993\n",
            "Epoch [68/200], Step [3/63] Loss: 0.43433\n",
            "Epoch [68/200], Step [4/63] Loss: 0.92588\n",
            "Epoch [68/200], Step [5/63] Loss: 0.67785\n",
            "Epoch [68/200], Step [6/63] Loss: 0.54077\n",
            "Epoch [68/200], Step [7/63] Loss: 0.84369\n",
            "Epoch [68/200], Step [8/63] Loss: 0.62898\n",
            "Epoch [68/200], Step [9/63] Loss: 0.76895\n",
            "Epoch [68/200], Step [10/63] Loss: 0.81368\n",
            "Epoch [68/200], Step [11/63] Loss: 0.97848\n",
            "Epoch [68/200], Step [12/63] Loss: 0.70344\n",
            "Epoch [68/200], Step [13/63] Loss: 0.73611\n",
            "Epoch [68/200], Step [14/63] Loss: 0.98075\n",
            "Epoch [68/200], Step [15/63] Loss: 0.61763\n",
            "Epoch [68/200], Step [16/63] Loss: 0.64847\n",
            "Epoch [68/200], Step [17/63] Loss: 0.65397\n",
            "Epoch [68/200], Step [18/63] Loss: 0.94187\n",
            "Epoch [68/200], Step [19/63] Loss: 0.57627\n",
            "Epoch [68/200], Step [20/63] Loss: 0.78689\n",
            "Epoch [68/200], Step [21/63] Loss: 0.66556\n",
            "Epoch [68/200], Step [22/63] Loss: 0.81847\n",
            "Epoch [68/200], Step [23/63] Loss: 0.68170\n",
            "Epoch [68/200], Step [24/63] Loss: 0.56367\n",
            "Epoch [68/200], Step [25/63] Loss: 1.18931\n",
            "Epoch [68/200], Step [26/63] Loss: 0.58381\n",
            "Epoch [68/200], Step [27/63] Loss: 0.64677\n",
            "Epoch [68/200], Step [28/63] Loss: 0.84183\n",
            "Epoch [68/200], Step [29/63] Loss: 0.67612\n",
            "Epoch [68/200], Step [30/63] Loss: 0.49931\n",
            "Epoch [68/200], Step [31/63] Loss: 0.78836\n",
            "Epoch [68/200], Step [32/63] Loss: 0.66404\n",
            "Epoch [68/200], Step [33/63] Loss: 0.92440\n",
            "Epoch [68/200], Step [34/63] Loss: 0.77205\n",
            "Epoch [68/200], Step [35/63] Loss: 0.58233\n",
            "Epoch [68/200], Step [36/63] Loss: 1.00315\n",
            "Epoch [68/200], Step [37/63] Loss: 0.57327\n",
            "Epoch [68/200], Step [38/63] Loss: 0.88337\n",
            "Epoch [68/200], Step [39/63] Loss: 0.93492\n",
            "Epoch [68/200], Step [40/63] Loss: 0.71403\n",
            "Epoch [68/200], Step [41/63] Loss: 0.98908\n",
            "Epoch [68/200], Step [42/63] Loss: 0.90008\n",
            "Epoch [68/200], Step [43/63] Loss: 0.47441\n",
            "Epoch [68/200], Step [44/63] Loss: 0.69249\n",
            "Epoch [68/200], Step [45/63] Loss: 0.93533\n",
            "Epoch [68/200], Step [46/63] Loss: 0.67650\n",
            "Epoch [68/200], Step [47/63] Loss: 0.51843\n",
            "Epoch [68/200], Step [48/63] Loss: 0.69976\n",
            "Epoch [68/200], Step [49/63] Loss: 0.59957\n",
            "Epoch [68/200], Step [50/63] Loss: 0.64015\n",
            "Epoch [68/200], Step [51/63] Loss: 0.63221\n",
            "Epoch [68/200], Step [52/63] Loss: 0.70689\n",
            "Epoch [68/200], Step [53/63] Loss: 0.31828\n",
            "Epoch [68/200], Step [54/63] Loss: 0.91591\n",
            "Epoch [68/200], Step [55/63] Loss: 0.35239\n",
            "Epoch [68/200], Step [56/63] Loss: 0.60464\n",
            "Epoch [68/200], Step [57/63] Loss: 0.66998\n",
            "Epoch [68/200], Step [58/63] Loss: 0.92921\n",
            "Epoch [68/200], Step [59/63] Loss: 0.61516\n",
            "Epoch [68/200], Step [60/63] Loss: 0.55725\n",
            "Epoch [68/200], Step [61/63] Loss: 0.70964\n",
            "Epoch [68/200], Step [62/63] Loss: 0.90758\n",
            "Epoch [68/200], Step [63/63] Loss: 0.73775\n",
            "Valid Accuracy: 61.574074074074076 %\n",
            "Epoch [69/200], Step [1/63] Loss: 0.68266\n",
            "Epoch [69/200], Step [2/63] Loss: 0.53467\n",
            "Epoch [69/200], Step [3/63] Loss: 0.67182\n",
            "Epoch [69/200], Step [4/63] Loss: 0.29772\n",
            "Epoch [69/200], Step [5/63] Loss: 0.97635\n",
            "Epoch [69/200], Step [6/63] Loss: 0.59058\n",
            "Epoch [69/200], Step [7/63] Loss: 0.67796\n",
            "Epoch [69/200], Step [8/63] Loss: 0.99628\n",
            "Epoch [69/200], Step [9/63] Loss: 0.69478\n",
            "Epoch [69/200], Step [10/63] Loss: 0.80881\n",
            "Epoch [69/200], Step [11/63] Loss: 0.63562\n",
            "Epoch [69/200], Step [12/63] Loss: 0.77729\n",
            "Epoch [69/200], Step [13/63] Loss: 0.71789\n",
            "Epoch [69/200], Step [14/63] Loss: 0.70503\n",
            "Epoch [69/200], Step [15/63] Loss: 0.97461\n",
            "Epoch [69/200], Step [16/63] Loss: 0.63769\n",
            "Epoch [69/200], Step [17/63] Loss: 0.55765\n",
            "Epoch [69/200], Step [18/63] Loss: 0.66072\n",
            "Epoch [69/200], Step [19/63] Loss: 0.78121\n",
            "Epoch [69/200], Step [20/63] Loss: 0.58265\n",
            "Epoch [69/200], Step [21/63] Loss: 0.81125\n",
            "Epoch [69/200], Step [22/63] Loss: 0.75152\n",
            "Epoch [69/200], Step [23/63] Loss: 0.69743\n",
            "Epoch [69/200], Step [24/63] Loss: 0.53978\n",
            "Epoch [69/200], Step [25/63] Loss: 0.67917\n",
            "Epoch [69/200], Step [26/63] Loss: 0.67125\n",
            "Epoch [69/200], Step [27/63] Loss: 1.06092\n",
            "Epoch [69/200], Step [28/63] Loss: 0.93143\n",
            "Epoch [69/200], Step [29/63] Loss: 0.46595\n",
            "Epoch [69/200], Step [30/63] Loss: 0.47320\n",
            "Epoch [69/200], Step [31/63] Loss: 0.52813\n",
            "Epoch [69/200], Step [32/63] Loss: 0.84303\n",
            "Epoch [69/200], Step [33/63] Loss: 0.55824\n",
            "Epoch [69/200], Step [34/63] Loss: 0.90069\n",
            "Epoch [69/200], Step [35/63] Loss: 0.85528\n",
            "Epoch [69/200], Step [36/63] Loss: 0.76128\n",
            "Epoch [69/200], Step [37/63] Loss: 0.56789\n",
            "Epoch [69/200], Step [38/63] Loss: 0.90404\n",
            "Epoch [69/200], Step [39/63] Loss: 0.62546\n",
            "Epoch [69/200], Step [40/63] Loss: 0.74946\n",
            "Epoch [69/200], Step [41/63] Loss: 0.79278\n",
            "Epoch [69/200], Step [42/63] Loss: 0.51000\n",
            "Epoch [69/200], Step [43/63] Loss: 0.61249\n",
            "Epoch [69/200], Step [44/63] Loss: 0.58067\n",
            "Epoch [69/200], Step [45/63] Loss: 0.68434\n",
            "Epoch [69/200], Step [46/63] Loss: 0.41062\n",
            "Epoch [69/200], Step [47/63] Loss: 0.78436\n",
            "Epoch [69/200], Step [48/63] Loss: 0.62246\n",
            "Epoch [69/200], Step [49/63] Loss: 0.65345\n",
            "Epoch [69/200], Step [50/63] Loss: 0.73169\n",
            "Epoch [69/200], Step [51/63] Loss: 1.14417\n",
            "Epoch [69/200], Step [52/63] Loss: 0.69158\n",
            "Epoch [69/200], Step [53/63] Loss: 0.79771\n",
            "Epoch [69/200], Step [54/63] Loss: 0.82074\n",
            "Epoch [69/200], Step [55/63] Loss: 0.62321\n",
            "Epoch [69/200], Step [56/63] Loss: 0.41296\n",
            "Epoch [69/200], Step [57/63] Loss: 0.74405\n",
            "Epoch [69/200], Step [58/63] Loss: 0.88053\n",
            "Epoch [69/200], Step [59/63] Loss: 0.76123\n",
            "Epoch [69/200], Step [60/63] Loss: 0.87192\n",
            "Epoch [69/200], Step [61/63] Loss: 0.67052\n",
            "Epoch [69/200], Step [62/63] Loss: 0.58901\n",
            "Epoch [69/200], Step [63/63] Loss: 0.75273\n",
            "Valid Accuracy: 68.51851851851852 %\n",
            "Epoch [70/200], Step [1/63] Loss: 0.54689\n",
            "Epoch [70/200], Step [2/63] Loss: 0.44727\n",
            "Epoch [70/200], Step [3/63] Loss: 0.43941\n",
            "Epoch [70/200], Step [4/63] Loss: 0.92694\n",
            "Epoch [70/200], Step [5/63] Loss: 0.39667\n",
            "Epoch [70/200], Step [6/63] Loss: 0.85692\n",
            "Epoch [70/200], Step [7/63] Loss: 0.40840\n",
            "Epoch [70/200], Step [8/63] Loss: 1.05352\n",
            "Epoch [70/200], Step [9/63] Loss: 0.44322\n",
            "Epoch [70/200], Step [10/63] Loss: 0.46308\n",
            "Epoch [70/200], Step [11/63] Loss: 0.73106\n",
            "Epoch [70/200], Step [12/63] Loss: 0.55502\n",
            "Epoch [70/200], Step [13/63] Loss: 0.79230\n",
            "Epoch [70/200], Step [14/63] Loss: 0.42228\n",
            "Epoch [70/200], Step [15/63] Loss: 0.54348\n",
            "Epoch [70/200], Step [16/63] Loss: 0.70932\n",
            "Epoch [70/200], Step [17/63] Loss: 0.67003\n",
            "Epoch [70/200], Step [18/63] Loss: 0.39435\n",
            "Epoch [70/200], Step [19/63] Loss: 0.66625\n",
            "Epoch [70/200], Step [20/63] Loss: 0.62095\n",
            "Epoch [70/200], Step [21/63] Loss: 0.59783\n",
            "Epoch [70/200], Step [22/63] Loss: 1.02351\n",
            "Epoch [70/200], Step [23/63] Loss: 0.89119\n",
            "Epoch [70/200], Step [24/63] Loss: 0.78937\n",
            "Epoch [70/200], Step [25/63] Loss: 1.10659\n",
            "Epoch [70/200], Step [26/63] Loss: 0.60961\n",
            "Epoch [70/200], Step [27/63] Loss: 0.68862\n",
            "Epoch [70/200], Step [28/63] Loss: 0.65788\n",
            "Epoch [70/200], Step [29/63] Loss: 0.60574\n",
            "Epoch [70/200], Step [30/63] Loss: 0.56797\n",
            "Epoch [70/200], Step [31/63] Loss: 1.00411\n",
            "Epoch [70/200], Step [32/63] Loss: 0.73173\n",
            "Epoch [70/200], Step [33/63] Loss: 0.81118\n",
            "Epoch [70/200], Step [34/63] Loss: 0.69938\n",
            "Epoch [70/200], Step [35/63] Loss: 0.85395\n",
            "Epoch [70/200], Step [36/63] Loss: 0.67179\n",
            "Epoch [70/200], Step [37/63] Loss: 0.88152\n",
            "Epoch [70/200], Step [38/63] Loss: 0.65513\n",
            "Epoch [70/200], Step [39/63] Loss: 0.74295\n",
            "Epoch [70/200], Step [40/63] Loss: 0.65857\n",
            "Epoch [70/200], Step [41/63] Loss: 1.06111\n",
            "Epoch [70/200], Step [42/63] Loss: 0.54948\n",
            "Epoch [70/200], Step [43/63] Loss: 0.56156\n",
            "Epoch [70/200], Step [44/63] Loss: 0.65998\n",
            "Epoch [70/200], Step [45/63] Loss: 0.78334\n",
            "Epoch [70/200], Step [46/63] Loss: 0.65333\n",
            "Epoch [70/200], Step [47/63] Loss: 0.86343\n",
            "Epoch [70/200], Step [48/63] Loss: 0.53882\n",
            "Epoch [70/200], Step [49/63] Loss: 0.70922\n",
            "Epoch [70/200], Step [50/63] Loss: 0.64430\n",
            "Epoch [70/200], Step [51/63] Loss: 0.82587\n",
            "Epoch [70/200], Step [52/63] Loss: 0.62238\n",
            "Epoch [70/200], Step [53/63] Loss: 0.75661\n",
            "Epoch [70/200], Step [54/63] Loss: 0.75897\n",
            "Epoch [70/200], Step [55/63] Loss: 0.73258\n",
            "Epoch [70/200], Step [56/63] Loss: 0.78175\n",
            "Epoch [70/200], Step [57/63] Loss: 0.65084\n",
            "Epoch [70/200], Step [58/63] Loss: 0.55501\n",
            "Epoch [70/200], Step [59/63] Loss: 0.73863\n",
            "Epoch [70/200], Step [60/63] Loss: 0.78917\n",
            "Epoch [70/200], Step [61/63] Loss: 1.06666\n",
            "Epoch [70/200], Step [62/63] Loss: 0.53043\n",
            "Epoch [70/200], Step [63/63] Loss: 0.55643\n",
            "Valid Accuracy: 49.074074074074076 %\n",
            "Epoch [71/200], Step [1/63] Loss: 0.60508\n",
            "Epoch [71/200], Step [2/63] Loss: 0.60663\n",
            "Epoch [71/200], Step [3/63] Loss: 0.67127\n",
            "Epoch [71/200], Step [4/63] Loss: 0.87330\n",
            "Epoch [71/200], Step [5/63] Loss: 0.43176\n",
            "Epoch [71/200], Step [6/63] Loss: 0.87828\n",
            "Epoch [71/200], Step [7/63] Loss: 0.74145\n",
            "Epoch [71/200], Step [8/63] Loss: 0.91427\n",
            "Epoch [71/200], Step [9/63] Loss: 0.56597\n",
            "Epoch [71/200], Step [10/63] Loss: 0.71396\n",
            "Epoch [71/200], Step [11/63] Loss: 0.40282\n",
            "Epoch [71/200], Step [12/63] Loss: 0.51385\n",
            "Epoch [71/200], Step [13/63] Loss: 0.78531\n",
            "Epoch [71/200], Step [14/63] Loss: 0.63144\n",
            "Epoch [71/200], Step [15/63] Loss: 0.80810\n",
            "Epoch [71/200], Step [16/63] Loss: 0.66862\n",
            "Epoch [71/200], Step [17/63] Loss: 0.71842\n",
            "Epoch [71/200], Step [18/63] Loss: 0.80072\n",
            "Epoch [71/200], Step [19/63] Loss: 0.54650\n",
            "Epoch [71/200], Step [20/63] Loss: 0.51528\n",
            "Epoch [71/200], Step [21/63] Loss: 0.68831\n",
            "Epoch [71/200], Step [22/63] Loss: 0.71502\n",
            "Epoch [71/200], Step [23/63] Loss: 0.63272\n",
            "Epoch [71/200], Step [24/63] Loss: 0.65474\n",
            "Epoch [71/200], Step [25/63] Loss: 0.60132\n",
            "Epoch [71/200], Step [26/63] Loss: 0.56971\n",
            "Epoch [71/200], Step [27/63] Loss: 0.78033\n",
            "Epoch [71/200], Step [28/63] Loss: 1.01388\n",
            "Epoch [71/200], Step [29/63] Loss: 0.93604\n",
            "Epoch [71/200], Step [30/63] Loss: 1.09656\n",
            "Epoch [71/200], Step [31/63] Loss: 0.67135\n",
            "Epoch [71/200], Step [32/63] Loss: 0.85585\n",
            "Epoch [71/200], Step [33/63] Loss: 0.65306\n",
            "Epoch [71/200], Step [34/63] Loss: 0.98167\n",
            "Epoch [71/200], Step [35/63] Loss: 0.83117\n",
            "Epoch [71/200], Step [36/63] Loss: 0.70007\n",
            "Epoch [71/200], Step [37/63] Loss: 0.62225\n",
            "Epoch [71/200], Step [38/63] Loss: 0.58026\n",
            "Epoch [71/200], Step [39/63] Loss: 0.52273\n",
            "Epoch [71/200], Step [40/63] Loss: 0.65102\n",
            "Epoch [71/200], Step [41/63] Loss: 0.82199\n",
            "Epoch [71/200], Step [42/63] Loss: 0.89665\n",
            "Epoch [71/200], Step [43/63] Loss: 0.84043\n",
            "Epoch [71/200], Step [44/63] Loss: 0.67012\n",
            "Epoch [71/200], Step [45/63] Loss: 0.86771\n",
            "Epoch [71/200], Step [46/63] Loss: 0.46925\n",
            "Epoch [71/200], Step [47/63] Loss: 0.67464\n",
            "Epoch [71/200], Step [48/63] Loss: 0.63153\n",
            "Epoch [71/200], Step [49/63] Loss: 0.73262\n",
            "Epoch [71/200], Step [50/63] Loss: 0.69767\n",
            "Epoch [71/200], Step [51/63] Loss: 0.57731\n",
            "Epoch [71/200], Step [52/63] Loss: 0.59132\n",
            "Epoch [71/200], Step [53/63] Loss: 0.65299\n",
            "Epoch [71/200], Step [54/63] Loss: 0.87908\n",
            "Epoch [71/200], Step [55/63] Loss: 0.63672\n",
            "Epoch [71/200], Step [56/63] Loss: 0.80156\n",
            "Epoch [71/200], Step [57/63] Loss: 0.69362\n",
            "Epoch [71/200], Step [58/63] Loss: 0.52862\n",
            "Epoch [71/200], Step [59/63] Loss: 0.66433\n",
            "Epoch [71/200], Step [60/63] Loss: 0.72227\n",
            "Epoch [71/200], Step [61/63] Loss: 0.78757\n",
            "Epoch [71/200], Step [62/63] Loss: 0.95951\n",
            "Epoch [71/200], Step [63/63] Loss: 0.54301\n",
            "Valid Accuracy: 56.71296296296296 %\n",
            "Epoch [72/200], Step [1/63] Loss: 0.63582\n",
            "Epoch [72/200], Step [2/63] Loss: 0.43246\n",
            "Epoch [72/200], Step [3/63] Loss: 1.04732\n",
            "Epoch [72/200], Step [4/63] Loss: 0.60711\n",
            "Epoch [72/200], Step [5/63] Loss: 0.79741\n",
            "Epoch [72/200], Step [6/63] Loss: 0.69411\n",
            "Epoch [72/200], Step [7/63] Loss: 0.97458\n",
            "Epoch [72/200], Step [8/63] Loss: 0.72592\n",
            "Epoch [72/200], Step [9/63] Loss: 0.68194\n",
            "Epoch [72/200], Step [10/63] Loss: 0.72758\n",
            "Epoch [72/200], Step [11/63] Loss: 1.04493\n",
            "Epoch [72/200], Step [12/63] Loss: 0.51408\n",
            "Epoch [72/200], Step [13/63] Loss: 0.56176\n",
            "Epoch [72/200], Step [14/63] Loss: 0.81974\n",
            "Epoch [72/200], Step [15/63] Loss: 0.84247\n",
            "Epoch [72/200], Step [16/63] Loss: 0.62861\n",
            "Epoch [72/200], Step [17/63] Loss: 0.41531\n",
            "Epoch [72/200], Step [18/63] Loss: 0.57109\n",
            "Epoch [72/200], Step [19/63] Loss: 0.57353\n",
            "Epoch [72/200], Step [20/63] Loss: 0.51476\n",
            "Epoch [72/200], Step [21/63] Loss: 0.86437\n",
            "Epoch [72/200], Step [22/63] Loss: 0.63491\n",
            "Epoch [72/200], Step [23/63] Loss: 0.44578\n",
            "Epoch [72/200], Step [24/63] Loss: 0.89667\n",
            "Epoch [72/200], Step [25/63] Loss: 0.51699\n",
            "Epoch [72/200], Step [26/63] Loss: 0.51764\n",
            "Epoch [72/200], Step [27/63] Loss: 0.55950\n",
            "Epoch [72/200], Step [28/63] Loss: 0.82887\n",
            "Epoch [72/200], Step [29/63] Loss: 0.46838\n",
            "Epoch [72/200], Step [30/63] Loss: 0.79096\n",
            "Epoch [72/200], Step [31/63] Loss: 0.50320\n",
            "Epoch [72/200], Step [32/63] Loss: 0.65725\n",
            "Epoch [72/200], Step [33/63] Loss: 0.54641\n",
            "Epoch [72/200], Step [34/63] Loss: 0.76562\n",
            "Epoch [72/200], Step [35/63] Loss: 0.63482\n",
            "Epoch [72/200], Step [36/63] Loss: 0.71609\n",
            "Epoch [72/200], Step [37/63] Loss: 0.72730\n",
            "Epoch [72/200], Step [38/63] Loss: 0.87997\n",
            "Epoch [72/200], Step [39/63] Loss: 0.60400\n",
            "Epoch [72/200], Step [40/63] Loss: 0.62198\n",
            "Epoch [72/200], Step [41/63] Loss: 0.88144\n",
            "Epoch [72/200], Step [42/63] Loss: 0.56999\n",
            "Epoch [72/200], Step [43/63] Loss: 0.83861\n",
            "Epoch [72/200], Step [44/63] Loss: 0.59343\n",
            "Epoch [72/200], Step [45/63] Loss: 0.83369\n",
            "Epoch [72/200], Step [46/63] Loss: 0.71509\n",
            "Epoch [72/200], Step [47/63] Loss: 0.61351\n",
            "Epoch [72/200], Step [48/63] Loss: 0.70518\n",
            "Epoch [72/200], Step [49/63] Loss: 0.96348\n",
            "Epoch [72/200], Step [50/63] Loss: 0.59370\n",
            "Epoch [72/200], Step [51/63] Loss: 0.67103\n",
            "Epoch [72/200], Step [52/63] Loss: 0.88516\n",
            "Epoch [72/200], Step [53/63] Loss: 0.48569\n",
            "Epoch [72/200], Step [54/63] Loss: 0.76671\n",
            "Epoch [72/200], Step [55/63] Loss: 0.74678\n",
            "Epoch [72/200], Step [56/63] Loss: 0.93966\n",
            "Epoch [72/200], Step [57/63] Loss: 0.52426\n",
            "Epoch [72/200], Step [58/63] Loss: 0.55352\n",
            "Epoch [72/200], Step [59/63] Loss: 0.73661\n",
            "Epoch [72/200], Step [60/63] Loss: 1.14767\n",
            "Epoch [72/200], Step [61/63] Loss: 0.77892\n",
            "Epoch [72/200], Step [62/63] Loss: 0.45200\n",
            "Epoch [72/200], Step [63/63] Loss: 0.64646\n",
            "Valid Accuracy: 65.74074074074075 %\n",
            "Epoch [73/200], Step [1/63] Loss: 0.84440\n",
            "Epoch [73/200], Step [2/63] Loss: 0.77479\n",
            "Epoch [73/200], Step [3/63] Loss: 0.70766\n",
            "Epoch [73/200], Step [4/63] Loss: 0.76528\n",
            "Epoch [73/200], Step [5/63] Loss: 0.67597\n",
            "Epoch [73/200], Step [6/63] Loss: 0.40910\n",
            "Epoch [73/200], Step [7/63] Loss: 0.90745\n",
            "Epoch [73/200], Step [8/63] Loss: 0.69979\n",
            "Epoch [73/200], Step [9/63] Loss: 0.59149\n",
            "Epoch [73/200], Step [10/63] Loss: 0.71623\n",
            "Epoch [73/200], Step [11/63] Loss: 0.60541\n",
            "Epoch [73/200], Step [12/63] Loss: 0.70247\n",
            "Epoch [73/200], Step [13/63] Loss: 0.46171\n",
            "Epoch [73/200], Step [14/63] Loss: 0.60676\n",
            "Epoch [73/200], Step [15/63] Loss: 1.12621\n",
            "Epoch [73/200], Step [16/63] Loss: 0.54993\n",
            "Epoch [73/200], Step [17/63] Loss: 0.80105\n",
            "Epoch [73/200], Step [18/63] Loss: 0.45195\n",
            "Epoch [73/200], Step [19/63] Loss: 0.50658\n",
            "Epoch [73/200], Step [20/63] Loss: 0.62840\n",
            "Epoch [73/200], Step [21/63] Loss: 0.50628\n",
            "Epoch [73/200], Step [22/63] Loss: 0.66352\n",
            "Epoch [73/200], Step [23/63] Loss: 0.67277\n",
            "Epoch [73/200], Step [24/63] Loss: 0.47264\n",
            "Epoch [73/200], Step [25/63] Loss: 0.74542\n",
            "Epoch [73/200], Step [26/63] Loss: 1.07285\n",
            "Epoch [73/200], Step [27/63] Loss: 0.71036\n",
            "Epoch [73/200], Step [28/63] Loss: 0.87046\n",
            "Epoch [73/200], Step [29/63] Loss: 0.63781\n",
            "Epoch [73/200], Step [30/63] Loss: 0.62432\n",
            "Epoch [73/200], Step [31/63] Loss: 0.77898\n",
            "Epoch [73/200], Step [32/63] Loss: 0.45270\n",
            "Epoch [73/200], Step [33/63] Loss: 0.68335\n",
            "Epoch [73/200], Step [34/63] Loss: 0.73268\n",
            "Epoch [73/200], Step [35/63] Loss: 0.64398\n",
            "Epoch [73/200], Step [36/63] Loss: 0.42591\n",
            "Epoch [73/200], Step [37/63] Loss: 0.81316\n",
            "Epoch [73/200], Step [38/63] Loss: 0.67342\n",
            "Epoch [73/200], Step [39/63] Loss: 0.65816\n",
            "Epoch [73/200], Step [40/63] Loss: 0.55479\n",
            "Epoch [73/200], Step [41/63] Loss: 0.82132\n",
            "Epoch [73/200], Step [42/63] Loss: 0.64583\n",
            "Epoch [73/200], Step [43/63] Loss: 0.51331\n",
            "Epoch [73/200], Step [44/63] Loss: 0.49378\n",
            "Epoch [73/200], Step [45/63] Loss: 0.77393\n",
            "Epoch [73/200], Step [46/63] Loss: 0.50811\n",
            "Epoch [73/200], Step [47/63] Loss: 0.37702\n",
            "Epoch [73/200], Step [48/63] Loss: 0.64202\n",
            "Epoch [73/200], Step [49/63] Loss: 0.81065\n",
            "Epoch [73/200], Step [50/63] Loss: 0.57428\n",
            "Epoch [73/200], Step [51/63] Loss: 0.57678\n",
            "Epoch [73/200], Step [52/63] Loss: 0.85561\n",
            "Epoch [73/200], Step [53/63] Loss: 0.75681\n",
            "Epoch [73/200], Step [54/63] Loss: 0.81478\n",
            "Epoch [73/200], Step [55/63] Loss: 0.45916\n",
            "Epoch [73/200], Step [56/63] Loss: 0.42982\n",
            "Epoch [73/200], Step [57/63] Loss: 0.55803\n",
            "Epoch [73/200], Step [58/63] Loss: 0.88366\n",
            "Epoch [73/200], Step [59/63] Loss: 0.90818\n",
            "Epoch [73/200], Step [60/63] Loss: 0.56984\n",
            "Epoch [73/200], Step [61/63] Loss: 0.57668\n",
            "Epoch [73/200], Step [62/63] Loss: 0.93539\n",
            "Epoch [73/200], Step [63/63] Loss: 0.39994\n",
            "Valid Accuracy: 61.574074074074076 %\n",
            "Epoch [74/200], Step [1/63] Loss: 0.65267\n",
            "Epoch [74/200], Step [2/63] Loss: 0.76901\n",
            "Epoch [74/200], Step [3/63] Loss: 0.72672\n",
            "Epoch [74/200], Step [4/63] Loss: 0.53733\n",
            "Epoch [74/200], Step [5/63] Loss: 0.40252\n",
            "Epoch [74/200], Step [6/63] Loss: 0.57488\n",
            "Epoch [74/200], Step [7/63] Loss: 0.62409\n",
            "Epoch [74/200], Step [8/63] Loss: 0.81596\n",
            "Epoch [74/200], Step [9/63] Loss: 0.99494\n",
            "Epoch [74/200], Step [10/63] Loss: 0.57567\n",
            "Epoch [74/200], Step [11/63] Loss: 0.48445\n",
            "Epoch [74/200], Step [12/63] Loss: 0.71648\n",
            "Epoch [74/200], Step [13/63] Loss: 0.59604\n",
            "Epoch [74/200], Step [14/63] Loss: 0.65560\n",
            "Epoch [74/200], Step [15/63] Loss: 0.41355\n",
            "Epoch [74/200], Step [16/63] Loss: 0.49728\n",
            "Epoch [74/200], Step [17/63] Loss: 0.72266\n",
            "Epoch [74/200], Step [18/63] Loss: 0.52476\n",
            "Epoch [74/200], Step [19/63] Loss: 0.67730\n",
            "Epoch [74/200], Step [20/63] Loss: 0.56399\n",
            "Epoch [74/200], Step [21/63] Loss: 0.83680\n",
            "Epoch [74/200], Step [22/63] Loss: 0.69726\n",
            "Epoch [74/200], Step [23/63] Loss: 0.68875\n",
            "Epoch [74/200], Step [24/63] Loss: 0.57224\n",
            "Epoch [74/200], Step [25/63] Loss: 0.78867\n",
            "Epoch [74/200], Step [26/63] Loss: 0.90649\n",
            "Epoch [74/200], Step [27/63] Loss: 0.55354\n",
            "Epoch [74/200], Step [28/63] Loss: 0.79075\n",
            "Epoch [74/200], Step [29/63] Loss: 0.65373\n",
            "Epoch [74/200], Step [30/63] Loss: 0.85791\n",
            "Epoch [74/200], Step [31/63] Loss: 0.56380\n",
            "Epoch [74/200], Step [32/63] Loss: 0.74784\n",
            "Epoch [74/200], Step [33/63] Loss: 0.42203\n",
            "Epoch [74/200], Step [34/63] Loss: 0.40855\n",
            "Epoch [74/200], Step [35/63] Loss: 1.06409\n",
            "Epoch [74/200], Step [36/63] Loss: 0.56981\n",
            "Epoch [74/200], Step [37/63] Loss: 0.89789\n",
            "Epoch [74/200], Step [38/63] Loss: 0.58197\n",
            "Epoch [74/200], Step [39/63] Loss: 0.54523\n",
            "Epoch [74/200], Step [40/63] Loss: 0.64952\n",
            "Epoch [74/200], Step [41/63] Loss: 0.60724\n",
            "Epoch [74/200], Step [42/63] Loss: 0.82454\n",
            "Epoch [74/200], Step [43/63] Loss: 0.94473\n",
            "Epoch [74/200], Step [44/63] Loss: 0.98100\n",
            "Epoch [74/200], Step [45/63] Loss: 0.92120\n",
            "Epoch [74/200], Step [46/63] Loss: 1.00899\n",
            "Epoch [74/200], Step [47/63] Loss: 0.48782\n",
            "Epoch [74/200], Step [48/63] Loss: 0.52870\n",
            "Epoch [74/200], Step [49/63] Loss: 0.79850\n",
            "Epoch [74/200], Step [50/63] Loss: 0.70048\n",
            "Epoch [74/200], Step [51/63] Loss: 0.64833\n",
            "Epoch [74/200], Step [52/63] Loss: 0.66518\n",
            "Epoch [74/200], Step [53/63] Loss: 0.71841\n",
            "Epoch [74/200], Step [54/63] Loss: 0.64370\n",
            "Epoch [74/200], Step [55/63] Loss: 0.45249\n",
            "Epoch [74/200], Step [56/63] Loss: 0.60606\n",
            "Epoch [74/200], Step [57/63] Loss: 1.12739\n",
            "Epoch [74/200], Step [58/63] Loss: 0.45962\n",
            "Epoch [74/200], Step [59/63] Loss: 1.22778\n",
            "Epoch [74/200], Step [60/63] Loss: 0.75095\n",
            "Epoch [74/200], Step [61/63] Loss: 1.08518\n",
            "Epoch [74/200], Step [62/63] Loss: 0.78601\n",
            "Epoch [74/200], Step [63/63] Loss: 0.52849\n",
            "Valid Accuracy: 57.638888888888886 %\n",
            "Epoch [75/200], Step [1/63] Loss: 0.46020\n",
            "Epoch [75/200], Step [2/63] Loss: 0.74978\n",
            "Epoch [75/200], Step [3/63] Loss: 0.56666\n",
            "Epoch [75/200], Step [4/63] Loss: 0.76055\n",
            "Epoch [75/200], Step [5/63] Loss: 0.86525\n",
            "Epoch [75/200], Step [6/63] Loss: 0.49905\n",
            "Epoch [75/200], Step [7/63] Loss: 0.90052\n",
            "Epoch [75/200], Step [8/63] Loss: 0.48259\n",
            "Epoch [75/200], Step [9/63] Loss: 0.96317\n",
            "Epoch [75/200], Step [10/63] Loss: 0.54497\n",
            "Epoch [75/200], Step [11/63] Loss: 0.55933\n",
            "Epoch [75/200], Step [12/63] Loss: 0.47075\n",
            "Epoch [75/200], Step [13/63] Loss: 0.43748\n",
            "Epoch [75/200], Step [14/63] Loss: 0.59289\n",
            "Epoch [75/200], Step [15/63] Loss: 0.50985\n",
            "Epoch [75/200], Step [16/63] Loss: 0.28832\n",
            "Epoch [75/200], Step [17/63] Loss: 0.63021\n",
            "Epoch [75/200], Step [18/63] Loss: 0.77503\n",
            "Epoch [75/200], Step [19/63] Loss: 0.63610\n",
            "Epoch [75/200], Step [20/63] Loss: 0.82801\n",
            "Epoch [75/200], Step [21/63] Loss: 0.62726\n",
            "Epoch [75/200], Step [22/63] Loss: 0.68219\n",
            "Epoch [75/200], Step [23/63] Loss: 0.58324\n",
            "Epoch [75/200], Step [24/63] Loss: 0.55383\n",
            "Epoch [75/200], Step [25/63] Loss: 0.67397\n",
            "Epoch [75/200], Step [26/63] Loss: 0.57994\n",
            "Epoch [75/200], Step [27/63] Loss: 0.77116\n",
            "Epoch [75/200], Step [28/63] Loss: 0.70338\n",
            "Epoch [75/200], Step [29/63] Loss: 0.73206\n",
            "Epoch [75/200], Step [30/63] Loss: 0.66676\n",
            "Epoch [75/200], Step [31/63] Loss: 0.87562\n",
            "Epoch [75/200], Step [32/63] Loss: 0.65090\n",
            "Epoch [75/200], Step [33/63] Loss: 0.39509\n",
            "Epoch [75/200], Step [34/63] Loss: 0.69395\n",
            "Epoch [75/200], Step [35/63] Loss: 0.74911\n",
            "Epoch [75/200], Step [36/63] Loss: 0.61349\n",
            "Epoch [75/200], Step [37/63] Loss: 0.66012\n",
            "Epoch [75/200], Step [38/63] Loss: 0.51317\n",
            "Epoch [75/200], Step [39/63] Loss: 0.66188\n",
            "Epoch [75/200], Step [40/63] Loss: 0.58265\n",
            "Epoch [75/200], Step [41/63] Loss: 0.65844\n",
            "Epoch [75/200], Step [42/63] Loss: 0.72398\n",
            "Epoch [75/200], Step [43/63] Loss: 0.65212\n",
            "Epoch [75/200], Step [44/63] Loss: 0.68607\n",
            "Epoch [75/200], Step [45/63] Loss: 0.47948\n",
            "Epoch [75/200], Step [46/63] Loss: 0.81145\n",
            "Epoch [75/200], Step [47/63] Loss: 0.88311\n",
            "Epoch [75/200], Step [48/63] Loss: 0.66271\n",
            "Epoch [75/200], Step [49/63] Loss: 0.84772\n",
            "Epoch [75/200], Step [50/63] Loss: 0.79194\n",
            "Epoch [75/200], Step [51/63] Loss: 0.66579\n",
            "Epoch [75/200], Step [52/63] Loss: 0.98879\n",
            "Epoch [75/200], Step [53/63] Loss: 0.60622\n",
            "Epoch [75/200], Step [54/63] Loss: 0.82210\n",
            "Epoch [75/200], Step [55/63] Loss: 0.75577\n",
            "Epoch [75/200], Step [56/63] Loss: 0.48530\n",
            "Epoch [75/200], Step [57/63] Loss: 0.82739\n",
            "Epoch [75/200], Step [58/63] Loss: 0.46433\n",
            "Epoch [75/200], Step [59/63] Loss: 1.11978\n",
            "Epoch [75/200], Step [60/63] Loss: 0.53210\n",
            "Epoch [75/200], Step [61/63] Loss: 0.66779\n",
            "Epoch [75/200], Step [62/63] Loss: 0.75774\n",
            "Epoch [75/200], Step [63/63] Loss: 0.98227\n",
            "Valid Accuracy: 50.0 %\n",
            "Epoch [76/200], Step [1/63] Loss: 0.90649\n",
            "Epoch [76/200], Step [2/63] Loss: 0.48892\n",
            "Epoch [76/200], Step [3/63] Loss: 0.62082\n",
            "Epoch [76/200], Step [4/63] Loss: 0.57503\n",
            "Epoch [76/200], Step [5/63] Loss: 0.49896\n",
            "Epoch [76/200], Step [6/63] Loss: 0.57433\n",
            "Epoch [76/200], Step [7/63] Loss: 0.70856\n",
            "Epoch [76/200], Step [8/63] Loss: 0.70933\n",
            "Epoch [76/200], Step [9/63] Loss: 0.70165\n",
            "Epoch [76/200], Step [10/63] Loss: 0.54856\n",
            "Epoch [76/200], Step [11/63] Loss: 0.54871\n",
            "Epoch [76/200], Step [12/63] Loss: 0.78989\n",
            "Epoch [76/200], Step [13/63] Loss: 0.58279\n",
            "Epoch [76/200], Step [14/63] Loss: 0.79228\n",
            "Epoch [76/200], Step [15/63] Loss: 0.70069\n",
            "Epoch [76/200], Step [16/63] Loss: 0.40370\n",
            "Epoch [76/200], Step [17/63] Loss: 0.48356\n",
            "Epoch [76/200], Step [18/63] Loss: 0.84720\n",
            "Epoch [76/200], Step [19/63] Loss: 0.66578\n",
            "Epoch [76/200], Step [20/63] Loss: 0.80113\n",
            "Epoch [76/200], Step [21/63] Loss: 0.89041\n",
            "Epoch [76/200], Step [22/63] Loss: 0.77794\n",
            "Epoch [76/200], Step [23/63] Loss: 0.73285\n",
            "Epoch [76/200], Step [24/63] Loss: 0.67383\n",
            "Epoch [76/200], Step [25/63] Loss: 0.66372\n",
            "Epoch [76/200], Step [26/63] Loss: 0.73780\n",
            "Epoch [76/200], Step [27/63] Loss: 0.83819\n",
            "Epoch [76/200], Step [28/63] Loss: 0.93296\n",
            "Epoch [76/200], Step [29/63] Loss: 0.82508\n",
            "Epoch [76/200], Step [30/63] Loss: 0.62936\n",
            "Epoch [76/200], Step [31/63] Loss: 0.61587\n",
            "Epoch [76/200], Step [32/63] Loss: 0.58386\n",
            "Epoch [76/200], Step [33/63] Loss: 0.64600\n",
            "Epoch [76/200], Step [34/63] Loss: 0.52078\n",
            "Epoch [76/200], Step [35/63] Loss: 0.89588\n",
            "Epoch [76/200], Step [36/63] Loss: 0.77037\n",
            "Epoch [76/200], Step [37/63] Loss: 0.48119\n",
            "Epoch [76/200], Step [38/63] Loss: 0.73430\n",
            "Epoch [76/200], Step [39/63] Loss: 0.97992\n",
            "Epoch [76/200], Step [40/63] Loss: 0.56179\n",
            "Epoch [76/200], Step [41/63] Loss: 0.82086\n",
            "Epoch [76/200], Step [42/63] Loss: 0.67045\n",
            "Epoch [76/200], Step [43/63] Loss: 0.52403\n",
            "Epoch [76/200], Step [44/63] Loss: 0.92597\n",
            "Epoch [76/200], Step [45/63] Loss: 0.82126\n",
            "Epoch [76/200], Step [46/63] Loss: 0.76659\n",
            "Epoch [76/200], Step [47/63] Loss: 0.97354\n",
            "Epoch [76/200], Step [48/63] Loss: 0.56313\n",
            "Epoch [76/200], Step [49/63] Loss: 0.69631\n",
            "Epoch [76/200], Step [50/63] Loss: 0.54785\n",
            "Epoch [76/200], Step [51/63] Loss: 0.53778\n",
            "Epoch [76/200], Step [52/63] Loss: 0.54317\n",
            "Epoch [76/200], Step [53/63] Loss: 0.56322\n",
            "Epoch [76/200], Step [54/63] Loss: 0.74138\n",
            "Epoch [76/200], Step [55/63] Loss: 0.50110\n",
            "Epoch [76/200], Step [56/63] Loss: 0.58522\n",
            "Epoch [76/200], Step [57/63] Loss: 0.49726\n",
            "Epoch [76/200], Step [58/63] Loss: 0.90935\n",
            "Epoch [76/200], Step [59/63] Loss: 0.55520\n",
            "Epoch [76/200], Step [60/63] Loss: 0.61351\n",
            "Epoch [76/200], Step [61/63] Loss: 0.70742\n",
            "Epoch [76/200], Step [62/63] Loss: 0.53113\n",
            "Epoch [76/200], Step [63/63] Loss: 1.11240\n",
            "Valid Accuracy: 66.20370370370371 %\n",
            "Epoch [77/200], Step [1/63] Loss: 0.72142\n",
            "Epoch [77/200], Step [2/63] Loss: 0.63764\n",
            "Epoch [77/200], Step [3/63] Loss: 0.63325\n",
            "Epoch [77/200], Step [4/63] Loss: 0.48906\n",
            "Epoch [77/200], Step [5/63] Loss: 0.52694\n",
            "Epoch [77/200], Step [6/63] Loss: 0.39362\n",
            "Epoch [77/200], Step [7/63] Loss: 0.39397\n",
            "Epoch [77/200], Step [8/63] Loss: 0.71047\n",
            "Epoch [77/200], Step [9/63] Loss: 0.61434\n",
            "Epoch [77/200], Step [10/63] Loss: 0.72627\n",
            "Epoch [77/200], Step [11/63] Loss: 0.52363\n",
            "Epoch [77/200], Step [12/63] Loss: 0.60193\n",
            "Epoch [77/200], Step [13/63] Loss: 0.33347\n",
            "Epoch [77/200], Step [14/63] Loss: 0.64705\n",
            "Epoch [77/200], Step [15/63] Loss: 0.81184\n",
            "Epoch [77/200], Step [16/63] Loss: 0.76231\n",
            "Epoch [77/200], Step [17/63] Loss: 0.53057\n",
            "Epoch [77/200], Step [18/63] Loss: 0.90900\n",
            "Epoch [77/200], Step [19/63] Loss: 0.51831\n",
            "Epoch [77/200], Step [20/63] Loss: 0.78455\n",
            "Epoch [77/200], Step [21/63] Loss: 0.86657\n",
            "Epoch [77/200], Step [22/63] Loss: 0.62479\n",
            "Epoch [77/200], Step [23/63] Loss: 0.61638\n",
            "Epoch [77/200], Step [24/63] Loss: 0.69594\n",
            "Epoch [77/200], Step [25/63] Loss: 0.56399\n",
            "Epoch [77/200], Step [26/63] Loss: 0.76684\n",
            "Epoch [77/200], Step [27/63] Loss: 0.71868\n",
            "Epoch [77/200], Step [28/63] Loss: 0.43168\n",
            "Epoch [77/200], Step [29/63] Loss: 0.65681\n",
            "Epoch [77/200], Step [30/63] Loss: 0.51069\n",
            "Epoch [77/200], Step [31/63] Loss: 0.95929\n",
            "Epoch [77/200], Step [32/63] Loss: 0.68624\n",
            "Epoch [77/200], Step [33/63] Loss: 0.42696\n",
            "Epoch [77/200], Step [34/63] Loss: 0.89377\n",
            "Epoch [77/200], Step [35/63] Loss: 0.93542\n",
            "Epoch [77/200], Step [36/63] Loss: 0.62466\n",
            "Epoch [77/200], Step [37/63] Loss: 0.53239\n",
            "Epoch [77/200], Step [38/63] Loss: 0.84215\n",
            "Epoch [77/200], Step [39/63] Loss: 0.80115\n",
            "Epoch [77/200], Step [40/63] Loss: 0.82197\n",
            "Epoch [77/200], Step [41/63] Loss: 0.75433\n",
            "Epoch [77/200], Step [42/63] Loss: 0.50097\n",
            "Epoch [77/200], Step [43/63] Loss: 0.86314\n",
            "Epoch [77/200], Step [44/63] Loss: 0.90186\n",
            "Epoch [77/200], Step [45/63] Loss: 0.84966\n",
            "Epoch [77/200], Step [46/63] Loss: 0.45576\n",
            "Epoch [77/200], Step [47/63] Loss: 0.72941\n",
            "Epoch [77/200], Step [48/63] Loss: 0.70292\n",
            "Epoch [77/200], Step [49/63] Loss: 0.75080\n",
            "Epoch [77/200], Step [50/63] Loss: 0.66799\n",
            "Epoch [77/200], Step [51/63] Loss: 0.51037\n",
            "Epoch [77/200], Step [52/63] Loss: 0.66649\n",
            "Epoch [77/200], Step [53/63] Loss: 0.60205\n",
            "Epoch [77/200], Step [54/63] Loss: 0.49420\n",
            "Epoch [77/200], Step [55/63] Loss: 0.94563\n",
            "Epoch [77/200], Step [56/63] Loss: 0.63145\n",
            "Epoch [77/200], Step [57/63] Loss: 0.67530\n",
            "Epoch [77/200], Step [58/63] Loss: 0.62723\n",
            "Epoch [77/200], Step [59/63] Loss: 0.65751\n",
            "Epoch [77/200], Step [60/63] Loss: 0.58689\n",
            "Epoch [77/200], Step [61/63] Loss: 0.44817\n",
            "Epoch [77/200], Step [62/63] Loss: 0.58424\n",
            "Epoch [77/200], Step [63/63] Loss: 0.58755\n",
            "Valid Accuracy: 59.49074074074074 %\n",
            "Epoch [78/200], Step [1/63] Loss: 0.57076\n",
            "Epoch [78/200], Step [2/63] Loss: 0.81199\n",
            "Epoch [78/200], Step [3/63] Loss: 0.53784\n",
            "Epoch [78/200], Step [4/63] Loss: 1.08354\n",
            "Epoch [78/200], Step [5/63] Loss: 0.61401\n",
            "Epoch [78/200], Step [6/63] Loss: 0.71473\n",
            "Epoch [78/200], Step [7/63] Loss: 0.67355\n",
            "Epoch [78/200], Step [8/63] Loss: 0.70457\n",
            "Epoch [78/200], Step [9/63] Loss: 0.81115\n",
            "Epoch [78/200], Step [10/63] Loss: 0.56505\n",
            "Epoch [78/200], Step [11/63] Loss: 0.61007\n",
            "Epoch [78/200], Step [12/63] Loss: 0.36213\n",
            "Epoch [78/200], Step [13/63] Loss: 0.64202\n",
            "Epoch [78/200], Step [14/63] Loss: 0.50275\n",
            "Epoch [78/200], Step [15/63] Loss: 0.93617\n",
            "Epoch [78/200], Step [16/63] Loss: 0.52928\n",
            "Epoch [78/200], Step [17/63] Loss: 0.52656\n",
            "Epoch [78/200], Step [18/63] Loss: 0.66724\n",
            "Epoch [78/200], Step [19/63] Loss: 0.36877\n",
            "Epoch [78/200], Step [20/63] Loss: 0.76420\n",
            "Epoch [78/200], Step [21/63] Loss: 0.49255\n",
            "Epoch [78/200], Step [22/63] Loss: 0.76093\n",
            "Epoch [78/200], Step [23/63] Loss: 0.50944\n",
            "Epoch [78/200], Step [24/63] Loss: 0.79404\n",
            "Epoch [78/200], Step [25/63] Loss: 0.48317\n",
            "Epoch [78/200], Step [26/63] Loss: 0.61657\n",
            "Epoch [78/200], Step [27/63] Loss: 0.47025\n",
            "Epoch [78/200], Step [28/63] Loss: 0.81530\n",
            "Epoch [78/200], Step [29/63] Loss: 0.79596\n",
            "Epoch [78/200], Step [30/63] Loss: 0.74554\n",
            "Epoch [78/200], Step [31/63] Loss: 0.77095\n",
            "Epoch [78/200], Step [32/63] Loss: 0.86090\n",
            "Epoch [78/200], Step [33/63] Loss: 0.61354\n",
            "Epoch [78/200], Step [34/63] Loss: 0.71995\n",
            "Epoch [78/200], Step [35/63] Loss: 0.65137\n",
            "Epoch [78/200], Step [36/63] Loss: 0.64269\n",
            "Epoch [78/200], Step [37/63] Loss: 0.57746\n",
            "Epoch [78/200], Step [38/63] Loss: 0.69253\n",
            "Epoch [78/200], Step [39/63] Loss: 0.78464\n",
            "Epoch [78/200], Step [40/63] Loss: 0.54071\n",
            "Epoch [78/200], Step [41/63] Loss: 0.64829\n",
            "Epoch [78/200], Step [42/63] Loss: 0.54857\n",
            "Epoch [78/200], Step [43/63] Loss: 0.53083\n",
            "Epoch [78/200], Step [44/63] Loss: 0.85052\n",
            "Epoch [78/200], Step [45/63] Loss: 0.72353\n",
            "Epoch [78/200], Step [46/63] Loss: 0.34721\n",
            "Epoch [78/200], Step [47/63] Loss: 0.76240\n",
            "Epoch [78/200], Step [48/63] Loss: 0.69220\n",
            "Epoch [78/200], Step [49/63] Loss: 0.95233\n",
            "Epoch [78/200], Step [50/63] Loss: 0.72178\n",
            "Epoch [78/200], Step [51/63] Loss: 0.75340\n",
            "Epoch [78/200], Step [52/63] Loss: 0.47862\n",
            "Epoch [78/200], Step [53/63] Loss: 0.71856\n",
            "Epoch [78/200], Step [54/63] Loss: 0.57139\n",
            "Epoch [78/200], Step [55/63] Loss: 0.76455\n",
            "Epoch [78/200], Step [56/63] Loss: 0.53438\n",
            "Epoch [78/200], Step [57/63] Loss: 0.53969\n",
            "Epoch [78/200], Step [58/63] Loss: 0.91277\n",
            "Epoch [78/200], Step [59/63] Loss: 0.44232\n",
            "Epoch [78/200], Step [60/63] Loss: 0.84992\n",
            "Epoch [78/200], Step [61/63] Loss: 0.45416\n",
            "Epoch [78/200], Step [62/63] Loss: 0.46772\n",
            "Epoch [78/200], Step [63/63] Loss: 0.86815\n",
            "Valid Accuracy: 67.36111111111111 %\n",
            "Epoch [79/200], Step [1/63] Loss: 0.90316\n",
            "Epoch [79/200], Step [2/63] Loss: 0.42500\n",
            "Epoch [79/200], Step [3/63] Loss: 0.68607\n",
            "Epoch [79/200], Step [4/63] Loss: 0.70716\n",
            "Epoch [79/200], Step [5/63] Loss: 0.57586\n",
            "Epoch [79/200], Step [6/63] Loss: 0.41027\n",
            "Epoch [79/200], Step [7/63] Loss: 0.74221\n",
            "Epoch [79/200], Step [8/63] Loss: 0.96290\n",
            "Epoch [79/200], Step [9/63] Loss: 0.66336\n",
            "Epoch [79/200], Step [10/63] Loss: 0.63806\n",
            "Epoch [79/200], Step [11/63] Loss: 0.80073\n",
            "Epoch [79/200], Step [12/63] Loss: 0.48897\n",
            "Epoch [79/200], Step [13/63] Loss: 0.58900\n",
            "Epoch [79/200], Step [14/63] Loss: 0.64168\n",
            "Epoch [79/200], Step [15/63] Loss: 0.74794\n",
            "Epoch [79/200], Step [16/63] Loss: 0.73193\n",
            "Epoch [79/200], Step [17/63] Loss: 0.56762\n",
            "Epoch [79/200], Step [18/63] Loss: 0.59569\n",
            "Epoch [79/200], Step [19/63] Loss: 1.00040\n",
            "Epoch [79/200], Step [20/63] Loss: 0.93961\n",
            "Epoch [79/200], Step [21/63] Loss: 0.42386\n",
            "Epoch [79/200], Step [22/63] Loss: 0.48052\n",
            "Epoch [79/200], Step [23/63] Loss: 0.75223\n",
            "Epoch [79/200], Step [24/63] Loss: 0.77807\n",
            "Epoch [79/200], Step [25/63] Loss: 0.72029\n",
            "Epoch [79/200], Step [26/63] Loss: 0.55699\n",
            "Epoch [79/200], Step [27/63] Loss: 0.58940\n",
            "Epoch [79/200], Step [28/63] Loss: 0.52793\n",
            "Epoch [79/200], Step [29/63] Loss: 0.82184\n",
            "Epoch [79/200], Step [30/63] Loss: 0.53642\n",
            "Epoch [79/200], Step [31/63] Loss: 0.57136\n",
            "Epoch [79/200], Step [32/63] Loss: 0.64233\n",
            "Epoch [79/200], Step [33/63] Loss: 0.53180\n",
            "Epoch [79/200], Step [34/63] Loss: 0.48764\n",
            "Epoch [79/200], Step [35/63] Loss: 0.93296\n",
            "Epoch [79/200], Step [36/63] Loss: 0.99112\n",
            "Epoch [79/200], Step [37/63] Loss: 0.46194\n",
            "Epoch [79/200], Step [38/63] Loss: 0.53940\n",
            "Epoch [79/200], Step [39/63] Loss: 0.92728\n",
            "Epoch [79/200], Step [40/63] Loss: 0.56072\n",
            "Epoch [79/200], Step [41/63] Loss: 1.14420\n",
            "Epoch [79/200], Step [42/63] Loss: 0.87001\n",
            "Epoch [79/200], Step [43/63] Loss: 0.86615\n",
            "Epoch [79/200], Step [44/63] Loss: 0.66046\n",
            "Epoch [79/200], Step [45/63] Loss: 0.67164\n",
            "Epoch [79/200], Step [46/63] Loss: 0.62437\n",
            "Epoch [79/200], Step [47/63] Loss: 0.45753\n",
            "Epoch [79/200], Step [48/63] Loss: 0.41730\n",
            "Epoch [79/200], Step [49/63] Loss: 0.50499\n",
            "Epoch [79/200], Step [50/63] Loss: 0.74773\n",
            "Epoch [79/200], Step [51/63] Loss: 1.10571\n",
            "Epoch [79/200], Step [52/63] Loss: 0.41538\n",
            "Epoch [79/200], Step [53/63] Loss: 0.79485\n",
            "Epoch [79/200], Step [54/63] Loss: 0.64746\n",
            "Epoch [79/200], Step [55/63] Loss: 0.81799\n",
            "Epoch [79/200], Step [56/63] Loss: 0.45368\n",
            "Epoch [79/200], Step [57/63] Loss: 0.73519\n",
            "Epoch [79/200], Step [58/63] Loss: 0.63697\n",
            "Epoch [79/200], Step [59/63] Loss: 0.87613\n",
            "Epoch [79/200], Step [60/63] Loss: 0.71156\n",
            "Epoch [79/200], Step [61/63] Loss: 0.79087\n",
            "Epoch [79/200], Step [62/63] Loss: 0.76694\n",
            "Epoch [79/200], Step [63/63] Loss: 0.59631\n",
            "Valid Accuracy: 57.638888888888886 %\n",
            "Epoch [80/200], Step [1/63] Loss: 0.52499\n",
            "Epoch [80/200], Step [2/63] Loss: 0.52467\n",
            "Epoch [80/200], Step [3/63] Loss: 0.52806\n",
            "Epoch [80/200], Step [4/63] Loss: 1.07150\n",
            "Epoch [80/200], Step [5/63] Loss: 0.75828\n",
            "Epoch [80/200], Step [6/63] Loss: 0.54328\n",
            "Epoch [80/200], Step [7/63] Loss: 0.76749\n",
            "Epoch [80/200], Step [8/63] Loss: 0.96568\n",
            "Epoch [80/200], Step [9/63] Loss: 0.81032\n",
            "Epoch [80/200], Step [10/63] Loss: 0.82308\n",
            "Epoch [80/200], Step [11/63] Loss: 0.81435\n",
            "Epoch [80/200], Step [12/63] Loss: 0.61143\n",
            "Epoch [80/200], Step [13/63] Loss: 0.62608\n",
            "Epoch [80/200], Step [14/63] Loss: 0.78474\n",
            "Epoch [80/200], Step [15/63] Loss: 0.66090\n",
            "Epoch [80/200], Step [16/63] Loss: 0.72594\n",
            "Epoch [80/200], Step [17/63] Loss: 0.68935\n",
            "Epoch [80/200], Step [18/63] Loss: 0.60932\n",
            "Epoch [80/200], Step [19/63] Loss: 0.60918\n",
            "Epoch [80/200], Step [20/63] Loss: 0.84808\n",
            "Epoch [80/200], Step [21/63] Loss: 0.69996\n",
            "Epoch [80/200], Step [22/63] Loss: 0.93127\n",
            "Epoch [80/200], Step [23/63] Loss: 0.57096\n",
            "Epoch [80/200], Step [24/63] Loss: 0.79061\n",
            "Epoch [80/200], Step [25/63] Loss: 0.46856\n",
            "Epoch [80/200], Step [26/63] Loss: 0.67289\n",
            "Epoch [80/200], Step [27/63] Loss: 0.72329\n",
            "Epoch [80/200], Step [28/63] Loss: 0.69094\n",
            "Epoch [80/200], Step [29/63] Loss: 0.76605\n",
            "Epoch [80/200], Step [30/63] Loss: 0.76520\n",
            "Epoch [80/200], Step [31/63] Loss: 0.51253\n",
            "Epoch [80/200], Step [32/63] Loss: 0.75588\n",
            "Epoch [80/200], Step [33/63] Loss: 0.38420\n",
            "Epoch [80/200], Step [34/63] Loss: 0.77217\n",
            "Epoch [80/200], Step [35/63] Loss: 0.88296\n",
            "Epoch [80/200], Step [36/63] Loss: 0.31720\n",
            "Epoch [80/200], Step [37/63] Loss: 0.96090\n",
            "Epoch [80/200], Step [38/63] Loss: 0.44876\n",
            "Epoch [80/200], Step [39/63] Loss: 0.82763\n",
            "Epoch [80/200], Step [40/63] Loss: 0.91610\n",
            "Epoch [80/200], Step [41/63] Loss: 0.85284\n",
            "Epoch [80/200], Step [42/63] Loss: 0.46828\n",
            "Epoch [80/200], Step [43/63] Loss: 0.55188\n",
            "Epoch [80/200], Step [44/63] Loss: 0.91653\n",
            "Epoch [80/200], Step [45/63] Loss: 0.38947\n",
            "Epoch [80/200], Step [46/63] Loss: 0.47474\n",
            "Epoch [80/200], Step [47/63] Loss: 0.71651\n",
            "Epoch [80/200], Step [48/63] Loss: 0.79749\n",
            "Epoch [80/200], Step [49/63] Loss: 0.63319\n",
            "Epoch [80/200], Step [50/63] Loss: 0.45672\n",
            "Epoch [80/200], Step [51/63] Loss: 0.54248\n",
            "Epoch [80/200], Step [52/63] Loss: 0.72797\n",
            "Epoch [80/200], Step [53/63] Loss: 0.77368\n",
            "Epoch [80/200], Step [54/63] Loss: 0.68575\n",
            "Epoch [80/200], Step [55/63] Loss: 0.58031\n",
            "Epoch [80/200], Step [56/63] Loss: 0.68516\n",
            "Epoch [80/200], Step [57/63] Loss: 0.72584\n",
            "Epoch [80/200], Step [58/63] Loss: 0.56827\n",
            "Epoch [80/200], Step [59/63] Loss: 0.80864\n",
            "Epoch [80/200], Step [60/63] Loss: 0.48022\n",
            "Epoch [80/200], Step [61/63] Loss: 0.56869\n",
            "Epoch [80/200], Step [62/63] Loss: 0.55822\n",
            "Epoch [80/200], Step [63/63] Loss: 0.44770\n",
            "Valid Accuracy: 66.20370370370371 %\n",
            "Epoch [81/200], Step [1/63] Loss: 0.56446\n",
            "Epoch [81/200], Step [2/63] Loss: 0.52626\n",
            "Epoch [81/200], Step [3/63] Loss: 0.50933\n",
            "Epoch [81/200], Step [4/63] Loss: 0.72107\n",
            "Epoch [81/200], Step [5/63] Loss: 0.56048\n",
            "Epoch [81/200], Step [6/63] Loss: 0.94977\n",
            "Epoch [81/200], Step [7/63] Loss: 0.53556\n",
            "Epoch [81/200], Step [8/63] Loss: 0.57127\n",
            "Epoch [81/200], Step [9/63] Loss: 0.59461\n",
            "Epoch [81/200], Step [10/63] Loss: 0.76513\n",
            "Epoch [81/200], Step [11/63] Loss: 0.86076\n",
            "Epoch [81/200], Step [12/63] Loss: 0.64350\n",
            "Epoch [81/200], Step [13/63] Loss: 0.52928\n",
            "Epoch [81/200], Step [14/63] Loss: 0.53737\n",
            "Epoch [81/200], Step [15/63] Loss: 0.57225\n",
            "Epoch [81/200], Step [16/63] Loss: 0.71547\n",
            "Epoch [81/200], Step [17/63] Loss: 0.62831\n",
            "Epoch [81/200], Step [18/63] Loss: 0.53111\n",
            "Epoch [81/200], Step [19/63] Loss: 0.74267\n",
            "Epoch [81/200], Step [20/63] Loss: 0.82009\n",
            "Epoch [81/200], Step [21/63] Loss: 0.60897\n",
            "Epoch [81/200], Step [22/63] Loss: 0.81130\n",
            "Epoch [81/200], Step [23/63] Loss: 0.62624\n",
            "Epoch [81/200], Step [24/63] Loss: 0.72202\n",
            "Epoch [81/200], Step [25/63] Loss: 0.48832\n",
            "Epoch [81/200], Step [26/63] Loss: 0.65917\n",
            "Epoch [81/200], Step [27/63] Loss: 0.69752\n",
            "Epoch [81/200], Step [28/63] Loss: 0.55402\n",
            "Epoch [81/200], Step [29/63] Loss: 0.41556\n",
            "Epoch [81/200], Step [30/63] Loss: 0.63868\n",
            "Epoch [81/200], Step [31/63] Loss: 1.01407\n",
            "Epoch [81/200], Step [32/63] Loss: 0.45084\n",
            "Epoch [81/200], Step [33/63] Loss: 0.59284\n",
            "Epoch [81/200], Step [34/63] Loss: 0.49333\n",
            "Epoch [81/200], Step [35/63] Loss: 0.61111\n",
            "Epoch [81/200], Step [36/63] Loss: 0.85428\n",
            "Epoch [81/200], Step [37/63] Loss: 0.73521\n",
            "Epoch [81/200], Step [38/63] Loss: 0.53361\n",
            "Epoch [81/200], Step [39/63] Loss: 0.71531\n",
            "Epoch [81/200], Step [40/63] Loss: 0.36154\n",
            "Epoch [81/200], Step [41/63] Loss: 0.96461\n",
            "Epoch [81/200], Step [42/63] Loss: 0.78156\n",
            "Epoch [81/200], Step [43/63] Loss: 0.46954\n",
            "Epoch [81/200], Step [44/63] Loss: 0.60459\n",
            "Epoch [81/200], Step [45/63] Loss: 0.76223\n",
            "Epoch [81/200], Step [46/63] Loss: 0.46974\n",
            "Epoch [81/200], Step [47/63] Loss: 0.49861\n",
            "Epoch [81/200], Step [48/63] Loss: 1.06956\n",
            "Epoch [81/200], Step [49/63] Loss: 0.40221\n",
            "Epoch [81/200], Step [50/63] Loss: 0.89596\n",
            "Epoch [81/200], Step [51/63] Loss: 0.34449\n",
            "Epoch [81/200], Step [52/63] Loss: 0.67660\n",
            "Epoch [81/200], Step [53/63] Loss: 0.57841\n",
            "Epoch [81/200], Step [54/63] Loss: 0.87352\n",
            "Epoch [81/200], Step [55/63] Loss: 0.74839\n",
            "Epoch [81/200], Step [56/63] Loss: 0.98099\n",
            "Epoch [81/200], Step [57/63] Loss: 0.67600\n",
            "Epoch [81/200], Step [58/63] Loss: 0.90040\n",
            "Epoch [81/200], Step [59/63] Loss: 0.55737\n",
            "Epoch [81/200], Step [60/63] Loss: 0.79892\n",
            "Epoch [81/200], Step [61/63] Loss: 0.61371\n",
            "Epoch [81/200], Step [62/63] Loss: 0.69601\n",
            "Epoch [81/200], Step [63/63] Loss: 0.58235\n",
            "Valid Accuracy: 69.9074074074074 %\n",
            "Epoch [82/200], Step [1/63] Loss: 0.43445\n",
            "Epoch [82/200], Step [2/63] Loss: 0.59432\n",
            "Epoch [82/200], Step [3/63] Loss: 0.62181\n",
            "Epoch [82/200], Step [4/63] Loss: 0.81097\n",
            "Epoch [82/200], Step [5/63] Loss: 0.50910\n",
            "Epoch [82/200], Step [6/63] Loss: 0.50097\n",
            "Epoch [82/200], Step [7/63] Loss: 0.75061\n",
            "Epoch [82/200], Step [8/63] Loss: 0.43136\n",
            "Epoch [82/200], Step [9/63] Loss: 0.60982\n",
            "Epoch [82/200], Step [10/63] Loss: 0.84972\n",
            "Epoch [82/200], Step [11/63] Loss: 0.58831\n",
            "Epoch [82/200], Step [12/63] Loss: 0.59673\n",
            "Epoch [82/200], Step [13/63] Loss: 0.81715\n",
            "Epoch [82/200], Step [14/63] Loss: 0.63759\n",
            "Epoch [82/200], Step [15/63] Loss: 0.68674\n",
            "Epoch [82/200], Step [16/63] Loss: 0.74898\n",
            "Epoch [82/200], Step [17/63] Loss: 0.89115\n",
            "Epoch [82/200], Step [18/63] Loss: 0.67866\n",
            "Epoch [82/200], Step [19/63] Loss: 0.76143\n",
            "Epoch [82/200], Step [20/63] Loss: 0.40676\n",
            "Epoch [82/200], Step [21/63] Loss: 0.70289\n",
            "Epoch [82/200], Step [22/63] Loss: 0.75217\n",
            "Epoch [82/200], Step [23/63] Loss: 1.21213\n",
            "Epoch [82/200], Step [24/63] Loss: 0.47364\n",
            "Epoch [82/200], Step [25/63] Loss: 0.55266\n",
            "Epoch [82/200], Step [26/63] Loss: 0.55855\n",
            "Epoch [82/200], Step [27/63] Loss: 0.81636\n",
            "Epoch [82/200], Step [28/63] Loss: 0.59269\n",
            "Epoch [82/200], Step [29/63] Loss: 0.71754\n",
            "Epoch [82/200], Step [30/63] Loss: 0.75018\n",
            "Epoch [82/200], Step [31/63] Loss: 0.34819\n",
            "Epoch [82/200], Step [32/63] Loss: 0.64266\n",
            "Epoch [82/200], Step [33/63] Loss: 0.52876\n",
            "Epoch [82/200], Step [34/63] Loss: 0.77418\n",
            "Epoch [82/200], Step [35/63] Loss: 0.72790\n",
            "Epoch [82/200], Step [36/63] Loss: 0.48519\n",
            "Epoch [82/200], Step [37/63] Loss: 0.60211\n",
            "Epoch [82/200], Step [38/63] Loss: 0.57414\n",
            "Epoch [82/200], Step [39/63] Loss: 0.61301\n",
            "Epoch [82/200], Step [40/63] Loss: 0.54126\n",
            "Epoch [82/200], Step [41/63] Loss: 0.95794\n",
            "Epoch [82/200], Step [42/63] Loss: 0.71594\n",
            "Epoch [82/200], Step [43/63] Loss: 0.78057\n",
            "Epoch [82/200], Step [44/63] Loss: 0.63748\n",
            "Epoch [82/200], Step [45/63] Loss: 0.65510\n",
            "Epoch [82/200], Step [46/63] Loss: 0.51163\n",
            "Epoch [82/200], Step [47/63] Loss: 0.57630\n",
            "Epoch [82/200], Step [48/63] Loss: 0.68888\n",
            "Epoch [82/200], Step [49/63] Loss: 0.81029\n",
            "Epoch [82/200], Step [50/63] Loss: 1.01337\n",
            "Epoch [82/200], Step [51/63] Loss: 0.72352\n",
            "Epoch [82/200], Step [52/63] Loss: 0.62369\n",
            "Epoch [82/200], Step [53/63] Loss: 0.54280\n",
            "Epoch [82/200], Step [54/63] Loss: 0.61868\n",
            "Epoch [82/200], Step [55/63] Loss: 0.82769\n",
            "Epoch [82/200], Step [56/63] Loss: 0.69335\n",
            "Epoch [82/200], Step [57/63] Loss: 0.57312\n",
            "Epoch [82/200], Step [58/63] Loss: 0.64101\n",
            "Epoch [82/200], Step [59/63] Loss: 0.45945\n",
            "Epoch [82/200], Step [60/63] Loss: 0.67500\n",
            "Epoch [82/200], Step [61/63] Loss: 0.44071\n",
            "Epoch [82/200], Step [62/63] Loss: 0.60812\n",
            "Epoch [82/200], Step [63/63] Loss: 0.59490\n",
            "Valid Accuracy: 69.67592592592592 %\n",
            "Epoch [83/200], Step [1/63] Loss: 0.70893\n",
            "Epoch [83/200], Step [2/63] Loss: 0.54852\n",
            "Epoch [83/200], Step [3/63] Loss: 0.36812\n",
            "Epoch [83/200], Step [4/63] Loss: 0.38496\n",
            "Epoch [83/200], Step [5/63] Loss: 0.67503\n",
            "Epoch [83/200], Step [6/63] Loss: 0.91438\n",
            "Epoch [83/200], Step [7/63] Loss: 0.88961\n",
            "Epoch [83/200], Step [8/63] Loss: 0.59674\n",
            "Epoch [83/200], Step [9/63] Loss: 0.54023\n",
            "Epoch [83/200], Step [10/63] Loss: 0.59942\n",
            "Epoch [83/200], Step [11/63] Loss: 0.68915\n",
            "Epoch [83/200], Step [12/63] Loss: 0.51850\n",
            "Epoch [83/200], Step [13/63] Loss: 0.43964\n",
            "Epoch [83/200], Step [14/63] Loss: 0.47755\n",
            "Epoch [83/200], Step [15/63] Loss: 0.75325\n",
            "Epoch [83/200], Step [16/63] Loss: 0.58448\n",
            "Epoch [83/200], Step [17/63] Loss: 0.59174\n",
            "Epoch [83/200], Step [18/63] Loss: 1.01051\n",
            "Epoch [83/200], Step [19/63] Loss: 0.48740\n",
            "Epoch [83/200], Step [20/63] Loss: 0.66110\n",
            "Epoch [83/200], Step [21/63] Loss: 0.78706\n",
            "Epoch [83/200], Step [22/63] Loss: 0.35375\n",
            "Epoch [83/200], Step [23/63] Loss: 0.61010\n",
            "Epoch [83/200], Step [24/63] Loss: 0.69125\n",
            "Epoch [83/200], Step [25/63] Loss: 0.55981\n",
            "Epoch [83/200], Step [26/63] Loss: 0.51122\n",
            "Epoch [83/200], Step [27/63] Loss: 0.43920\n",
            "Epoch [83/200], Step [28/63] Loss: 0.84489\n",
            "Epoch [83/200], Step [29/63] Loss: 0.89841\n",
            "Epoch [83/200], Step [30/63] Loss: 0.78875\n",
            "Epoch [83/200], Step [31/63] Loss: 0.74040\n",
            "Epoch [83/200], Step [32/63] Loss: 0.62032\n",
            "Epoch [83/200], Step [33/63] Loss: 0.67431\n",
            "Epoch [83/200], Step [34/63] Loss: 0.67122\n",
            "Epoch [83/200], Step [35/63] Loss: 0.53144\n",
            "Epoch [83/200], Step [36/63] Loss: 0.51267\n",
            "Epoch [83/200], Step [37/63] Loss: 0.52893\n",
            "Epoch [83/200], Step [38/63] Loss: 0.33516\n",
            "Epoch [83/200], Step [39/63] Loss: 0.99947\n",
            "Epoch [83/200], Step [40/63] Loss: 0.50728\n",
            "Epoch [83/200], Step [41/63] Loss: 0.73734\n",
            "Epoch [83/200], Step [42/63] Loss: 0.95519\n",
            "Epoch [83/200], Step [43/63] Loss: 0.49600\n",
            "Epoch [83/200], Step [44/63] Loss: 0.56151\n",
            "Epoch [83/200], Step [45/63] Loss: 0.88331\n",
            "Epoch [83/200], Step [46/63] Loss: 0.63740\n",
            "Epoch [83/200], Step [47/63] Loss: 0.70404\n",
            "Epoch [83/200], Step [48/63] Loss: 0.71070\n",
            "Epoch [83/200], Step [49/63] Loss: 0.66756\n",
            "Epoch [83/200], Step [50/63] Loss: 0.47418\n",
            "Epoch [83/200], Step [51/63] Loss: 0.58734\n",
            "Epoch [83/200], Step [52/63] Loss: 0.63334\n",
            "Epoch [83/200], Step [53/63] Loss: 0.46011\n",
            "Epoch [83/200], Step [54/63] Loss: 0.53576\n",
            "Epoch [83/200], Step [55/63] Loss: 0.73609\n",
            "Epoch [83/200], Step [56/63] Loss: 0.70691\n",
            "Epoch [83/200], Step [57/63] Loss: 0.36909\n",
            "Epoch [83/200], Step [58/63] Loss: 0.80880\n",
            "Epoch [83/200], Step [59/63] Loss: 0.72753\n",
            "Epoch [83/200], Step [60/63] Loss: 0.81456\n",
            "Epoch [83/200], Step [61/63] Loss: 0.58879\n",
            "Epoch [83/200], Step [62/63] Loss: 0.57607\n",
            "Epoch [83/200], Step [63/63] Loss: 0.58550\n",
            "Valid Accuracy: 34.02777777777778 %\n",
            "Epoch [84/200], Step [1/63] Loss: 0.72482\n",
            "Epoch [84/200], Step [2/63] Loss: 0.64989\n",
            "Epoch [84/200], Step [3/63] Loss: 0.68825\n",
            "Epoch [84/200], Step [4/63] Loss: 0.82334\n",
            "Epoch [84/200], Step [5/63] Loss: 0.57903\n",
            "Epoch [84/200], Step [6/63] Loss: 0.62211\n",
            "Epoch [84/200], Step [7/63] Loss: 0.71865\n",
            "Epoch [84/200], Step [8/63] Loss: 0.78058\n",
            "Epoch [84/200], Step [9/63] Loss: 0.58304\n",
            "Epoch [84/200], Step [10/63] Loss: 0.74527\n",
            "Epoch [84/200], Step [11/63] Loss: 0.44736\n",
            "Epoch [84/200], Step [12/63] Loss: 0.64974\n",
            "Epoch [84/200], Step [13/63] Loss: 0.44705\n",
            "Epoch [84/200], Step [14/63] Loss: 0.65692\n",
            "Epoch [84/200], Step [15/63] Loss: 0.79698\n",
            "Epoch [84/200], Step [16/63] Loss: 0.47969\n",
            "Epoch [84/200], Step [17/63] Loss: 0.46527\n",
            "Epoch [84/200], Step [18/63] Loss: 0.59406\n",
            "Epoch [84/200], Step [19/63] Loss: 0.64298\n",
            "Epoch [84/200], Step [20/63] Loss: 0.52869\n",
            "Epoch [84/200], Step [21/63] Loss: 0.36151\n",
            "Epoch [84/200], Step [22/63] Loss: 0.76020\n",
            "Epoch [84/200], Step [23/63] Loss: 0.73280\n",
            "Epoch [84/200], Step [24/63] Loss: 0.47811\n",
            "Epoch [84/200], Step [25/63] Loss: 0.67535\n",
            "Epoch [84/200], Step [26/63] Loss: 0.50168\n",
            "Epoch [84/200], Step [27/63] Loss: 0.54804\n",
            "Epoch [84/200], Step [28/63] Loss: 0.93691\n",
            "Epoch [84/200], Step [29/63] Loss: 0.83485\n",
            "Epoch [84/200], Step [30/63] Loss: 0.59518\n",
            "Epoch [84/200], Step [31/63] Loss: 1.49532\n",
            "Epoch [84/200], Step [32/63] Loss: 0.72154\n",
            "Epoch [84/200], Step [33/63] Loss: 0.87608\n",
            "Epoch [84/200], Step [34/63] Loss: 0.63646\n",
            "Epoch [84/200], Step [35/63] Loss: 0.62189\n",
            "Epoch [84/200], Step [36/63] Loss: 0.58416\n",
            "Epoch [84/200], Step [37/63] Loss: 0.46204\n",
            "Epoch [84/200], Step [38/63] Loss: 0.68602\n",
            "Epoch [84/200], Step [39/63] Loss: 0.59769\n",
            "Epoch [84/200], Step [40/63] Loss: 0.56175\n",
            "Epoch [84/200], Step [41/63] Loss: 0.63031\n",
            "Epoch [84/200], Step [42/63] Loss: 0.47922\n",
            "Epoch [84/200], Step [43/63] Loss: 0.64074\n",
            "Epoch [84/200], Step [44/63] Loss: 0.43031\n",
            "Epoch [84/200], Step [45/63] Loss: 0.56483\n",
            "Epoch [84/200], Step [46/63] Loss: 0.59455\n",
            "Epoch [84/200], Step [47/63] Loss: 0.66381\n",
            "Epoch [84/200], Step [48/63] Loss: 0.54231\n",
            "Epoch [84/200], Step [49/63] Loss: 0.78566\n",
            "Epoch [84/200], Step [50/63] Loss: 0.45719\n",
            "Epoch [84/200], Step [51/63] Loss: 0.69772\n",
            "Epoch [84/200], Step [52/63] Loss: 0.63691\n",
            "Epoch [84/200], Step [53/63] Loss: 0.48565\n",
            "Epoch [84/200], Step [54/63] Loss: 0.44333\n",
            "Epoch [84/200], Step [55/63] Loss: 0.88359\n",
            "Epoch [84/200], Step [56/63] Loss: 0.72711\n",
            "Epoch [84/200], Step [57/63] Loss: 0.72087\n",
            "Epoch [84/200], Step [58/63] Loss: 0.78518\n",
            "Epoch [84/200], Step [59/63] Loss: 0.78472\n",
            "Epoch [84/200], Step [60/63] Loss: 1.03374\n",
            "Epoch [84/200], Step [61/63] Loss: 0.52257\n",
            "Epoch [84/200], Step [62/63] Loss: 0.53909\n",
            "Epoch [84/200], Step [63/63] Loss: 0.61196\n",
            "Valid Accuracy: 62.26851851851852 %\n",
            "Epoch [85/200], Step [1/63] Loss: 0.57962\n",
            "Epoch [85/200], Step [2/63] Loss: 0.62930\n",
            "Epoch [85/200], Step [3/63] Loss: 0.79775\n",
            "Epoch [85/200], Step [4/63] Loss: 0.51817\n",
            "Epoch [85/200], Step [5/63] Loss: 0.62945\n",
            "Epoch [85/200], Step [6/63] Loss: 0.33914\n",
            "Epoch [85/200], Step [7/63] Loss: 0.94852\n",
            "Epoch [85/200], Step [8/63] Loss: 0.56914\n",
            "Epoch [85/200], Step [9/63] Loss: 0.50170\n",
            "Epoch [85/200], Step [10/63] Loss: 0.44812\n",
            "Epoch [85/200], Step [11/63] Loss: 0.53056\n",
            "Epoch [85/200], Step [12/63] Loss: 0.43067\n",
            "Epoch [85/200], Step [13/63] Loss: 0.45481\n",
            "Epoch [85/200], Step [14/63] Loss: 0.58319\n",
            "Epoch [85/200], Step [15/63] Loss: 0.63711\n",
            "Epoch [85/200], Step [16/63] Loss: 0.66073\n",
            "Epoch [85/200], Step [17/63] Loss: 0.81288\n",
            "Epoch [85/200], Step [18/63] Loss: 0.99141\n",
            "Epoch [85/200], Step [19/63] Loss: 0.49608\n",
            "Epoch [85/200], Step [20/63] Loss: 0.80334\n",
            "Epoch [85/200], Step [21/63] Loss: 0.60565\n",
            "Epoch [85/200], Step [22/63] Loss: 0.40201\n",
            "Epoch [85/200], Step [23/63] Loss: 0.42754\n",
            "Epoch [85/200], Step [24/63] Loss: 0.58605\n",
            "Epoch [85/200], Step [25/63] Loss: 0.67688\n",
            "Epoch [85/200], Step [26/63] Loss: 0.88821\n",
            "Epoch [85/200], Step [27/63] Loss: 0.66507\n",
            "Epoch [85/200], Step [28/63] Loss: 0.70317\n",
            "Epoch [85/200], Step [29/63] Loss: 0.66689\n",
            "Epoch [85/200], Step [30/63] Loss: 0.55817\n",
            "Epoch [85/200], Step [31/63] Loss: 0.48557\n",
            "Epoch [85/200], Step [32/63] Loss: 0.96842\n",
            "Epoch [85/200], Step [33/63] Loss: 0.75208\n",
            "Epoch [85/200], Step [34/63] Loss: 0.74461\n",
            "Epoch [85/200], Step [35/63] Loss: 0.54640\n",
            "Epoch [85/200], Step [36/63] Loss: 0.69305\n",
            "Epoch [85/200], Step [37/63] Loss: 0.82061\n",
            "Epoch [85/200], Step [38/63] Loss: 0.70402\n",
            "Epoch [85/200], Step [39/63] Loss: 0.41268\n",
            "Epoch [85/200], Step [40/63] Loss: 0.63820\n",
            "Epoch [85/200], Step [41/63] Loss: 0.44009\n",
            "Epoch [85/200], Step [42/63] Loss: 0.55079\n",
            "Epoch [85/200], Step [43/63] Loss: 0.85929\n",
            "Epoch [85/200], Step [44/63] Loss: 0.67794\n",
            "Epoch [85/200], Step [45/63] Loss: 0.51190\n",
            "Epoch [85/200], Step [46/63] Loss: 0.44293\n",
            "Epoch [85/200], Step [47/63] Loss: 0.69917\n",
            "Epoch [85/200], Step [48/63] Loss: 0.56607\n",
            "Epoch [85/200], Step [49/63] Loss: 0.50648\n",
            "Epoch [85/200], Step [50/63] Loss: 0.45490\n",
            "Epoch [85/200], Step [51/63] Loss: 0.56591\n",
            "Epoch [85/200], Step [52/63] Loss: 0.81858\n",
            "Epoch [85/200], Step [53/63] Loss: 0.59828\n",
            "Epoch [85/200], Step [54/63] Loss: 0.71489\n",
            "Epoch [85/200], Step [55/63] Loss: 0.50547\n",
            "Epoch [85/200], Step [56/63] Loss: 0.56685\n",
            "Epoch [85/200], Step [57/63] Loss: 0.93027\n",
            "Epoch [85/200], Step [58/63] Loss: 0.57254\n",
            "Epoch [85/200], Step [59/63] Loss: 0.61830\n",
            "Epoch [85/200], Step [60/63] Loss: 0.90382\n",
            "Epoch [85/200], Step [61/63] Loss: 0.43473\n",
            "Epoch [85/200], Step [62/63] Loss: 0.73338\n",
            "Epoch [85/200], Step [63/63] Loss: 0.53785\n",
            "Valid Accuracy: 63.657407407407405 %\n",
            "Epoch [86/200], Step [1/63] Loss: 0.73742\n",
            "Epoch [86/200], Step [2/63] Loss: 0.50628\n",
            "Epoch [86/200], Step [3/63] Loss: 0.40202\n",
            "Epoch [86/200], Step [4/63] Loss: 0.46273\n",
            "Epoch [86/200], Step [5/63] Loss: 0.71352\n",
            "Epoch [86/200], Step [6/63] Loss: 0.61038\n",
            "Epoch [86/200], Step [7/63] Loss: 0.50100\n",
            "Epoch [86/200], Step [8/63] Loss: 0.37308\n",
            "Epoch [86/200], Step [9/63] Loss: 0.44871\n",
            "Epoch [86/200], Step [10/63] Loss: 0.62021\n",
            "Epoch [86/200], Step [11/63] Loss: 0.63818\n",
            "Epoch [86/200], Step [12/63] Loss: 0.55787\n",
            "Epoch [86/200], Step [13/63] Loss: 0.56537\n",
            "Epoch [86/200], Step [14/63] Loss: 0.44038\n",
            "Epoch [86/200], Step [15/63] Loss: 0.82480\n",
            "Epoch [86/200], Step [16/63] Loss: 0.22527\n",
            "Epoch [86/200], Step [17/63] Loss: 0.56008\n",
            "Epoch [86/200], Step [18/63] Loss: 0.59761\n",
            "Epoch [86/200], Step [19/63] Loss: 0.59853\n",
            "Epoch [86/200], Step [20/63] Loss: 0.41089\n",
            "Epoch [86/200], Step [21/63] Loss: 1.12688\n",
            "Epoch [86/200], Step [22/63] Loss: 0.57783\n",
            "Epoch [86/200], Step [23/63] Loss: 0.44855\n",
            "Epoch [86/200], Step [24/63] Loss: 0.49114\n",
            "Epoch [86/200], Step [25/63] Loss: 0.69085\n",
            "Epoch [86/200], Step [26/63] Loss: 0.53071\n",
            "Epoch [86/200], Step [27/63] Loss: 0.67650\n",
            "Epoch [86/200], Step [28/63] Loss: 0.37494\n",
            "Epoch [86/200], Step [29/63] Loss: 0.68656\n",
            "Epoch [86/200], Step [30/63] Loss: 0.88525\n",
            "Epoch [86/200], Step [31/63] Loss: 0.75890\n",
            "Epoch [86/200], Step [32/63] Loss: 0.67210\n",
            "Epoch [86/200], Step [33/63] Loss: 0.69406\n",
            "Epoch [86/200], Step [34/63] Loss: 0.67520\n",
            "Epoch [86/200], Step [35/63] Loss: 0.78179\n",
            "Epoch [86/200], Step [36/63] Loss: 0.54844\n",
            "Epoch [86/200], Step [37/63] Loss: 0.81635\n",
            "Epoch [86/200], Step [38/63] Loss: 0.63794\n",
            "Epoch [86/200], Step [39/63] Loss: 0.46992\n",
            "Epoch [86/200], Step [40/63] Loss: 0.41830\n",
            "Epoch [86/200], Step [41/63] Loss: 0.60045\n",
            "Epoch [86/200], Step [42/63] Loss: 0.72164\n",
            "Epoch [86/200], Step [43/63] Loss: 0.76498\n",
            "Epoch [86/200], Step [44/63] Loss: 0.60962\n",
            "Epoch [86/200], Step [45/63] Loss: 0.52103\n",
            "Epoch [86/200], Step [46/63] Loss: 0.47664\n",
            "Epoch [86/200], Step [47/63] Loss: 0.76653\n",
            "Epoch [86/200], Step [48/63] Loss: 0.64745\n",
            "Epoch [86/200], Step [49/63] Loss: 0.48372\n",
            "Epoch [86/200], Step [50/63] Loss: 0.62566\n",
            "Epoch [86/200], Step [51/63] Loss: 0.56061\n",
            "Epoch [86/200], Step [52/63] Loss: 0.70071\n",
            "Epoch [86/200], Step [53/63] Loss: 0.61892\n",
            "Epoch [86/200], Step [54/63] Loss: 0.76346\n",
            "Epoch [86/200], Step [55/63] Loss: 0.73988\n",
            "Epoch [86/200], Step [56/63] Loss: 0.82945\n",
            "Epoch [86/200], Step [57/63] Loss: 0.93504\n",
            "Epoch [86/200], Step [58/63] Loss: 1.03860\n",
            "Epoch [86/200], Step [59/63] Loss: 0.57229\n",
            "Epoch [86/200], Step [60/63] Loss: 0.58485\n",
            "Epoch [86/200], Step [61/63] Loss: 0.93062\n",
            "Epoch [86/200], Step [62/63] Loss: 0.65097\n",
            "Epoch [86/200], Step [63/63] Loss: 0.73197\n",
            "Valid Accuracy: 43.28703703703704 %\n",
            "Epoch [87/200], Step [1/63] Loss: 0.71625\n",
            "Epoch [87/200], Step [2/63] Loss: 0.53044\n",
            "Epoch [87/200], Step [3/63] Loss: 0.62306\n",
            "Epoch [87/200], Step [4/63] Loss: 0.52078\n",
            "Epoch [87/200], Step [5/63] Loss: 0.70792\n",
            "Epoch [87/200], Step [6/63] Loss: 0.63125\n",
            "Epoch [87/200], Step [7/63] Loss: 0.66654\n",
            "Epoch [87/200], Step [8/63] Loss: 0.57859\n",
            "Epoch [87/200], Step [9/63] Loss: 0.38823\n",
            "Epoch [87/200], Step [10/63] Loss: 0.65782\n",
            "Epoch [87/200], Step [11/63] Loss: 0.47813\n",
            "Epoch [87/200], Step [12/63] Loss: 0.71265\n",
            "Epoch [87/200], Step [13/63] Loss: 1.04524\n",
            "Epoch [87/200], Step [14/63] Loss: 0.43300\n",
            "Epoch [87/200], Step [15/63] Loss: 0.44964\n",
            "Epoch [87/200], Step [16/63] Loss: 0.64747\n",
            "Epoch [87/200], Step [17/63] Loss: 0.65433\n",
            "Epoch [87/200], Step [18/63] Loss: 0.79628\n",
            "Epoch [87/200], Step [19/63] Loss: 0.70349\n",
            "Epoch [87/200], Step [20/63] Loss: 0.41615\n",
            "Epoch [87/200], Step [21/63] Loss: 0.61258\n",
            "Epoch [87/200], Step [22/63] Loss: 0.54223\n",
            "Epoch [87/200], Step [23/63] Loss: 0.64720\n",
            "Epoch [87/200], Step [24/63] Loss: 0.32386\n",
            "Epoch [87/200], Step [25/63] Loss: 0.40967\n",
            "Epoch [87/200], Step [26/63] Loss: 0.51975\n",
            "Epoch [87/200], Step [27/63] Loss: 0.74852\n",
            "Epoch [87/200], Step [28/63] Loss: 0.67673\n",
            "Epoch [87/200], Step [29/63] Loss: 0.52932\n",
            "Epoch [87/200], Step [30/63] Loss: 0.71859\n",
            "Epoch [87/200], Step [31/63] Loss: 0.68119\n",
            "Epoch [87/200], Step [32/63] Loss: 0.49738\n",
            "Epoch [87/200], Step [33/63] Loss: 0.48889\n",
            "Epoch [87/200], Step [34/63] Loss: 0.56081\n",
            "Epoch [87/200], Step [35/63] Loss: 0.74258\n",
            "Epoch [87/200], Step [36/63] Loss: 0.67763\n",
            "Epoch [87/200], Step [37/63] Loss: 0.88683\n",
            "Epoch [87/200], Step [38/63] Loss: 0.68977\n",
            "Epoch [87/200], Step [39/63] Loss: 0.65269\n",
            "Epoch [87/200], Step [40/63] Loss: 0.54612\n",
            "Epoch [87/200], Step [41/63] Loss: 0.52776\n",
            "Epoch [87/200], Step [42/63] Loss: 0.89488\n",
            "Epoch [87/200], Step [43/63] Loss: 0.38934\n",
            "Epoch [87/200], Step [44/63] Loss: 0.74396\n",
            "Epoch [87/200], Step [45/63] Loss: 0.71262\n",
            "Epoch [87/200], Step [46/63] Loss: 0.86384\n",
            "Epoch [87/200], Step [47/63] Loss: 0.45791\n",
            "Epoch [87/200], Step [48/63] Loss: 0.49675\n",
            "Epoch [87/200], Step [49/63] Loss: 0.34659\n",
            "Epoch [87/200], Step [50/63] Loss: 0.78876\n",
            "Epoch [87/200], Step [51/63] Loss: 0.67372\n",
            "Epoch [87/200], Step [52/63] Loss: 0.59525\n",
            "Epoch [87/200], Step [53/63] Loss: 0.58473\n",
            "Epoch [87/200], Step [54/63] Loss: 0.47596\n",
            "Epoch [87/200], Step [55/63] Loss: 0.91360\n",
            "Epoch [87/200], Step [56/63] Loss: 0.65511\n",
            "Epoch [87/200], Step [57/63] Loss: 0.78853\n",
            "Epoch [87/200], Step [58/63] Loss: 0.81509\n",
            "Epoch [87/200], Step [59/63] Loss: 0.53931\n",
            "Epoch [87/200], Step [60/63] Loss: 0.51500\n",
            "Epoch [87/200], Step [61/63] Loss: 0.80831\n",
            "Epoch [87/200], Step [62/63] Loss: 0.60077\n",
            "Epoch [87/200], Step [63/63] Loss: 0.45444\n",
            "Valid Accuracy: 57.175925925925924 %\n",
            "Epoch [88/200], Step [1/63] Loss: 0.58170\n",
            "Epoch [88/200], Step [2/63] Loss: 0.63410\n",
            "Epoch [88/200], Step [3/63] Loss: 0.78236\n",
            "Epoch [88/200], Step [4/63] Loss: 0.73891\n",
            "Epoch [88/200], Step [5/63] Loss: 0.46515\n",
            "Epoch [88/200], Step [6/63] Loss: 0.54768\n",
            "Epoch [88/200], Step [7/63] Loss: 0.70049\n",
            "Epoch [88/200], Step [8/63] Loss: 0.49213\n",
            "Epoch [88/200], Step [9/63] Loss: 0.52711\n",
            "Epoch [88/200], Step [10/63] Loss: 0.50339\n",
            "Epoch [88/200], Step [11/63] Loss: 0.43378\n",
            "Epoch [88/200], Step [12/63] Loss: 0.65964\n",
            "Epoch [88/200], Step [13/63] Loss: 0.54191\n",
            "Epoch [88/200], Step [14/63] Loss: 0.48685\n",
            "Epoch [88/200], Step [15/63] Loss: 0.41255\n",
            "Epoch [88/200], Step [16/63] Loss: 0.55123\n",
            "Epoch [88/200], Step [17/63] Loss: 0.48071\n",
            "Epoch [88/200], Step [18/63] Loss: 1.06901\n",
            "Epoch [88/200], Step [19/63] Loss: 0.80669\n",
            "Epoch [88/200], Step [20/63] Loss: 0.51428\n",
            "Epoch [88/200], Step [21/63] Loss: 0.65892\n",
            "Epoch [88/200], Step [22/63] Loss: 0.50865\n",
            "Epoch [88/200], Step [23/63] Loss: 0.46026\n",
            "Epoch [88/200], Step [24/63] Loss: 0.55169\n",
            "Epoch [88/200], Step [25/63] Loss: 0.77274\n",
            "Epoch [88/200], Step [26/63] Loss: 0.45949\n",
            "Epoch [88/200], Step [27/63] Loss: 0.74361\n",
            "Epoch [88/200], Step [28/63] Loss: 0.60647\n",
            "Epoch [88/200], Step [29/63] Loss: 1.05309\n",
            "Epoch [88/200], Step [30/63] Loss: 0.78679\n",
            "Epoch [88/200], Step [31/63] Loss: 0.72436\n",
            "Epoch [88/200], Step [32/63] Loss: 0.45491\n",
            "Epoch [88/200], Step [33/63] Loss: 0.69067\n",
            "Epoch [88/200], Step [34/63] Loss: 0.70550\n",
            "Epoch [88/200], Step [35/63] Loss: 0.73945\n",
            "Epoch [88/200], Step [36/63] Loss: 0.60374\n",
            "Epoch [88/200], Step [37/63] Loss: 0.73000\n",
            "Epoch [88/200], Step [38/63] Loss: 0.43532\n",
            "Epoch [88/200], Step [39/63] Loss: 0.76768\n",
            "Epoch [88/200], Step [40/63] Loss: 0.55026\n",
            "Epoch [88/200], Step [41/63] Loss: 0.58202\n",
            "Epoch [88/200], Step [42/63] Loss: 0.86815\n",
            "Epoch [88/200], Step [43/63] Loss: 0.51197\n",
            "Epoch [88/200], Step [44/63] Loss: 0.60490\n",
            "Epoch [88/200], Step [45/63] Loss: 0.50575\n",
            "Epoch [88/200], Step [46/63] Loss: 0.61496\n",
            "Epoch [88/200], Step [47/63] Loss: 0.53512\n",
            "Epoch [88/200], Step [48/63] Loss: 0.56149\n",
            "Epoch [88/200], Step [49/63] Loss: 0.67024\n",
            "Epoch [88/200], Step [50/63] Loss: 0.66258\n",
            "Epoch [88/200], Step [51/63] Loss: 0.64977\n",
            "Epoch [88/200], Step [52/63] Loss: 0.60582\n",
            "Epoch [88/200], Step [53/63] Loss: 0.48989\n",
            "Epoch [88/200], Step [54/63] Loss: 0.38964\n",
            "Epoch [88/200], Step [55/63] Loss: 0.53813\n",
            "Epoch [88/200], Step [56/63] Loss: 0.54007\n",
            "Epoch [88/200], Step [57/63] Loss: 0.51369\n",
            "Epoch [88/200], Step [58/63] Loss: 0.43028\n",
            "Epoch [88/200], Step [59/63] Loss: 0.59303\n",
            "Epoch [88/200], Step [60/63] Loss: 0.48136\n",
            "Epoch [88/200], Step [61/63] Loss: 0.37599\n",
            "Epoch [88/200], Step [62/63] Loss: 0.44026\n",
            "Epoch [88/200], Step [63/63] Loss: 0.68130\n",
            "Valid Accuracy: 64.12037037037037 %\n",
            "Epoch [89/200], Step [1/63] Loss: 0.43732\n",
            "Epoch [89/200], Step [2/63] Loss: 0.68352\n",
            "Epoch [89/200], Step [3/63] Loss: 0.55102\n",
            "Epoch [89/200], Step [4/63] Loss: 0.83765\n",
            "Epoch [89/200], Step [5/63] Loss: 0.73898\n",
            "Epoch [89/200], Step [6/63] Loss: 0.60443\n",
            "Epoch [89/200], Step [7/63] Loss: 0.54357\n",
            "Epoch [89/200], Step [8/63] Loss: 0.60158\n",
            "Epoch [89/200], Step [9/63] Loss: 1.02183\n",
            "Epoch [89/200], Step [10/63] Loss: 0.53298\n",
            "Epoch [89/200], Step [11/63] Loss: 0.57301\n",
            "Epoch [89/200], Step [12/63] Loss: 0.77525\n",
            "Epoch [89/200], Step [13/63] Loss: 0.60284\n",
            "Epoch [89/200], Step [14/63] Loss: 0.50427\n",
            "Epoch [89/200], Step [15/63] Loss: 0.42730\n",
            "Epoch [89/200], Step [16/63] Loss: 0.45227\n",
            "Epoch [89/200], Step [17/63] Loss: 1.47767\n",
            "Epoch [89/200], Step [18/63] Loss: 0.84464\n",
            "Epoch [89/200], Step [19/63] Loss: 0.76509\n",
            "Epoch [89/200], Step [20/63] Loss: 0.59450\n",
            "Epoch [89/200], Step [21/63] Loss: 0.52451\n",
            "Epoch [89/200], Step [22/63] Loss: 0.92465\n",
            "Epoch [89/200], Step [23/63] Loss: 0.77810\n",
            "Epoch [89/200], Step [24/63] Loss: 0.55572\n",
            "Epoch [89/200], Step [25/63] Loss: 0.67590\n",
            "Epoch [89/200], Step [26/63] Loss: 0.66951\n",
            "Epoch [89/200], Step [27/63] Loss: 0.58144\n",
            "Epoch [89/200], Step [28/63] Loss: 0.69558\n",
            "Epoch [89/200], Step [29/63] Loss: 0.54525\n",
            "Epoch [89/200], Step [30/63] Loss: 0.68402\n",
            "Epoch [89/200], Step [31/63] Loss: 0.56787\n",
            "Epoch [89/200], Step [32/63] Loss: 0.59171\n",
            "Epoch [89/200], Step [33/63] Loss: 0.72473\n",
            "Epoch [89/200], Step [34/63] Loss: 0.67375\n",
            "Epoch [89/200], Step [35/63] Loss: 0.93873\n",
            "Epoch [89/200], Step [36/63] Loss: 0.50931\n",
            "Epoch [89/200], Step [37/63] Loss: 0.56282\n",
            "Epoch [89/200], Step [38/63] Loss: 0.59698\n",
            "Epoch [89/200], Step [39/63] Loss: 0.63504\n",
            "Epoch [89/200], Step [40/63] Loss: 0.77377\n",
            "Epoch [89/200], Step [41/63] Loss: 0.46989\n",
            "Epoch [89/200], Step [42/63] Loss: 0.49285\n",
            "Epoch [89/200], Step [43/63] Loss: 0.66326\n",
            "Epoch [89/200], Step [44/63] Loss: 0.48085\n",
            "Epoch [89/200], Step [45/63] Loss: 0.35449\n",
            "Epoch [89/200], Step [46/63] Loss: 0.49826\n",
            "Epoch [89/200], Step [47/63] Loss: 0.53871\n",
            "Epoch [89/200], Step [48/63] Loss: 1.14898\n",
            "Epoch [89/200], Step [49/63] Loss: 0.56498\n",
            "Epoch [89/200], Step [50/63] Loss: 0.77816\n",
            "Epoch [89/200], Step [51/63] Loss: 0.70578\n",
            "Epoch [89/200], Step [52/63] Loss: 0.43692\n",
            "Epoch [89/200], Step [53/63] Loss: 0.70606\n",
            "Epoch [89/200], Step [54/63] Loss: 0.43804\n",
            "Epoch [89/200], Step [55/63] Loss: 0.79421\n",
            "Epoch [89/200], Step [56/63] Loss: 0.67588\n",
            "Epoch [89/200], Step [57/63] Loss: 0.55206\n",
            "Epoch [89/200], Step [58/63] Loss: 0.41101\n",
            "Epoch [89/200], Step [59/63] Loss: 0.65438\n",
            "Epoch [89/200], Step [60/63] Loss: 0.69535\n",
            "Epoch [89/200], Step [61/63] Loss: 0.92918\n",
            "Epoch [89/200], Step [62/63] Loss: 0.58753\n",
            "Epoch [89/200], Step [63/63] Loss: 0.67170\n",
            "Valid Accuracy: 57.407407407407405 %\n",
            "Epoch [90/200], Step [1/63] Loss: 0.43199\n",
            "Epoch [90/200], Step [2/63] Loss: 0.63089\n",
            "Epoch [90/200], Step [3/63] Loss: 0.57666\n",
            "Epoch [90/200], Step [4/63] Loss: 0.46475\n",
            "Epoch [90/200], Step [5/63] Loss: 0.80312\n",
            "Epoch [90/200], Step [6/63] Loss: 0.73424\n",
            "Epoch [90/200], Step [7/63] Loss: 0.47140\n",
            "Epoch [90/200], Step [8/63] Loss: 0.69504\n",
            "Epoch [90/200], Step [9/63] Loss: 0.35651\n",
            "Epoch [90/200], Step [10/63] Loss: 0.48029\n",
            "Epoch [90/200], Step [11/63] Loss: 0.64733\n",
            "Epoch [90/200], Step [12/63] Loss: 0.44849\n",
            "Epoch [90/200], Step [13/63] Loss: 0.55340\n",
            "Epoch [90/200], Step [14/63] Loss: 0.28128\n",
            "Epoch [90/200], Step [15/63] Loss: 0.57698\n",
            "Epoch [90/200], Step [16/63] Loss: 0.50425\n",
            "Epoch [90/200], Step [17/63] Loss: 0.85967\n",
            "Epoch [90/200], Step [18/63] Loss: 0.43201\n",
            "Epoch [90/200], Step [19/63] Loss: 0.48892\n",
            "Epoch [90/200], Step [20/63] Loss: 0.50485\n",
            "Epoch [90/200], Step [21/63] Loss: 0.57960\n",
            "Epoch [90/200], Step [22/63] Loss: 0.39052\n",
            "Epoch [90/200], Step [23/63] Loss: 0.57517\n",
            "Epoch [90/200], Step [24/63] Loss: 0.62818\n",
            "Epoch [90/200], Step [25/63] Loss: 0.61165\n",
            "Epoch [90/200], Step [26/63] Loss: 0.53066\n",
            "Epoch [90/200], Step [27/63] Loss: 0.46999\n",
            "Epoch [90/200], Step [28/63] Loss: 0.40828\n",
            "Epoch [90/200], Step [29/63] Loss: 0.59755\n",
            "Epoch [90/200], Step [30/63] Loss: 0.58152\n",
            "Epoch [90/200], Step [31/63] Loss: 0.26145\n",
            "Epoch [90/200], Step [32/63] Loss: 0.90037\n",
            "Epoch [90/200], Step [33/63] Loss: 0.65311\n",
            "Epoch [90/200], Step [34/63] Loss: 0.67780\n",
            "Epoch [90/200], Step [35/63] Loss: 0.56508\n",
            "Epoch [90/200], Step [36/63] Loss: 0.52302\n",
            "Epoch [90/200], Step [37/63] Loss: 0.65262\n",
            "Epoch [90/200], Step [38/63] Loss: 0.80289\n",
            "Epoch [90/200], Step [39/63] Loss: 0.64934\n",
            "Epoch [90/200], Step [40/63] Loss: 0.64848\n",
            "Epoch [90/200], Step [41/63] Loss: 0.81027\n",
            "Epoch [90/200], Step [42/63] Loss: 0.60537\n",
            "Epoch [90/200], Step [43/63] Loss: 0.78370\n",
            "Epoch [90/200], Step [44/63] Loss: 0.78335\n",
            "Epoch [90/200], Step [45/63] Loss: 0.55030\n",
            "Epoch [90/200], Step [46/63] Loss: 0.60883\n",
            "Epoch [90/200], Step [47/63] Loss: 0.59893\n",
            "Epoch [90/200], Step [48/63] Loss: 0.78832\n",
            "Epoch [90/200], Step [49/63] Loss: 0.81001\n",
            "Epoch [90/200], Step [50/63] Loss: 0.44691\n",
            "Epoch [90/200], Step [51/63] Loss: 0.58362\n",
            "Epoch [90/200], Step [52/63] Loss: 0.80128\n",
            "Epoch [90/200], Step [53/63] Loss: 0.45147\n",
            "Epoch [90/200], Step [54/63] Loss: 0.87811\n",
            "Epoch [90/200], Step [55/63] Loss: 0.54227\n",
            "Epoch [90/200], Step [56/63] Loss: 0.41154\n",
            "Epoch [90/200], Step [57/63] Loss: 0.53788\n",
            "Epoch [90/200], Step [58/63] Loss: 0.77736\n",
            "Epoch [90/200], Step [59/63] Loss: 0.72084\n",
            "Epoch [90/200], Step [60/63] Loss: 1.07553\n",
            "Epoch [90/200], Step [61/63] Loss: 0.78783\n",
            "Epoch [90/200], Step [62/63] Loss: 0.48309\n",
            "Epoch [90/200], Step [63/63] Loss: 0.55648\n",
            "Valid Accuracy: 48.148148148148145 %\n",
            "Epoch [91/200], Step [1/63] Loss: 0.59145\n",
            "Epoch [91/200], Step [2/63] Loss: 0.74104\n",
            "Epoch [91/200], Step [3/63] Loss: 0.81392\n",
            "Epoch [91/200], Step [4/63] Loss: 0.54803\n",
            "Epoch [91/200], Step [5/63] Loss: 0.63896\n",
            "Epoch [91/200], Step [6/63] Loss: 0.59081\n",
            "Epoch [91/200], Step [7/63] Loss: 0.47560\n",
            "Epoch [91/200], Step [8/63] Loss: 0.61765\n",
            "Epoch [91/200], Step [9/63] Loss: 0.70933\n",
            "Epoch [91/200], Step [10/63] Loss: 0.36057\n",
            "Epoch [91/200], Step [11/63] Loss: 0.38246\n",
            "Epoch [91/200], Step [12/63] Loss: 0.57390\n",
            "Epoch [91/200], Step [13/63] Loss: 0.58960\n",
            "Epoch [91/200], Step [14/63] Loss: 0.45570\n",
            "Epoch [91/200], Step [15/63] Loss: 0.79065\n",
            "Epoch [91/200], Step [16/63] Loss: 1.11667\n",
            "Epoch [91/200], Step [17/63] Loss: 0.92099\n",
            "Epoch [91/200], Step [18/63] Loss: 0.54446\n",
            "Epoch [91/200], Step [19/63] Loss: 0.84658\n",
            "Epoch [91/200], Step [20/63] Loss: 0.56529\n",
            "Epoch [91/200], Step [21/63] Loss: 0.49843\n",
            "Epoch [91/200], Step [22/63] Loss: 0.45949\n",
            "Epoch [91/200], Step [23/63] Loss: 0.45288\n",
            "Epoch [91/200], Step [24/63] Loss: 0.68893\n",
            "Epoch [91/200], Step [25/63] Loss: 0.60793\n",
            "Epoch [91/200], Step [26/63] Loss: 0.33498\n",
            "Epoch [91/200], Step [27/63] Loss: 0.51429\n",
            "Epoch [91/200], Step [28/63] Loss: 0.63596\n",
            "Epoch [91/200], Step [29/63] Loss: 0.41796\n",
            "Epoch [91/200], Step [30/63] Loss: 0.86291\n",
            "Epoch [91/200], Step [31/63] Loss: 0.49560\n",
            "Epoch [91/200], Step [32/63] Loss: 0.77073\n",
            "Epoch [91/200], Step [33/63] Loss: 0.48688\n",
            "Epoch [91/200], Step [34/63] Loss: 0.54317\n",
            "Epoch [91/200], Step [35/63] Loss: 0.74958\n",
            "Epoch [91/200], Step [36/63] Loss: 0.43976\n",
            "Epoch [91/200], Step [37/63] Loss: 0.59169\n",
            "Epoch [91/200], Step [38/63] Loss: 0.71442\n",
            "Epoch [91/200], Step [39/63] Loss: 0.52746\n",
            "Epoch [91/200], Step [40/63] Loss: 0.64232\n",
            "Epoch [91/200], Step [41/63] Loss: 0.70791\n",
            "Epoch [91/200], Step [42/63] Loss: 0.51568\n",
            "Epoch [91/200], Step [43/63] Loss: 0.53873\n",
            "Epoch [91/200], Step [44/63] Loss: 0.55851\n",
            "Epoch [91/200], Step [45/63] Loss: 0.65981\n",
            "Epoch [91/200], Step [46/63] Loss: 0.86458\n",
            "Epoch [91/200], Step [47/63] Loss: 0.48405\n",
            "Epoch [91/200], Step [48/63] Loss: 0.60081\n",
            "Epoch [91/200], Step [49/63] Loss: 0.64846\n",
            "Epoch [91/200], Step [50/63] Loss: 0.50691\n",
            "Epoch [91/200], Step [51/63] Loss: 0.67170\n",
            "Epoch [91/200], Step [52/63] Loss: 0.54197\n",
            "Epoch [91/200], Step [53/63] Loss: 0.73643\n",
            "Epoch [91/200], Step [54/63] Loss: 0.50850\n",
            "Epoch [91/200], Step [55/63] Loss: 0.89213\n",
            "Epoch [91/200], Step [56/63] Loss: 0.44787\n",
            "Epoch [91/200], Step [57/63] Loss: 0.50005\n",
            "Epoch [91/200], Step [58/63] Loss: 0.67652\n",
            "Epoch [91/200], Step [59/63] Loss: 0.65397\n",
            "Epoch [91/200], Step [60/63] Loss: 0.45453\n",
            "Epoch [91/200], Step [61/63] Loss: 0.53719\n",
            "Epoch [91/200], Step [62/63] Loss: 0.77544\n",
            "Epoch [91/200], Step [63/63] Loss: 0.57884\n",
            "Valid Accuracy: 60.879629629629626 %\n",
            "Epoch [92/200], Step [1/63] Loss: 0.73595\n",
            "Epoch [92/200], Step [2/63] Loss: 0.44796\n",
            "Epoch [92/200], Step [3/63] Loss: 0.77598\n",
            "Epoch [92/200], Step [4/63] Loss: 0.41892\n",
            "Epoch [92/200], Step [5/63] Loss: 0.47727\n",
            "Epoch [92/200], Step [6/63] Loss: 0.52929\n",
            "Epoch [92/200], Step [7/63] Loss: 0.51463\n",
            "Epoch [92/200], Step [8/63] Loss: 0.70281\n",
            "Epoch [92/200], Step [9/63] Loss: 0.60065\n",
            "Epoch [92/200], Step [10/63] Loss: 0.55138\n",
            "Epoch [92/200], Step [11/63] Loss: 0.30432\n",
            "Epoch [92/200], Step [12/63] Loss: 0.48707\n",
            "Epoch [92/200], Step [13/63] Loss: 0.55840\n",
            "Epoch [92/200], Step [14/63] Loss: 0.47338\n",
            "Epoch [92/200], Step [15/63] Loss: 0.65322\n",
            "Epoch [92/200], Step [16/63] Loss: 0.43369\n",
            "Epoch [92/200], Step [17/63] Loss: 0.67772\n",
            "Epoch [92/200], Step [18/63] Loss: 0.75836\n",
            "Epoch [92/200], Step [19/63] Loss: 0.69607\n",
            "Epoch [92/200], Step [20/63] Loss: 0.43214\n",
            "Epoch [92/200], Step [21/63] Loss: 0.84507\n",
            "Epoch [92/200], Step [22/63] Loss: 0.22709\n",
            "Epoch [92/200], Step [23/63] Loss: 0.65731\n",
            "Epoch [92/200], Step [24/63] Loss: 0.41962\n",
            "Epoch [92/200], Step [25/63] Loss: 0.51404\n",
            "Epoch [92/200], Step [26/63] Loss: 0.64605\n",
            "Epoch [92/200], Step [27/63] Loss: 0.66917\n",
            "Epoch [92/200], Step [28/63] Loss: 0.64699\n",
            "Epoch [92/200], Step [29/63] Loss: 0.81182\n",
            "Epoch [92/200], Step [30/63] Loss: 0.60949\n",
            "Epoch [92/200], Step [31/63] Loss: 0.46608\n",
            "Epoch [92/200], Step [32/63] Loss: 0.52908\n",
            "Epoch [92/200], Step [33/63] Loss: 0.58806\n",
            "Epoch [92/200], Step [34/63] Loss: 0.48367\n",
            "Epoch [92/200], Step [35/63] Loss: 0.59060\n",
            "Epoch [92/200], Step [36/63] Loss: 0.62462\n",
            "Epoch [92/200], Step [37/63] Loss: 0.55509\n",
            "Epoch [92/200], Step [38/63] Loss: 0.55572\n",
            "Epoch [92/200], Step [39/63] Loss: 0.59668\n",
            "Epoch [92/200], Step [40/63] Loss: 0.59937\n",
            "Epoch [92/200], Step [41/63] Loss: 0.57404\n",
            "Epoch [92/200], Step [42/63] Loss: 0.45186\n",
            "Epoch [92/200], Step [43/63] Loss: 0.78097\n",
            "Epoch [92/200], Step [44/63] Loss: 0.47810\n",
            "Epoch [92/200], Step [45/63] Loss: 0.71934\n",
            "Epoch [92/200], Step [46/63] Loss: 0.58637\n",
            "Epoch [92/200], Step [47/63] Loss: 0.49894\n",
            "Epoch [92/200], Step [48/63] Loss: 0.59434\n",
            "Epoch [92/200], Step [49/63] Loss: 0.84480\n",
            "Epoch [92/200], Step [50/63] Loss: 0.66957\n",
            "Epoch [92/200], Step [51/63] Loss: 0.57683\n",
            "Epoch [92/200], Step [52/63] Loss: 0.35951\n",
            "Epoch [92/200], Step [53/63] Loss: 0.51126\n",
            "Epoch [92/200], Step [54/63] Loss: 0.74759\n",
            "Epoch [92/200], Step [55/63] Loss: 0.60054\n",
            "Epoch [92/200], Step [56/63] Loss: 0.60743\n",
            "Epoch [92/200], Step [57/63] Loss: 0.43155\n",
            "Epoch [92/200], Step [58/63] Loss: 0.57718\n",
            "Epoch [92/200], Step [59/63] Loss: 0.53247\n",
            "Epoch [92/200], Step [60/63] Loss: 0.54363\n",
            "Epoch [92/200], Step [61/63] Loss: 0.65631\n",
            "Epoch [92/200], Step [62/63] Loss: 0.42263\n",
            "Epoch [92/200], Step [63/63] Loss: 0.98463\n",
            "Valid Accuracy: 66.20370370370371 %\n",
            "Epoch [93/200], Step [1/63] Loss: 0.49629\n",
            "Epoch [93/200], Step [2/63] Loss: 0.44483\n",
            "Epoch [93/200], Step [3/63] Loss: 0.40335\n",
            "Epoch [93/200], Step [4/63] Loss: 1.11623\n",
            "Epoch [93/200], Step [5/63] Loss: 0.56068\n",
            "Epoch [93/200], Step [6/63] Loss: 0.59598\n",
            "Epoch [93/200], Step [7/63] Loss: 0.66116\n",
            "Epoch [93/200], Step [8/63] Loss: 1.03271\n",
            "Epoch [93/200], Step [9/63] Loss: 0.47910\n",
            "Epoch [93/200], Step [10/63] Loss: 0.77129\n",
            "Epoch [93/200], Step [11/63] Loss: 0.59745\n",
            "Epoch [93/200], Step [12/63] Loss: 0.63177\n",
            "Epoch [93/200], Step [13/63] Loss: 0.71225\n",
            "Epoch [93/200], Step [14/63] Loss: 1.07067\n",
            "Epoch [93/200], Step [15/63] Loss: 0.52622\n",
            "Epoch [93/200], Step [16/63] Loss: 0.56711\n",
            "Epoch [93/200], Step [17/63] Loss: 0.61988\n",
            "Epoch [93/200], Step [18/63] Loss: 0.57350\n",
            "Epoch [93/200], Step [19/63] Loss: 0.54358\n",
            "Epoch [93/200], Step [20/63] Loss: 0.47287\n",
            "Epoch [93/200], Step [21/63] Loss: 1.00115\n",
            "Epoch [93/200], Step [22/63] Loss: 0.71174\n",
            "Epoch [93/200], Step [23/63] Loss: 0.42331\n",
            "Epoch [93/200], Step [24/63] Loss: 0.56252\n",
            "Epoch [93/200], Step [25/63] Loss: 0.40121\n",
            "Epoch [93/200], Step [26/63] Loss: 0.49056\n",
            "Epoch [93/200], Step [27/63] Loss: 0.44048\n",
            "Epoch [93/200], Step [28/63] Loss: 0.56841\n",
            "Epoch [93/200], Step [29/63] Loss: 0.69712\n",
            "Epoch [93/200], Step [30/63] Loss: 0.65675\n",
            "Epoch [93/200], Step [31/63] Loss: 0.83524\n",
            "Epoch [93/200], Step [32/63] Loss: 0.58843\n",
            "Epoch [93/200], Step [33/63] Loss: 0.56969\n",
            "Epoch [93/200], Step [34/63] Loss: 0.53232\n",
            "Epoch [93/200], Step [35/63] Loss: 0.52806\n",
            "Epoch [93/200], Step [36/63] Loss: 0.37454\n",
            "Epoch [93/200], Step [37/63] Loss: 1.04995\n",
            "Epoch [93/200], Step [38/63] Loss: 0.80828\n",
            "Epoch [93/200], Step [39/63] Loss: 0.35261\n",
            "Epoch [93/200], Step [40/63] Loss: 0.40631\n",
            "Epoch [93/200], Step [41/63] Loss: 0.63515\n",
            "Epoch [93/200], Step [42/63] Loss: 0.62591\n",
            "Epoch [93/200], Step [43/63] Loss: 0.64054\n",
            "Epoch [93/200], Step [44/63] Loss: 0.55949\n",
            "Epoch [93/200], Step [45/63] Loss: 0.61659\n",
            "Epoch [93/200], Step [46/63] Loss: 0.36668\n",
            "Epoch [93/200], Step [47/63] Loss: 0.26543\n",
            "Epoch [93/200], Step [48/63] Loss: 0.49612\n",
            "Epoch [93/200], Step [49/63] Loss: 0.58288\n",
            "Epoch [93/200], Step [50/63] Loss: 0.71338\n",
            "Epoch [93/200], Step [51/63] Loss: 0.54923\n",
            "Epoch [93/200], Step [52/63] Loss: 0.65252\n",
            "Epoch [93/200], Step [53/63] Loss: 0.81493\n",
            "Epoch [93/200], Step [54/63] Loss: 0.61681\n",
            "Epoch [93/200], Step [55/63] Loss: 0.84483\n",
            "Epoch [93/200], Step [56/63] Loss: 0.57972\n",
            "Epoch [93/200], Step [57/63] Loss: 0.48959\n",
            "Epoch [93/200], Step [58/63] Loss: 0.66966\n",
            "Epoch [93/200], Step [59/63] Loss: 0.50130\n",
            "Epoch [93/200], Step [60/63] Loss: 0.43857\n",
            "Epoch [93/200], Step [61/63] Loss: 0.56909\n",
            "Epoch [93/200], Step [62/63] Loss: 0.66383\n",
            "Epoch [93/200], Step [63/63] Loss: 0.48023\n",
            "Valid Accuracy: 63.425925925925924 %\n",
            "Epoch [94/200], Step [1/63] Loss: 0.51931\n",
            "Epoch [94/200], Step [2/63] Loss: 0.62559\n",
            "Epoch [94/200], Step [3/63] Loss: 0.69911\n",
            "Epoch [94/200], Step [4/63] Loss: 0.82639\n",
            "Epoch [94/200], Step [5/63] Loss: 0.52581\n",
            "Epoch [94/200], Step [6/63] Loss: 0.70823\n",
            "Epoch [94/200], Step [7/63] Loss: 0.53977\n",
            "Epoch [94/200], Step [8/63] Loss: 0.45642\n",
            "Epoch [94/200], Step [9/63] Loss: 0.60342\n",
            "Epoch [94/200], Step [10/63] Loss: 0.42549\n",
            "Epoch [94/200], Step [11/63] Loss: 0.67548\n",
            "Epoch [94/200], Step [12/63] Loss: 0.39180\n",
            "Epoch [94/200], Step [13/63] Loss: 0.81670\n",
            "Epoch [94/200], Step [14/63] Loss: 0.34107\n",
            "Epoch [94/200], Step [15/63] Loss: 0.65814\n",
            "Epoch [94/200], Step [16/63] Loss: 0.31862\n",
            "Epoch [94/200], Step [17/63] Loss: 0.24189\n",
            "Epoch [94/200], Step [18/63] Loss: 0.40929\n",
            "Epoch [94/200], Step [19/63] Loss: 0.41659\n",
            "Epoch [94/200], Step [20/63] Loss: 0.67675\n",
            "Epoch [94/200], Step [21/63] Loss: 0.66444\n",
            "Epoch [94/200], Step [22/63] Loss: 0.66012\n",
            "Epoch [94/200], Step [23/63] Loss: 0.51686\n",
            "Epoch [94/200], Step [24/63] Loss: 0.78921\n",
            "Epoch [94/200], Step [25/63] Loss: 0.54488\n",
            "Epoch [94/200], Step [26/63] Loss: 0.53490\n",
            "Epoch [94/200], Step [27/63] Loss: 0.44958\n",
            "Epoch [94/200], Step [28/63] Loss: 0.60819\n",
            "Epoch [94/200], Step [29/63] Loss: 0.35310\n",
            "Epoch [94/200], Step [30/63] Loss: 0.61296\n",
            "Epoch [94/200], Step [31/63] Loss: 0.87286\n",
            "Epoch [94/200], Step [32/63] Loss: 0.49478\n",
            "Epoch [94/200], Step [33/63] Loss: 0.58266\n",
            "Epoch [94/200], Step [34/63] Loss: 0.51983\n",
            "Epoch [94/200], Step [35/63] Loss: 0.87291\n",
            "Epoch [94/200], Step [36/63] Loss: 0.30345\n",
            "Epoch [94/200], Step [37/63] Loss: 0.44378\n",
            "Epoch [94/200], Step [38/63] Loss: 0.55181\n",
            "Epoch [94/200], Step [39/63] Loss: 0.64932\n",
            "Epoch [94/200], Step [40/63] Loss: 0.60100\n",
            "Epoch [94/200], Step [41/63] Loss: 0.61982\n",
            "Epoch [94/200], Step [42/63] Loss: 0.49521\n",
            "Epoch [94/200], Step [43/63] Loss: 0.67613\n",
            "Epoch [94/200], Step [44/63] Loss: 0.85997\n",
            "Epoch [94/200], Step [45/63] Loss: 0.51415\n",
            "Epoch [94/200], Step [46/63] Loss: 0.74347\n",
            "Epoch [94/200], Step [47/63] Loss: 0.46105\n",
            "Epoch [94/200], Step [48/63] Loss: 0.34675\n",
            "Epoch [94/200], Step [49/63] Loss: 0.81562\n",
            "Epoch [94/200], Step [50/63] Loss: 0.39827\n",
            "Epoch [94/200], Step [51/63] Loss: 0.54067\n",
            "Epoch [94/200], Step [52/63] Loss: 0.64291\n",
            "Epoch [94/200], Step [53/63] Loss: 0.58426\n",
            "Epoch [94/200], Step [54/63] Loss: 0.38425\n",
            "Epoch [94/200], Step [55/63] Loss: 0.69178\n",
            "Epoch [94/200], Step [56/63] Loss: 0.71808\n",
            "Epoch [94/200], Step [57/63] Loss: 0.69001\n",
            "Epoch [94/200], Step [58/63] Loss: 0.41799\n",
            "Epoch [94/200], Step [59/63] Loss: 0.72177\n",
            "Epoch [94/200], Step [60/63] Loss: 0.59464\n",
            "Epoch [94/200], Step [61/63] Loss: 0.60348\n",
            "Epoch [94/200], Step [62/63] Loss: 0.46398\n",
            "Epoch [94/200], Step [63/63] Loss: 0.34411\n",
            "Valid Accuracy: 49.76851851851852 %\n",
            "Epoch [95/200], Step [1/63] Loss: 0.83627\n",
            "Epoch [95/200], Step [2/63] Loss: 0.70467\n",
            "Epoch [95/200], Step [3/63] Loss: 0.60000\n",
            "Epoch [95/200], Step [4/63] Loss: 0.67948\n",
            "Epoch [95/200], Step [5/63] Loss: 0.53512\n",
            "Epoch [95/200], Step [6/63] Loss: 0.42918\n",
            "Epoch [95/200], Step [7/63] Loss: 0.55804\n",
            "Epoch [95/200], Step [8/63] Loss: 0.63426\n",
            "Epoch [95/200], Step [9/63] Loss: 0.73336\n",
            "Epoch [95/200], Step [10/63] Loss: 0.47254\n",
            "Epoch [95/200], Step [11/63] Loss: 0.57763\n",
            "Epoch [95/200], Step [12/63] Loss: 0.51654\n",
            "Epoch [95/200], Step [13/63] Loss: 0.39292\n",
            "Epoch [95/200], Step [14/63] Loss: 0.65434\n",
            "Epoch [95/200], Step [15/63] Loss: 0.66017\n",
            "Epoch [95/200], Step [16/63] Loss: 0.40779\n",
            "Epoch [95/200], Step [17/63] Loss: 0.66504\n",
            "Epoch [95/200], Step [18/63] Loss: 0.55806\n",
            "Epoch [95/200], Step [19/63] Loss: 0.55950\n",
            "Epoch [95/200], Step [20/63] Loss: 0.52956\n",
            "Epoch [95/200], Step [21/63] Loss: 0.70715\n",
            "Epoch [95/200], Step [22/63] Loss: 0.65071\n",
            "Epoch [95/200], Step [23/63] Loss: 0.67107\n",
            "Epoch [95/200], Step [24/63] Loss: 0.48125\n",
            "Epoch [95/200], Step [25/63] Loss: 0.79111\n",
            "Epoch [95/200], Step [26/63] Loss: 0.52774\n",
            "Epoch [95/200], Step [27/63] Loss: 0.40192\n",
            "Epoch [95/200], Step [28/63] Loss: 0.64540\n",
            "Epoch [95/200], Step [29/63] Loss: 0.49231\n",
            "Epoch [95/200], Step [30/63] Loss: 0.52468\n",
            "Epoch [95/200], Step [31/63] Loss: 0.36189\n",
            "Epoch [95/200], Step [32/63] Loss: 0.59359\n",
            "Epoch [95/200], Step [33/63] Loss: 0.42904\n",
            "Epoch [95/200], Step [34/63] Loss: 0.39126\n",
            "Epoch [95/200], Step [35/63] Loss: 0.67571\n",
            "Epoch [95/200], Step [36/63] Loss: 0.42677\n",
            "Epoch [95/200], Step [37/63] Loss: 0.68376\n",
            "Epoch [95/200], Step [38/63] Loss: 0.38358\n",
            "Epoch [95/200], Step [39/63] Loss: 0.30657\n",
            "Epoch [95/200], Step [40/63] Loss: 0.46750\n",
            "Epoch [95/200], Step [41/63] Loss: 0.64378\n",
            "Epoch [95/200], Step [42/63] Loss: 0.35550\n",
            "Epoch [95/200], Step [43/63] Loss: 0.77589\n",
            "Epoch [95/200], Step [44/63] Loss: 0.44125\n",
            "Epoch [95/200], Step [45/63] Loss: 0.40139\n",
            "Epoch [95/200], Step [46/63] Loss: 0.50686\n",
            "Epoch [95/200], Step [47/63] Loss: 0.50109\n",
            "Epoch [95/200], Step [48/63] Loss: 0.54502\n",
            "Epoch [95/200], Step [49/63] Loss: 0.78678\n",
            "Epoch [95/200], Step [50/63] Loss: 0.53046\n",
            "Epoch [95/200], Step [51/63] Loss: 0.47587\n",
            "Epoch [95/200], Step [52/63] Loss: 0.50124\n",
            "Epoch [95/200], Step [53/63] Loss: 0.30210\n",
            "Epoch [95/200], Step [54/63] Loss: 0.43644\n",
            "Epoch [95/200], Step [55/63] Loss: 0.79426\n",
            "Epoch [95/200], Step [56/63] Loss: 0.61679\n",
            "Epoch [95/200], Step [57/63] Loss: 0.53584\n",
            "Epoch [95/200], Step [58/63] Loss: 0.40884\n",
            "Epoch [95/200], Step [59/63] Loss: 0.65395\n",
            "Epoch [95/200], Step [60/63] Loss: 0.57297\n",
            "Epoch [95/200], Step [61/63] Loss: 0.63397\n",
            "Epoch [95/200], Step [62/63] Loss: 0.42559\n",
            "Epoch [95/200], Step [63/63] Loss: 0.82530\n",
            "Valid Accuracy: 65.27777777777777 %\n",
            "Epoch [96/200], Step [1/63] Loss: 0.53436\n",
            "Epoch [96/200], Step [2/63] Loss: 0.41958\n",
            "Epoch [96/200], Step [3/63] Loss: 0.47971\n",
            "Epoch [96/200], Step [4/63] Loss: 0.62122\n",
            "Epoch [96/200], Step [5/63] Loss: 0.49771\n",
            "Epoch [96/200], Step [6/63] Loss: 0.45468\n",
            "Epoch [96/200], Step [7/63] Loss: 0.40487\n",
            "Epoch [96/200], Step [8/63] Loss: 0.60923\n",
            "Epoch [96/200], Step [9/63] Loss: 0.46595\n",
            "Epoch [96/200], Step [10/63] Loss: 0.77620\n",
            "Epoch [96/200], Step [11/63] Loss: 0.50756\n",
            "Epoch [96/200], Step [12/63] Loss: 0.55498\n",
            "Epoch [96/200], Step [13/63] Loss: 0.60919\n",
            "Epoch [96/200], Step [14/63] Loss: 0.44412\n",
            "Epoch [96/200], Step [15/63] Loss: 0.51430\n",
            "Epoch [96/200], Step [16/63] Loss: 0.65346\n",
            "Epoch [96/200], Step [17/63] Loss: 0.33699\n",
            "Epoch [96/200], Step [18/63] Loss: 0.83362\n",
            "Epoch [96/200], Step [19/63] Loss: 0.49418\n",
            "Epoch [96/200], Step [20/63] Loss: 0.43272\n",
            "Epoch [96/200], Step [21/63] Loss: 0.67848\n",
            "Epoch [96/200], Step [22/63] Loss: 0.36889\n",
            "Epoch [96/200], Step [23/63] Loss: 0.63077\n",
            "Epoch [96/200], Step [24/63] Loss: 0.45407\n",
            "Epoch [96/200], Step [25/63] Loss: 0.41837\n",
            "Epoch [96/200], Step [26/63] Loss: 0.48129\n",
            "Epoch [96/200], Step [27/63] Loss: 0.75030\n",
            "Epoch [96/200], Step [28/63] Loss: 0.58521\n",
            "Epoch [96/200], Step [29/63] Loss: 0.39670\n",
            "Epoch [96/200], Step [30/63] Loss: 0.62497\n",
            "Epoch [96/200], Step [31/63] Loss: 0.51522\n",
            "Epoch [96/200], Step [32/63] Loss: 0.39449\n",
            "Epoch [96/200], Step [33/63] Loss: 0.58545\n",
            "Epoch [96/200], Step [34/63] Loss: 0.55836\n",
            "Epoch [96/200], Step [35/63] Loss: 0.35999\n",
            "Epoch [96/200], Step [36/63] Loss: 0.38554\n",
            "Epoch [96/200], Step [37/63] Loss: 0.57125\n",
            "Epoch [96/200], Step [38/63] Loss: 1.00793\n",
            "Epoch [96/200], Step [39/63] Loss: 0.52243\n",
            "Epoch [96/200], Step [40/63] Loss: 0.42781\n",
            "Epoch [96/200], Step [41/63] Loss: 0.50367\n",
            "Epoch [96/200], Step [42/63] Loss: 0.36755\n",
            "Epoch [96/200], Step [43/63] Loss: 0.37412\n",
            "Epoch [96/200], Step [44/63] Loss: 0.59694\n",
            "Epoch [96/200], Step [45/63] Loss: 0.34296\n",
            "Epoch [96/200], Step [46/63] Loss: 0.72780\n",
            "Epoch [96/200], Step [47/63] Loss: 0.52402\n",
            "Epoch [96/200], Step [48/63] Loss: 0.94915\n",
            "Epoch [96/200], Step [49/63] Loss: 0.68254\n",
            "Epoch [96/200], Step [50/63] Loss: 0.89213\n",
            "Epoch [96/200], Step [51/63] Loss: 0.60034\n",
            "Epoch [96/200], Step [52/63] Loss: 0.40321\n",
            "Epoch [96/200], Step [53/63] Loss: 0.49569\n",
            "Epoch [96/200], Step [54/63] Loss: 0.49901\n",
            "Epoch [96/200], Step [55/63] Loss: 0.47483\n",
            "Epoch [96/200], Step [56/63] Loss: 0.49570\n",
            "Epoch [96/200], Step [57/63] Loss: 0.90369\n",
            "Epoch [96/200], Step [58/63] Loss: 0.39304\n",
            "Epoch [96/200], Step [59/63] Loss: 0.56065\n",
            "Epoch [96/200], Step [60/63] Loss: 0.55297\n",
            "Epoch [96/200], Step [61/63] Loss: 0.39706\n",
            "Epoch [96/200], Step [62/63] Loss: 0.40445\n",
            "Epoch [96/200], Step [63/63] Loss: 0.53224\n",
            "Valid Accuracy: 67.12962962962963 %\n",
            "Epoch [97/200], Step [1/63] Loss: 0.30734\n",
            "Epoch [97/200], Step [2/63] Loss: 0.41128\n",
            "Epoch [97/200], Step [3/63] Loss: 0.16083\n",
            "Epoch [97/200], Step [4/63] Loss: 0.63483\n",
            "Epoch [97/200], Step [5/63] Loss: 0.52122\n",
            "Epoch [97/200], Step [6/63] Loss: 0.83647\n",
            "Epoch [97/200], Step [7/63] Loss: 0.49401\n",
            "Epoch [97/200], Step [8/63] Loss: 0.34208\n",
            "Epoch [97/200], Step [9/63] Loss: 0.59958\n",
            "Epoch [97/200], Step [10/63] Loss: 0.39259\n",
            "Epoch [97/200], Step [11/63] Loss: 0.25643\n",
            "Epoch [97/200], Step [12/63] Loss: 0.49665\n",
            "Epoch [97/200], Step [13/63] Loss: 0.35138\n",
            "Epoch [97/200], Step [14/63] Loss: 0.47823\n",
            "Epoch [97/200], Step [15/63] Loss: 0.57980\n",
            "Epoch [97/200], Step [16/63] Loss: 0.65340\n",
            "Epoch [97/200], Step [17/63] Loss: 0.21758\n",
            "Epoch [97/200], Step [18/63] Loss: 0.57422\n",
            "Epoch [97/200], Step [19/63] Loss: 0.46683\n",
            "Epoch [97/200], Step [20/63] Loss: 0.57785\n",
            "Epoch [97/200], Step [21/63] Loss: 0.38998\n",
            "Epoch [97/200], Step [22/63] Loss: 0.64467\n",
            "Epoch [97/200], Step [23/63] Loss: 0.65776\n",
            "Epoch [97/200], Step [24/63] Loss: 0.61117\n",
            "Epoch [97/200], Step [25/63] Loss: 0.58305\n",
            "Epoch [97/200], Step [26/63] Loss: 0.31679\n",
            "Epoch [97/200], Step [27/63] Loss: 0.53313\n",
            "Epoch [97/200], Step [28/63] Loss: 0.71717\n",
            "Epoch [97/200], Step [29/63] Loss: 0.32555\n",
            "Epoch [97/200], Step [30/63] Loss: 0.78097\n",
            "Epoch [97/200], Step [31/63] Loss: 0.83469\n",
            "Epoch [97/200], Step [32/63] Loss: 0.52259\n",
            "Epoch [97/200], Step [33/63] Loss: 0.35724\n",
            "Epoch [97/200], Step [34/63] Loss: 0.51266\n",
            "Epoch [97/200], Step [35/63] Loss: 0.44557\n",
            "Epoch [97/200], Step [36/63] Loss: 0.36895\n",
            "Epoch [97/200], Step [37/63] Loss: 0.83571\n",
            "Epoch [97/200], Step [38/63] Loss: 0.63251\n",
            "Epoch [97/200], Step [39/63] Loss: 0.60019\n",
            "Epoch [97/200], Step [40/63] Loss: 0.37171\n",
            "Epoch [97/200], Step [41/63] Loss: 0.58109\n",
            "Epoch [97/200], Step [42/63] Loss: 0.81848\n",
            "Epoch [97/200], Step [43/63] Loss: 0.53368\n",
            "Epoch [97/200], Step [44/63] Loss: 0.81970\n",
            "Epoch [97/200], Step [45/63] Loss: 0.57699\n",
            "Epoch [97/200], Step [46/63] Loss: 0.54298\n",
            "Epoch [97/200], Step [47/63] Loss: 0.60966\n",
            "Epoch [97/200], Step [48/63] Loss: 0.65863\n",
            "Epoch [97/200], Step [49/63] Loss: 0.52468\n",
            "Epoch [97/200], Step [50/63] Loss: 0.62908\n",
            "Epoch [97/200], Step [51/63] Loss: 0.75041\n",
            "Epoch [97/200], Step [52/63] Loss: 0.78043\n",
            "Epoch [97/200], Step [53/63] Loss: 0.46892\n",
            "Epoch [97/200], Step [54/63] Loss: 0.77971\n",
            "Epoch [97/200], Step [55/63] Loss: 0.55231\n",
            "Epoch [97/200], Step [56/63] Loss: 0.58317\n",
            "Epoch [97/200], Step [57/63] Loss: 0.74716\n",
            "Epoch [97/200], Step [58/63] Loss: 0.48404\n",
            "Epoch [97/200], Step [59/63] Loss: 0.40654\n",
            "Epoch [97/200], Step [60/63] Loss: 0.79186\n",
            "Epoch [97/200], Step [61/63] Loss: 0.67466\n",
            "Epoch [97/200], Step [62/63] Loss: 0.52327\n",
            "Epoch [97/200], Step [63/63] Loss: 0.72506\n",
            "Valid Accuracy: 53.93518518518518 %\n",
            "Epoch [98/200], Step [1/63] Loss: 0.42177\n",
            "Epoch [98/200], Step [2/63] Loss: 0.43032\n",
            "Epoch [98/200], Step [3/63] Loss: 0.52498\n",
            "Epoch [98/200], Step [4/63] Loss: 0.25243\n",
            "Epoch [98/200], Step [5/63] Loss: 0.56213\n",
            "Epoch [98/200], Step [6/63] Loss: 0.31970\n",
            "Epoch [98/200], Step [7/63] Loss: 0.40216\n",
            "Epoch [98/200], Step [8/63] Loss: 0.63281\n",
            "Epoch [98/200], Step [9/63] Loss: 0.49733\n",
            "Epoch [98/200], Step [10/63] Loss: 0.73058\n",
            "Epoch [98/200], Step [11/63] Loss: 0.42394\n",
            "Epoch [98/200], Step [12/63] Loss: 0.50803\n",
            "Epoch [98/200], Step [13/63] Loss: 0.56304\n",
            "Epoch [98/200], Step [14/63] Loss: 0.55904\n",
            "Epoch [98/200], Step [15/63] Loss: 0.41399\n",
            "Epoch [98/200], Step [16/63] Loss: 0.63752\n",
            "Epoch [98/200], Step [17/63] Loss: 0.58639\n",
            "Epoch [98/200], Step [18/63] Loss: 0.38299\n",
            "Epoch [98/200], Step [19/63] Loss: 0.95344\n",
            "Epoch [98/200], Step [20/63] Loss: 0.42049\n",
            "Epoch [98/200], Step [21/63] Loss: 0.39937\n",
            "Epoch [98/200], Step [22/63] Loss: 0.49641\n",
            "Epoch [98/200], Step [23/63] Loss: 0.70986\n",
            "Epoch [98/200], Step [24/63] Loss: 0.51817\n",
            "Epoch [98/200], Step [25/63] Loss: 0.41317\n",
            "Epoch [98/200], Step [26/63] Loss: 0.53932\n",
            "Epoch [98/200], Step [27/63] Loss: 0.42356\n",
            "Epoch [98/200], Step [28/63] Loss: 0.47789\n",
            "Epoch [98/200], Step [29/63] Loss: 0.59952\n",
            "Epoch [98/200], Step [30/63] Loss: 0.50930\n",
            "Epoch [98/200], Step [31/63] Loss: 0.28497\n",
            "Epoch [98/200], Step [32/63] Loss: 0.78905\n",
            "Epoch [98/200], Step [33/63] Loss: 0.44911\n",
            "Epoch [98/200], Step [34/63] Loss: 0.53391\n",
            "Epoch [98/200], Step [35/63] Loss: 0.54243\n",
            "Epoch [98/200], Step [36/63] Loss: 0.73624\n",
            "Epoch [98/200], Step [37/63] Loss: 0.35269\n",
            "Epoch [98/200], Step [38/63] Loss: 0.59078\n",
            "Epoch [98/200], Step [39/63] Loss: 0.33225\n",
            "Epoch [98/200], Step [40/63] Loss: 0.54341\n",
            "Epoch [98/200], Step [41/63] Loss: 0.33320\n",
            "Epoch [98/200], Step [42/63] Loss: 0.51098\n",
            "Epoch [98/200], Step [43/63] Loss: 0.61161\n",
            "Epoch [98/200], Step [44/63] Loss: 0.55623\n",
            "Epoch [98/200], Step [45/63] Loss: 0.37771\n",
            "Epoch [98/200], Step [46/63] Loss: 0.45822\n",
            "Epoch [98/200], Step [47/63] Loss: 0.49766\n",
            "Epoch [98/200], Step [48/63] Loss: 0.62363\n",
            "Epoch [98/200], Step [49/63] Loss: 0.44335\n",
            "Epoch [98/200], Step [50/63] Loss: 0.52231\n",
            "Epoch [98/200], Step [51/63] Loss: 0.44070\n",
            "Epoch [98/200], Step [52/63] Loss: 0.44396\n",
            "Epoch [98/200], Step [53/63] Loss: 0.90613\n",
            "Epoch [98/200], Step [54/63] Loss: 0.54221\n",
            "Epoch [98/200], Step [55/63] Loss: 0.46950\n",
            "Epoch [98/200], Step [56/63] Loss: 0.51214\n",
            "Epoch [98/200], Step [57/63] Loss: 0.44569\n",
            "Epoch [98/200], Step [58/63] Loss: 0.89320\n",
            "Epoch [98/200], Step [59/63] Loss: 0.42649\n",
            "Epoch [98/200], Step [60/63] Loss: 0.37068\n",
            "Epoch [98/200], Step [61/63] Loss: 0.52585\n",
            "Epoch [98/200], Step [62/63] Loss: 0.96777\n",
            "Epoch [98/200], Step [63/63] Loss: 0.49679\n",
            "Valid Accuracy: 67.5925925925926 %\n",
            "Epoch [99/200], Step [1/63] Loss: 0.51574\n",
            "Epoch [99/200], Step [2/63] Loss: 0.28952\n",
            "Epoch [99/200], Step [3/63] Loss: 0.42652\n",
            "Epoch [99/200], Step [4/63] Loss: 0.60608\n",
            "Epoch [99/200], Step [5/63] Loss: 0.75243\n",
            "Epoch [99/200], Step [6/63] Loss: 0.33774\n",
            "Epoch [99/200], Step [7/63] Loss: 0.44547\n",
            "Epoch [99/200], Step [8/63] Loss: 0.44118\n",
            "Epoch [99/200], Step [9/63] Loss: 0.54299\n",
            "Epoch [99/200], Step [10/63] Loss: 0.48173\n",
            "Epoch [99/200], Step [11/63] Loss: 0.45705\n",
            "Epoch [99/200], Step [12/63] Loss: 0.64901\n",
            "Epoch [99/200], Step [13/63] Loss: 0.72362\n",
            "Epoch [99/200], Step [14/63] Loss: 0.52497\n",
            "Epoch [99/200], Step [15/63] Loss: 0.52901\n",
            "Epoch [99/200], Step [16/63] Loss: 0.30039\n",
            "Epoch [99/200], Step [17/63] Loss: 0.46225\n",
            "Epoch [99/200], Step [18/63] Loss: 0.40129\n",
            "Epoch [99/200], Step [19/63] Loss: 0.57394\n",
            "Epoch [99/200], Step [20/63] Loss: 0.56270\n",
            "Epoch [99/200], Step [21/63] Loss: 0.60039\n",
            "Epoch [99/200], Step [22/63] Loss: 0.35051\n",
            "Epoch [99/200], Step [23/63] Loss: 0.95611\n",
            "Epoch [99/200], Step [24/63] Loss: 0.42591\n",
            "Epoch [99/200], Step [25/63] Loss: 0.55407\n",
            "Epoch [99/200], Step [26/63] Loss: 0.53862\n",
            "Epoch [99/200], Step [27/63] Loss: 0.55913\n",
            "Epoch [99/200], Step [28/63] Loss: 0.62425\n",
            "Epoch [99/200], Step [29/63] Loss: 0.37470\n",
            "Epoch [99/200], Step [30/63] Loss: 0.80891\n",
            "Epoch [99/200], Step [31/63] Loss: 0.44914\n",
            "Epoch [99/200], Step [32/63] Loss: 0.54082\n",
            "Epoch [99/200], Step [33/63] Loss: 0.40479\n",
            "Epoch [99/200], Step [34/63] Loss: 0.59701\n",
            "Epoch [99/200], Step [35/63] Loss: 0.51445\n",
            "Epoch [99/200], Step [36/63] Loss: 0.43660\n",
            "Epoch [99/200], Step [37/63] Loss: 0.54758\n",
            "Epoch [99/200], Step [38/63] Loss: 0.44739\n",
            "Epoch [99/200], Step [39/63] Loss: 0.68839\n",
            "Epoch [99/200], Step [40/63] Loss: 0.32312\n",
            "Epoch [99/200], Step [41/63] Loss: 0.51371\n",
            "Epoch [99/200], Step [42/63] Loss: 0.67300\n",
            "Epoch [99/200], Step [43/63] Loss: 0.72476\n",
            "Epoch [99/200], Step [44/63] Loss: 0.53522\n",
            "Epoch [99/200], Step [45/63] Loss: 0.54597\n",
            "Epoch [99/200], Step [46/63] Loss: 0.69707\n",
            "Epoch [99/200], Step [47/63] Loss: 0.60830\n",
            "Epoch [99/200], Step [48/63] Loss: 0.45173\n",
            "Epoch [99/200], Step [49/63] Loss: 0.48954\n",
            "Epoch [99/200], Step [50/63] Loss: 0.25558\n",
            "Epoch [99/200], Step [51/63] Loss: 0.80312\n",
            "Epoch [99/200], Step [52/63] Loss: 0.89544\n",
            "Epoch [99/200], Step [53/63] Loss: 0.49286\n",
            "Epoch [99/200], Step [54/63] Loss: 0.61422\n",
            "Epoch [99/200], Step [55/63] Loss: 0.85040\n",
            "Epoch [99/200], Step [56/63] Loss: 0.44963\n",
            "Epoch [99/200], Step [57/63] Loss: 0.72425\n",
            "Epoch [99/200], Step [58/63] Loss: 0.40131\n",
            "Epoch [99/200], Step [59/63] Loss: 0.32417\n",
            "Epoch [99/200], Step [60/63] Loss: 0.67462\n",
            "Epoch [99/200], Step [61/63] Loss: 0.52147\n",
            "Epoch [99/200], Step [62/63] Loss: 0.52187\n",
            "Epoch [99/200], Step [63/63] Loss: 1.44844\n",
            "Valid Accuracy: 53.00925925925926 %\n",
            "Epoch [100/200], Step [1/63] Loss: 0.59900\n",
            "Epoch [100/200], Step [2/63] Loss: 0.40757\n",
            "Epoch [100/200], Step [3/63] Loss: 0.52926\n",
            "Epoch [100/200], Step [4/63] Loss: 0.45379\n",
            "Epoch [100/200], Step [5/63] Loss: 0.86731\n",
            "Epoch [100/200], Step [6/63] Loss: 0.40450\n",
            "Epoch [100/200], Step [7/63] Loss: 0.60230\n",
            "Epoch [100/200], Step [8/63] Loss: 0.45707\n",
            "Epoch [100/200], Step [9/63] Loss: 0.52756\n",
            "Epoch [100/200], Step [10/63] Loss: 0.50224\n",
            "Epoch [100/200], Step [11/63] Loss: 0.68144\n",
            "Epoch [100/200], Step [12/63] Loss: 0.57788\n",
            "Epoch [100/200], Step [13/63] Loss: 0.36020\n",
            "Epoch [100/200], Step [14/63] Loss: 0.43343\n",
            "Epoch [100/200], Step [15/63] Loss: 0.57475\n",
            "Epoch [100/200], Step [16/63] Loss: 0.37603\n",
            "Epoch [100/200], Step [17/63] Loss: 0.67229\n",
            "Epoch [100/200], Step [18/63] Loss: 0.28071\n",
            "Epoch [100/200], Step [19/63] Loss: 0.59613\n",
            "Epoch [100/200], Step [20/63] Loss: 0.70473\n",
            "Epoch [100/200], Step [21/63] Loss: 0.40899\n",
            "Epoch [100/200], Step [22/63] Loss: 0.73675\n",
            "Epoch [100/200], Step [23/63] Loss: 0.41446\n",
            "Epoch [100/200], Step [24/63] Loss: 0.29634\n",
            "Epoch [100/200], Step [25/63] Loss: 0.33378\n",
            "Epoch [100/200], Step [26/63] Loss: 0.60164\n",
            "Epoch [100/200], Step [27/63] Loss: 0.44794\n",
            "Epoch [100/200], Step [28/63] Loss: 0.92735\n",
            "Epoch [100/200], Step [29/63] Loss: 0.55824\n",
            "Epoch [100/200], Step [30/63] Loss: 0.67269\n",
            "Epoch [100/200], Step [31/63] Loss: 0.63761\n",
            "Epoch [100/200], Step [32/63] Loss: 0.52650\n",
            "Epoch [100/200], Step [33/63] Loss: 0.53018\n",
            "Epoch [100/200], Step [34/63] Loss: 0.29845\n",
            "Epoch [100/200], Step [35/63] Loss: 0.47281\n",
            "Epoch [100/200], Step [36/63] Loss: 0.34176\n",
            "Epoch [100/200], Step [37/63] Loss: 0.39007\n",
            "Epoch [100/200], Step [38/63] Loss: 0.40419\n",
            "Epoch [100/200], Step [39/63] Loss: 0.35037\n",
            "Epoch [100/200], Step [40/63] Loss: 0.35878\n",
            "Epoch [100/200], Step [41/63] Loss: 0.38031\n",
            "Epoch [100/200], Step [42/63] Loss: 0.35951\n",
            "Epoch [100/200], Step [43/63] Loss: 0.46377\n",
            "Epoch [100/200], Step [44/63] Loss: 0.76805\n",
            "Epoch [100/200], Step [45/63] Loss: 0.56049\n",
            "Epoch [100/200], Step [46/63] Loss: 0.72717\n",
            "Epoch [100/200], Step [47/63] Loss: 0.38385\n",
            "Epoch [100/200], Step [48/63] Loss: 0.66673\n",
            "Epoch [100/200], Step [49/63] Loss: 0.36840\n",
            "Epoch [100/200], Step [50/63] Loss: 0.87462\n",
            "Epoch [100/200], Step [51/63] Loss: 0.51988\n",
            "Epoch [100/200], Step [52/63] Loss: 0.52382\n",
            "Epoch [100/200], Step [53/63] Loss: 0.58889\n",
            "Epoch [100/200], Step [54/63] Loss: 0.60426\n",
            "Epoch [100/200], Step [55/63] Loss: 0.59592\n",
            "Epoch [100/200], Step [56/63] Loss: 0.86908\n",
            "Epoch [100/200], Step [57/63] Loss: 0.39229\n",
            "Epoch [100/200], Step [58/63] Loss: 0.41028\n",
            "Epoch [100/200], Step [59/63] Loss: 0.44085\n",
            "Epoch [100/200], Step [60/63] Loss: 0.63497\n",
            "Epoch [100/200], Step [61/63] Loss: 0.43559\n",
            "Epoch [100/200], Step [62/63] Loss: 0.67477\n",
            "Epoch [100/200], Step [63/63] Loss: 0.32722\n",
            "Valid Accuracy: 65.27777777777777 %\n",
            "Epoch [101/200], Step [1/63] Loss: 0.44426\n",
            "Epoch [101/200], Step [2/63] Loss: 0.32688\n",
            "Epoch [101/200], Step [3/63] Loss: 0.38419\n",
            "Epoch [101/200], Step [4/63] Loss: 0.30486\n",
            "Epoch [101/200], Step [5/63] Loss: 0.43888\n",
            "Epoch [101/200], Step [6/63] Loss: 0.44175\n",
            "Epoch [101/200], Step [7/63] Loss: 0.73525\n",
            "Epoch [101/200], Step [8/63] Loss: 0.73666\n",
            "Epoch [101/200], Step [9/63] Loss: 0.54357\n",
            "Epoch [101/200], Step [10/63] Loss: 0.55356\n",
            "Epoch [101/200], Step [11/63] Loss: 0.40504\n",
            "Epoch [101/200], Step [12/63] Loss: 0.30596\n",
            "Epoch [101/200], Step [13/63] Loss: 0.37746\n",
            "Epoch [101/200], Step [14/63] Loss: 0.51009\n",
            "Epoch [101/200], Step [15/63] Loss: 0.50991\n",
            "Epoch [101/200], Step [16/63] Loss: 0.32791\n",
            "Epoch [101/200], Step [17/63] Loss: 0.47673\n",
            "Epoch [101/200], Step [18/63] Loss: 0.55155\n",
            "Epoch [101/200], Step [19/63] Loss: 0.48491\n",
            "Epoch [101/200], Step [20/63] Loss: 0.46276\n",
            "Epoch [101/200], Step [21/63] Loss: 0.32317\n",
            "Epoch [101/200], Step [22/63] Loss: 0.38136\n",
            "Epoch [101/200], Step [23/63] Loss: 0.49399\n",
            "Epoch [101/200], Step [24/63] Loss: 0.39067\n",
            "Epoch [101/200], Step [25/63] Loss: 0.45613\n",
            "Epoch [101/200], Step [26/63] Loss: 0.46278\n",
            "Epoch [101/200], Step [27/63] Loss: 0.39658\n",
            "Epoch [101/200], Step [28/63] Loss: 0.71002\n",
            "Epoch [101/200], Step [29/63] Loss: 0.39333\n",
            "Epoch [101/200], Step [30/63] Loss: 0.29546\n",
            "Epoch [101/200], Step [31/63] Loss: 0.86325\n",
            "Epoch [101/200], Step [32/63] Loss: 0.83625\n",
            "Epoch [101/200], Step [33/63] Loss: 0.59276\n",
            "Epoch [101/200], Step [34/63] Loss: 0.32726\n",
            "Epoch [101/200], Step [35/63] Loss: 0.55084\n",
            "Epoch [101/200], Step [36/63] Loss: 0.48906\n",
            "Epoch [101/200], Step [37/63] Loss: 0.54123\n",
            "Epoch [101/200], Step [38/63] Loss: 0.50495\n",
            "Epoch [101/200], Step [39/63] Loss: 0.26115\n",
            "Epoch [101/200], Step [40/63] Loss: 0.55639\n",
            "Epoch [101/200], Step [41/63] Loss: 0.59695\n",
            "Epoch [101/200], Step [42/63] Loss: 0.80534\n",
            "Epoch [101/200], Step [43/63] Loss: 0.51835\n",
            "Epoch [101/200], Step [44/63] Loss: 0.56333\n",
            "Epoch [101/200], Step [45/63] Loss: 0.37738\n",
            "Epoch [101/200], Step [46/63] Loss: 0.62685\n",
            "Epoch [101/200], Step [47/63] Loss: 0.50256\n",
            "Epoch [101/200], Step [48/63] Loss: 0.54996\n",
            "Epoch [101/200], Step [49/63] Loss: 0.56686\n",
            "Epoch [101/200], Step [50/63] Loss: 0.69385\n",
            "Epoch [101/200], Step [51/63] Loss: 0.70837\n",
            "Epoch [101/200], Step [52/63] Loss: 0.44141\n",
            "Epoch [101/200], Step [53/63] Loss: 0.69901\n",
            "Epoch [101/200], Step [54/63] Loss: 0.89203\n",
            "Epoch [101/200], Step [55/63] Loss: 0.59252\n",
            "Epoch [101/200], Step [56/63] Loss: 0.48164\n",
            "Epoch [101/200], Step [57/63] Loss: 0.41150\n",
            "Epoch [101/200], Step [58/63] Loss: 0.44745\n",
            "Epoch [101/200], Step [59/63] Loss: 0.56170\n",
            "Epoch [101/200], Step [60/63] Loss: 0.98357\n",
            "Epoch [101/200], Step [61/63] Loss: 0.53431\n",
            "Epoch [101/200], Step [62/63] Loss: 0.71866\n",
            "Epoch [101/200], Step [63/63] Loss: 0.41006\n",
            "Valid Accuracy: 67.12962962962963 %\n",
            "Epoch [102/200], Step [1/63] Loss: 0.47285\n",
            "Epoch [102/200], Step [2/63] Loss: 0.41027\n",
            "Epoch [102/200], Step [3/63] Loss: 0.55989\n",
            "Epoch [102/200], Step [4/63] Loss: 0.42929\n",
            "Epoch [102/200], Step [5/63] Loss: 0.51960\n",
            "Epoch [102/200], Step [6/63] Loss: 0.80022\n",
            "Epoch [102/200], Step [7/63] Loss: 0.41611\n",
            "Epoch [102/200], Step [8/63] Loss: 0.29248\n",
            "Epoch [102/200], Step [9/63] Loss: 0.35833\n",
            "Epoch [102/200], Step [10/63] Loss: 0.54505\n",
            "Epoch [102/200], Step [11/63] Loss: 0.53233\n",
            "Epoch [102/200], Step [12/63] Loss: 0.30843\n",
            "Epoch [102/200], Step [13/63] Loss: 0.47545\n",
            "Epoch [102/200], Step [14/63] Loss: 0.26130\n",
            "Epoch [102/200], Step [15/63] Loss: 0.53109\n",
            "Epoch [102/200], Step [16/63] Loss: 0.29850\n",
            "Epoch [102/200], Step [17/63] Loss: 0.47515\n",
            "Epoch [102/200], Step [18/63] Loss: 0.65188\n",
            "Epoch [102/200], Step [19/63] Loss: 0.81161\n",
            "Epoch [102/200], Step [20/63] Loss: 0.35484\n",
            "Epoch [102/200], Step [21/63] Loss: 0.33934\n",
            "Epoch [102/200], Step [22/63] Loss: 0.61170\n",
            "Epoch [102/200], Step [23/63] Loss: 0.60393\n",
            "Epoch [102/200], Step [24/63] Loss: 0.65828\n",
            "Epoch [102/200], Step [25/63] Loss: 0.66043\n",
            "Epoch [102/200], Step [26/63] Loss: 0.50012\n",
            "Epoch [102/200], Step [27/63] Loss: 0.37703\n",
            "Epoch [102/200], Step [28/63] Loss: 0.30834\n",
            "Epoch [102/200], Step [29/63] Loss: 0.54916\n",
            "Epoch [102/200], Step [30/63] Loss: 0.47727\n",
            "Epoch [102/200], Step [31/63] Loss: 0.54508\n",
            "Epoch [102/200], Step [32/63] Loss: 0.33950\n",
            "Epoch [102/200], Step [33/63] Loss: 0.25646\n",
            "Epoch [102/200], Step [34/63] Loss: 0.58867\n",
            "Epoch [102/200], Step [35/63] Loss: 0.37407\n",
            "Epoch [102/200], Step [36/63] Loss: 0.65897\n",
            "Epoch [102/200], Step [37/63] Loss: 1.12678\n",
            "Epoch [102/200], Step [38/63] Loss: 0.63802\n",
            "Epoch [102/200], Step [39/63] Loss: 0.35704\n",
            "Epoch [102/200], Step [40/63] Loss: 1.01875\n",
            "Epoch [102/200], Step [41/63] Loss: 0.92456\n",
            "Epoch [102/200], Step [42/63] Loss: 0.56956\n",
            "Epoch [102/200], Step [43/63] Loss: 0.80535\n",
            "Epoch [102/200], Step [44/63] Loss: 0.43430\n",
            "Epoch [102/200], Step [45/63] Loss: 0.27600\n",
            "Epoch [102/200], Step [46/63] Loss: 0.95869\n",
            "Epoch [102/200], Step [47/63] Loss: 0.49747\n",
            "Epoch [102/200], Step [48/63] Loss: 0.63121\n",
            "Epoch [102/200], Step [49/63] Loss: 0.45002\n",
            "Epoch [102/200], Step [50/63] Loss: 0.34283\n",
            "Epoch [102/200], Step [51/63] Loss: 0.64126\n",
            "Epoch [102/200], Step [52/63] Loss: 0.61830\n",
            "Epoch [102/200], Step [53/63] Loss: 0.72319\n",
            "Epoch [102/200], Step [54/63] Loss: 0.53579\n",
            "Epoch [102/200], Step [55/63] Loss: 0.60545\n",
            "Epoch [102/200], Step [56/63] Loss: 0.60036\n",
            "Epoch [102/200], Step [57/63] Loss: 0.46633\n",
            "Epoch [102/200], Step [58/63] Loss: 0.59370\n",
            "Epoch [102/200], Step [59/63] Loss: 0.45060\n",
            "Epoch [102/200], Step [60/63] Loss: 0.52781\n",
            "Epoch [102/200], Step [61/63] Loss: 0.70809\n",
            "Epoch [102/200], Step [62/63] Loss: 0.48344\n",
            "Epoch [102/200], Step [63/63] Loss: 0.41096\n",
            "Valid Accuracy: 63.19444444444444 %\n",
            "Epoch [103/200], Step [1/63] Loss: 0.42838\n",
            "Epoch [103/200], Step [2/63] Loss: 0.68230\n",
            "Epoch [103/200], Step [3/63] Loss: 0.55497\n",
            "Epoch [103/200], Step [4/63] Loss: 0.41286\n",
            "Epoch [103/200], Step [5/63] Loss: 0.69665\n",
            "Epoch [103/200], Step [6/63] Loss: 0.65625\n",
            "Epoch [103/200], Step [7/63] Loss: 0.56846\n",
            "Epoch [103/200], Step [8/63] Loss: 0.42078\n",
            "Epoch [103/200], Step [9/63] Loss: 0.48590\n",
            "Epoch [103/200], Step [10/63] Loss: 0.38559\n",
            "Epoch [103/200], Step [11/63] Loss: 0.54627\n",
            "Epoch [103/200], Step [12/63] Loss: 0.49849\n",
            "Epoch [103/200], Step [13/63] Loss: 0.62737\n",
            "Epoch [103/200], Step [14/63] Loss: 0.64062\n",
            "Epoch [103/200], Step [15/63] Loss: 0.57908\n",
            "Epoch [103/200], Step [16/63] Loss: 0.49092\n",
            "Epoch [103/200], Step [17/63] Loss: 0.50147\n",
            "Epoch [103/200], Step [18/63] Loss: 0.37308\n",
            "Epoch [103/200], Step [19/63] Loss: 0.46807\n",
            "Epoch [103/200], Step [20/63] Loss: 0.36516\n",
            "Epoch [103/200], Step [21/63] Loss: 0.70027\n",
            "Epoch [103/200], Step [22/63] Loss: 0.70245\n",
            "Epoch [103/200], Step [23/63] Loss: 0.60344\n",
            "Epoch [103/200], Step [24/63] Loss: 0.70914\n",
            "Epoch [103/200], Step [25/63] Loss: 0.39616\n",
            "Epoch [103/200], Step [26/63] Loss: 0.50983\n",
            "Epoch [103/200], Step [27/63] Loss: 0.95331\n",
            "Epoch [103/200], Step [28/63] Loss: 0.52205\n",
            "Epoch [103/200], Step [29/63] Loss: 0.58347\n",
            "Epoch [103/200], Step [30/63] Loss: 0.43652\n",
            "Epoch [103/200], Step [31/63] Loss: 0.29861\n",
            "Epoch [103/200], Step [32/63] Loss: 0.75086\n",
            "Epoch [103/200], Step [33/63] Loss: 0.52205\n",
            "Epoch [103/200], Step [34/63] Loss: 0.23893\n",
            "Epoch [103/200], Step [35/63] Loss: 0.52380\n",
            "Epoch [103/200], Step [36/63] Loss: 0.26406\n",
            "Epoch [103/200], Step [37/63] Loss: 0.53625\n",
            "Epoch [103/200], Step [38/63] Loss: 0.42793\n",
            "Epoch [103/200], Step [39/63] Loss: 0.95240\n",
            "Epoch [103/200], Step [40/63] Loss: 0.74599\n",
            "Epoch [103/200], Step [41/63] Loss: 0.29207\n",
            "Epoch [103/200], Step [42/63] Loss: 0.29875\n",
            "Epoch [103/200], Step [43/63] Loss: 0.42022\n",
            "Epoch [103/200], Step [44/63] Loss: 0.42600\n",
            "Epoch [103/200], Step [45/63] Loss: 0.44041\n",
            "Epoch [103/200], Step [46/63] Loss: 0.42933\n",
            "Epoch [103/200], Step [47/63] Loss: 0.50766\n",
            "Epoch [103/200], Step [48/63] Loss: 0.34723\n",
            "Epoch [103/200], Step [49/63] Loss: 0.39712\n",
            "Epoch [103/200], Step [50/63] Loss: 0.28332\n",
            "Epoch [103/200], Step [51/63] Loss: 0.49259\n",
            "Epoch [103/200], Step [52/63] Loss: 0.66741\n",
            "Epoch [103/200], Step [53/63] Loss: 0.79376\n",
            "Epoch [103/200], Step [54/63] Loss: 1.11629\n",
            "Epoch [103/200], Step [55/63] Loss: 0.31941\n",
            "Epoch [103/200], Step [56/63] Loss: 0.58650\n",
            "Epoch [103/200], Step [57/63] Loss: 0.45763\n",
            "Epoch [103/200], Step [58/63] Loss: 0.40049\n",
            "Epoch [103/200], Step [59/63] Loss: 0.36101\n",
            "Epoch [103/200], Step [60/63] Loss: 0.40906\n",
            "Epoch [103/200], Step [61/63] Loss: 0.35822\n",
            "Epoch [103/200], Step [62/63] Loss: 0.50686\n",
            "Epoch [103/200], Step [63/63] Loss: 0.70530\n",
            "Valid Accuracy: 64.35185185185185 %\n",
            "Epoch [104/200], Step [1/63] Loss: 0.59656\n",
            "Epoch [104/200], Step [2/63] Loss: 0.51495\n",
            "Epoch [104/200], Step [3/63] Loss: 0.45266\n",
            "Epoch [104/200], Step [4/63] Loss: 0.43831\n",
            "Epoch [104/200], Step [5/63] Loss: 0.46203\n",
            "Epoch [104/200], Step [6/63] Loss: 0.48023\n",
            "Epoch [104/200], Step [7/63] Loss: 0.37562\n",
            "Epoch [104/200], Step [8/63] Loss: 0.62203\n",
            "Epoch [104/200], Step [9/63] Loss: 0.56759\n",
            "Epoch [104/200], Step [10/63] Loss: 0.46403\n",
            "Epoch [104/200], Step [11/63] Loss: 0.43928\n",
            "Epoch [104/200], Step [12/63] Loss: 0.40758\n",
            "Epoch [104/200], Step [13/63] Loss: 0.63427\n",
            "Epoch [104/200], Step [14/63] Loss: 0.42784\n",
            "Epoch [104/200], Step [15/63] Loss: 0.69073\n",
            "Epoch [104/200], Step [16/63] Loss: 0.32973\n",
            "Epoch [104/200], Step [17/63] Loss: 0.41085\n",
            "Epoch [104/200], Step [18/63] Loss: 0.83840\n",
            "Epoch [104/200], Step [19/63] Loss: 0.44577\n",
            "Epoch [104/200], Step [20/63] Loss: 0.66213\n",
            "Epoch [104/200], Step [21/63] Loss: 0.24785\n",
            "Epoch [104/200], Step [22/63] Loss: 0.44801\n",
            "Epoch [104/200], Step [23/63] Loss: 0.63398\n",
            "Epoch [104/200], Step [24/63] Loss: 0.42070\n",
            "Epoch [104/200], Step [25/63] Loss: 0.37796\n",
            "Epoch [104/200], Step [26/63] Loss: 0.28762\n",
            "Epoch [104/200], Step [27/63] Loss: 0.45001\n",
            "Epoch [104/200], Step [28/63] Loss: 0.54459\n",
            "Epoch [104/200], Step [29/63] Loss: 0.44497\n",
            "Epoch [104/200], Step [30/63] Loss: 0.46271\n",
            "Epoch [104/200], Step [31/63] Loss: 0.49161\n",
            "Epoch [104/200], Step [32/63] Loss: 0.37318\n",
            "Epoch [104/200], Step [33/63] Loss: 0.42477\n",
            "Epoch [104/200], Step [34/63] Loss: 0.43896\n",
            "Epoch [104/200], Step [35/63] Loss: 0.57252\n",
            "Epoch [104/200], Step [36/63] Loss: 0.39109\n",
            "Epoch [104/200], Step [37/63] Loss: 0.31012\n",
            "Epoch [104/200], Step [38/63] Loss: 0.36709\n",
            "Epoch [104/200], Step [39/63] Loss: 0.38525\n",
            "Epoch [104/200], Step [40/63] Loss: 0.40283\n",
            "Epoch [104/200], Step [41/63] Loss: 0.63482\n",
            "Epoch [104/200], Step [42/63] Loss: 0.64626\n",
            "Epoch [104/200], Step [43/63] Loss: 0.31817\n",
            "Epoch [104/200], Step [44/63] Loss: 0.58575\n",
            "Epoch [104/200], Step [45/63] Loss: 0.54986\n",
            "Epoch [104/200], Step [46/63] Loss: 0.47759\n",
            "Epoch [104/200], Step [47/63] Loss: 0.34875\n",
            "Epoch [104/200], Step [48/63] Loss: 0.45141\n",
            "Epoch [104/200], Step [49/63] Loss: 0.24523\n",
            "Epoch [104/200], Step [50/63] Loss: 0.56636\n",
            "Epoch [104/200], Step [51/63] Loss: 0.75341\n",
            "Epoch [104/200], Step [52/63] Loss: 0.41903\n",
            "Epoch [104/200], Step [53/63] Loss: 0.30053\n",
            "Epoch [104/200], Step [54/63] Loss: 0.67768\n",
            "Epoch [104/200], Step [55/63] Loss: 0.38907\n",
            "Epoch [104/200], Step [56/63] Loss: 0.64205\n",
            "Epoch [104/200], Step [57/63] Loss: 0.33052\n",
            "Epoch [104/200], Step [58/63] Loss: 0.22681\n",
            "Epoch [104/200], Step [59/63] Loss: 0.35113\n",
            "Epoch [104/200], Step [60/63] Loss: 0.55798\n",
            "Epoch [104/200], Step [61/63] Loss: 0.49832\n",
            "Epoch [104/200], Step [62/63] Loss: 0.58356\n",
            "Epoch [104/200], Step [63/63] Loss: 0.48808\n",
            "Valid Accuracy: 54.398148148148145 %\n",
            "Epoch [105/200], Step [1/63] Loss: 0.26848\n",
            "Epoch [105/200], Step [2/63] Loss: 0.47473\n",
            "Epoch [105/200], Step [3/63] Loss: 0.55515\n",
            "Epoch [105/200], Step [4/63] Loss: 0.39214\n",
            "Epoch [105/200], Step [5/63] Loss: 0.37616\n",
            "Epoch [105/200], Step [6/63] Loss: 0.27744\n",
            "Epoch [105/200], Step [7/63] Loss: 0.39590\n",
            "Epoch [105/200], Step [8/63] Loss: 0.32876\n",
            "Epoch [105/200], Step [9/63] Loss: 0.45132\n",
            "Epoch [105/200], Step [10/63] Loss: 0.34955\n",
            "Epoch [105/200], Step [11/63] Loss: 0.74489\n",
            "Epoch [105/200], Step [12/63] Loss: 0.67857\n",
            "Epoch [105/200], Step [13/63] Loss: 0.51495\n",
            "Epoch [105/200], Step [14/63] Loss: 0.62517\n",
            "Epoch [105/200], Step [15/63] Loss: 0.50241\n",
            "Epoch [105/200], Step [16/63] Loss: 0.57162\n",
            "Epoch [105/200], Step [17/63] Loss: 0.31473\n",
            "Epoch [105/200], Step [18/63] Loss: 0.81391\n",
            "Epoch [105/200], Step [19/63] Loss: 0.31582\n",
            "Epoch [105/200], Step [20/63] Loss: 0.51227\n",
            "Epoch [105/200], Step [21/63] Loss: 0.51672\n",
            "Epoch [105/200], Step [22/63] Loss: 0.44186\n",
            "Epoch [105/200], Step [23/63] Loss: 0.79273\n",
            "Epoch [105/200], Step [24/63] Loss: 0.39608\n",
            "Epoch [105/200], Step [25/63] Loss: 0.53259\n",
            "Epoch [105/200], Step [26/63] Loss: 0.39607\n",
            "Epoch [105/200], Step [27/63] Loss: 0.57322\n",
            "Epoch [105/200], Step [28/63] Loss: 0.36852\n",
            "Epoch [105/200], Step [29/63] Loss: 0.38300\n",
            "Epoch [105/200], Step [30/63] Loss: 0.61387\n",
            "Epoch [105/200], Step [31/63] Loss: 0.52819\n",
            "Epoch [105/200], Step [32/63] Loss: 0.32155\n",
            "Epoch [105/200], Step [33/63] Loss: 0.39276\n",
            "Epoch [105/200], Step [34/63] Loss: 0.48134\n",
            "Epoch [105/200], Step [35/63] Loss: 0.77363\n",
            "Epoch [105/200], Step [36/63] Loss: 0.45572\n",
            "Epoch [105/200], Step [37/63] Loss: 0.23193\n",
            "Epoch [105/200], Step [38/63] Loss: 0.44296\n",
            "Epoch [105/200], Step [39/63] Loss: 0.57820\n",
            "Epoch [105/200], Step [40/63] Loss: 0.62588\n",
            "Epoch [105/200], Step [41/63] Loss: 0.43816\n",
            "Epoch [105/200], Step [42/63] Loss: 0.45233\n",
            "Epoch [105/200], Step [43/63] Loss: 0.40555\n",
            "Epoch [105/200], Step [44/63] Loss: 0.49391\n",
            "Epoch [105/200], Step [45/63] Loss: 0.51857\n",
            "Epoch [105/200], Step [46/63] Loss: 0.70518\n",
            "Epoch [105/200], Step [47/63] Loss: 0.43676\n",
            "Epoch [105/200], Step [48/63] Loss: 0.82632\n",
            "Epoch [105/200], Step [49/63] Loss: 0.33371\n",
            "Epoch [105/200], Step [50/63] Loss: 0.43382\n",
            "Epoch [105/200], Step [51/63] Loss: 0.33254\n",
            "Epoch [105/200], Step [52/63] Loss: 0.36826\n",
            "Epoch [105/200], Step [53/63] Loss: 0.59752\n",
            "Epoch [105/200], Step [54/63] Loss: 0.39612\n",
            "Epoch [105/200], Step [55/63] Loss: 0.38608\n",
            "Epoch [105/200], Step [56/63] Loss: 0.45555\n",
            "Epoch [105/200], Step [57/63] Loss: 0.48143\n",
            "Epoch [105/200], Step [58/63] Loss: 0.47732\n",
            "Epoch [105/200], Step [59/63] Loss: 0.62927\n",
            "Epoch [105/200], Step [60/63] Loss: 0.37447\n",
            "Epoch [105/200], Step [61/63] Loss: 0.45014\n",
            "Epoch [105/200], Step [62/63] Loss: 0.54180\n",
            "Epoch [105/200], Step [63/63] Loss: 0.60683\n",
            "Valid Accuracy: 62.96296296296296 %\n",
            "Epoch [106/200], Step [1/63] Loss: 0.43854\n",
            "Epoch [106/200], Step [2/63] Loss: 0.27366\n",
            "Epoch [106/200], Step [3/63] Loss: 0.57786\n",
            "Epoch [106/200], Step [4/63] Loss: 0.37782\n",
            "Epoch [106/200], Step [5/63] Loss: 0.41568\n",
            "Epoch [106/200], Step [6/63] Loss: 0.44076\n",
            "Epoch [106/200], Step [7/63] Loss: 0.35928\n",
            "Epoch [106/200], Step [8/63] Loss: 0.60580\n",
            "Epoch [106/200], Step [9/63] Loss: 0.39078\n",
            "Epoch [106/200], Step [10/63] Loss: 0.50922\n",
            "Epoch [106/200], Step [11/63] Loss: 0.22188\n",
            "Epoch [106/200], Step [12/63] Loss: 0.59707\n",
            "Epoch [106/200], Step [13/63] Loss: 0.61997\n",
            "Epoch [106/200], Step [14/63] Loss: 0.16125\n",
            "Epoch [106/200], Step [15/63] Loss: 0.51721\n",
            "Epoch [106/200], Step [16/63] Loss: 0.32240\n",
            "Epoch [106/200], Step [17/63] Loss: 0.48123\n",
            "Epoch [106/200], Step [18/63] Loss: 0.75826\n",
            "Epoch [106/200], Step [19/63] Loss: 0.30261\n",
            "Epoch [106/200], Step [20/63] Loss: 0.36031\n",
            "Epoch [106/200], Step [21/63] Loss: 0.35581\n",
            "Epoch [106/200], Step [22/63] Loss: 0.30772\n",
            "Epoch [106/200], Step [23/63] Loss: 0.83552\n",
            "Epoch [106/200], Step [24/63] Loss: 0.51650\n",
            "Epoch [106/200], Step [25/63] Loss: 0.35918\n",
            "Epoch [106/200], Step [26/63] Loss: 0.45294\n",
            "Epoch [106/200], Step [27/63] Loss: 0.57286\n",
            "Epoch [106/200], Step [28/63] Loss: 0.56764\n",
            "Epoch [106/200], Step [29/63] Loss: 0.47545\n",
            "Epoch [106/200], Step [30/63] Loss: 0.39498\n",
            "Epoch [106/200], Step [31/63] Loss: 0.49906\n",
            "Epoch [106/200], Step [32/63] Loss: 0.64084\n",
            "Epoch [106/200], Step [33/63] Loss: 0.33576\n",
            "Epoch [106/200], Step [34/63] Loss: 0.45555\n",
            "Epoch [106/200], Step [35/63] Loss: 0.29465\n",
            "Epoch [106/200], Step [36/63] Loss: 0.39541\n",
            "Epoch [106/200], Step [37/63] Loss: 0.45162\n",
            "Epoch [106/200], Step [38/63] Loss: 0.24199\n",
            "Epoch [106/200], Step [39/63] Loss: 0.29646\n",
            "Epoch [106/200], Step [40/63] Loss: 0.26103\n",
            "Epoch [106/200], Step [41/63] Loss: 0.39686\n",
            "Epoch [106/200], Step [42/63] Loss: 0.35547\n",
            "Epoch [106/200], Step [43/63] Loss: 0.69011\n",
            "Epoch [106/200], Step [44/63] Loss: 0.72782\n",
            "Epoch [106/200], Step [45/63] Loss: 0.41343\n",
            "Epoch [106/200], Step [46/63] Loss: 0.25467\n",
            "Epoch [106/200], Step [47/63] Loss: 0.39078\n",
            "Epoch [106/200], Step [48/63] Loss: 0.43287\n",
            "Epoch [106/200], Step [49/63] Loss: 0.41294\n",
            "Epoch [106/200], Step [50/63] Loss: 0.38588\n",
            "Epoch [106/200], Step [51/63] Loss: 0.43520\n",
            "Epoch [106/200], Step [52/63] Loss: 0.57880\n",
            "Epoch [106/200], Step [53/63] Loss: 0.43627\n",
            "Epoch [106/200], Step [54/63] Loss: 0.28925\n",
            "Epoch [106/200], Step [55/63] Loss: 0.28853\n",
            "Epoch [106/200], Step [56/63] Loss: 0.60877\n",
            "Epoch [106/200], Step [57/63] Loss: 0.44481\n",
            "Epoch [106/200], Step [58/63] Loss: 0.63244\n",
            "Epoch [106/200], Step [59/63] Loss: 0.25983\n",
            "Epoch [106/200], Step [60/63] Loss: 0.32064\n",
            "Epoch [106/200], Step [61/63] Loss: 0.42907\n",
            "Epoch [106/200], Step [62/63] Loss: 0.37685\n",
            "Epoch [106/200], Step [63/63] Loss: 0.42881\n",
            "Valid Accuracy: 50.69444444444444 %\n",
            "Epoch [107/200], Step [1/63] Loss: 0.32513\n",
            "Epoch [107/200], Step [2/63] Loss: 0.35708\n",
            "Epoch [107/200], Step [3/63] Loss: 0.37000\n",
            "Epoch [107/200], Step [4/63] Loss: 0.55173\n",
            "Epoch [107/200], Step [5/63] Loss: 0.46500\n",
            "Epoch [107/200], Step [6/63] Loss: 0.27159\n",
            "Epoch [107/200], Step [7/63] Loss: 0.78012\n",
            "Epoch [107/200], Step [8/63] Loss: 0.56035\n",
            "Epoch [107/200], Step [9/63] Loss: 0.25742\n",
            "Epoch [107/200], Step [10/63] Loss: 0.49474\n",
            "Epoch [107/200], Step [11/63] Loss: 0.32071\n",
            "Epoch [107/200], Step [12/63] Loss: 0.45334\n",
            "Epoch [107/200], Step [13/63] Loss: 0.51968\n",
            "Epoch [107/200], Step [14/63] Loss: 0.38385\n",
            "Epoch [107/200], Step [15/63] Loss: 0.33877\n",
            "Epoch [107/200], Step [16/63] Loss: 0.31219\n",
            "Epoch [107/200], Step [17/63] Loss: 0.48967\n",
            "Epoch [107/200], Step [18/63] Loss: 0.55659\n",
            "Epoch [107/200], Step [19/63] Loss: 0.46599\n",
            "Epoch [107/200], Step [20/63] Loss: 0.33823\n",
            "Epoch [107/200], Step [21/63] Loss: 0.30947\n",
            "Epoch [107/200], Step [22/63] Loss: 0.42715\n",
            "Epoch [107/200], Step [23/63] Loss: 0.35024\n",
            "Epoch [107/200], Step [24/63] Loss: 0.61127\n",
            "Epoch [107/200], Step [25/63] Loss: 0.32044\n",
            "Epoch [107/200], Step [26/63] Loss: 0.38829\n",
            "Epoch [107/200], Step [27/63] Loss: 0.57478\n",
            "Epoch [107/200], Step [28/63] Loss: 0.57991\n",
            "Epoch [107/200], Step [29/63] Loss: 0.24576\n",
            "Epoch [107/200], Step [30/63] Loss: 0.25348\n",
            "Epoch [107/200], Step [31/63] Loss: 0.20518\n",
            "Epoch [107/200], Step [32/63] Loss: 0.68435\n",
            "Epoch [107/200], Step [33/63] Loss: 0.41924\n",
            "Epoch [107/200], Step [34/63] Loss: 0.46555\n",
            "Epoch [107/200], Step [35/63] Loss: 0.52646\n",
            "Epoch [107/200], Step [36/63] Loss: 0.20267\n",
            "Epoch [107/200], Step [37/63] Loss: 0.37361\n",
            "Epoch [107/200], Step [38/63] Loss: 0.42695\n",
            "Epoch [107/200], Step [39/63] Loss: 0.39191\n",
            "Epoch [107/200], Step [40/63] Loss: 0.62970\n",
            "Epoch [107/200], Step [41/63] Loss: 0.57724\n",
            "Epoch [107/200], Step [42/63] Loss: 0.28622\n",
            "Epoch [107/200], Step [43/63] Loss: 0.31296\n",
            "Epoch [107/200], Step [44/63] Loss: 0.52282\n",
            "Epoch [107/200], Step [45/63] Loss: 0.49229\n",
            "Epoch [107/200], Step [46/63] Loss: 0.62971\n",
            "Epoch [107/200], Step [47/63] Loss: 0.42614\n",
            "Epoch [107/200], Step [48/63] Loss: 0.49782\n",
            "Epoch [107/200], Step [49/63] Loss: 0.42742\n",
            "Epoch [107/200], Step [50/63] Loss: 0.34919\n",
            "Epoch [107/200], Step [51/63] Loss: 0.32517\n",
            "Epoch [107/200], Step [52/63] Loss: 0.37166\n",
            "Epoch [107/200], Step [53/63] Loss: 0.54548\n",
            "Epoch [107/200], Step [54/63] Loss: 0.48180\n",
            "Epoch [107/200], Step [55/63] Loss: 0.29953\n",
            "Epoch [107/200], Step [56/63] Loss: 0.50531\n",
            "Epoch [107/200], Step [57/63] Loss: 0.38417\n",
            "Epoch [107/200], Step [58/63] Loss: 0.27032\n",
            "Epoch [107/200], Step [59/63] Loss: 0.34332\n",
            "Epoch [107/200], Step [60/63] Loss: 0.84469\n",
            "Epoch [107/200], Step [61/63] Loss: 0.56410\n",
            "Epoch [107/200], Step [62/63] Loss: 0.51025\n",
            "Epoch [107/200], Step [63/63] Loss: 0.25658\n",
            "Valid Accuracy: 50.69444444444444 %\n",
            "Epoch [108/200], Step [1/63] Loss: 0.87197\n",
            "Epoch [108/200], Step [2/63] Loss: 0.35389\n",
            "Epoch [108/200], Step [3/63] Loss: 0.52619\n",
            "Epoch [108/200], Step [4/63] Loss: 0.27436\n",
            "Epoch [108/200], Step [5/63] Loss: 0.33398\n",
            "Epoch [108/200], Step [6/63] Loss: 0.40071\n",
            "Epoch [108/200], Step [7/63] Loss: 0.47438\n",
            "Epoch [108/200], Step [8/63] Loss: 0.64967\n",
            "Epoch [108/200], Step [9/63] Loss: 0.35446\n",
            "Epoch [108/200], Step [10/63] Loss: 0.25145\n",
            "Epoch [108/200], Step [11/63] Loss: 0.45542\n",
            "Epoch [108/200], Step [12/63] Loss: 0.38783\n",
            "Epoch [108/200], Step [13/63] Loss: 0.30316\n",
            "Epoch [108/200], Step [14/63] Loss: 0.40715\n",
            "Epoch [108/200], Step [15/63] Loss: 0.66435\n",
            "Epoch [108/200], Step [16/63] Loss: 0.44476\n",
            "Epoch [108/200], Step [17/63] Loss: 0.45116\n",
            "Epoch [108/200], Step [18/63] Loss: 0.48061\n",
            "Epoch [108/200], Step [19/63] Loss: 0.40155\n",
            "Epoch [108/200], Step [20/63] Loss: 0.32637\n",
            "Epoch [108/200], Step [21/63] Loss: 0.38446\n",
            "Epoch [108/200], Step [22/63] Loss: 0.42339\n",
            "Epoch [108/200], Step [23/63] Loss: 0.46785\n",
            "Epoch [108/200], Step [24/63] Loss: 0.40766\n",
            "Epoch [108/200], Step [25/63] Loss: 0.64425\n",
            "Epoch [108/200], Step [26/63] Loss: 0.35596\n",
            "Epoch [108/200], Step [27/63] Loss: 0.48740\n",
            "Epoch [108/200], Step [28/63] Loss: 0.76777\n",
            "Epoch [108/200], Step [29/63] Loss: 0.40401\n",
            "Epoch [108/200], Step [30/63] Loss: 0.33230\n",
            "Epoch [108/200], Step [31/63] Loss: 0.33489\n",
            "Epoch [108/200], Step [32/63] Loss: 0.38047\n",
            "Epoch [108/200], Step [33/63] Loss: 0.36229\n",
            "Epoch [108/200], Step [34/63] Loss: 0.52937\n",
            "Epoch [108/200], Step [35/63] Loss: 0.75872\n",
            "Epoch [108/200], Step [36/63] Loss: 0.24290\n",
            "Epoch [108/200], Step [37/63] Loss: 0.39253\n",
            "Epoch [108/200], Step [38/63] Loss: 0.33850\n",
            "Epoch [108/200], Step [39/63] Loss: 0.43970\n",
            "Epoch [108/200], Step [40/63] Loss: 0.42870\n",
            "Epoch [108/200], Step [41/63] Loss: 0.45276\n",
            "Epoch [108/200], Step [42/63] Loss: 0.54702\n",
            "Epoch [108/200], Step [43/63] Loss: 0.39013\n",
            "Epoch [108/200], Step [44/63] Loss: 0.38904\n",
            "Epoch [108/200], Step [45/63] Loss: 0.64132\n",
            "Epoch [108/200], Step [46/63] Loss: 0.49241\n",
            "Epoch [108/200], Step [47/63] Loss: 0.22416\n",
            "Epoch [108/200], Step [48/63] Loss: 0.46496\n",
            "Epoch [108/200], Step [49/63] Loss: 0.36010\n",
            "Epoch [108/200], Step [50/63] Loss: 0.38417\n",
            "Epoch [108/200], Step [51/63] Loss: 0.54826\n",
            "Epoch [108/200], Step [52/63] Loss: 0.61204\n",
            "Epoch [108/200], Step [53/63] Loss: 0.67229\n",
            "Epoch [108/200], Step [54/63] Loss: 0.34432\n",
            "Epoch [108/200], Step [55/63] Loss: 0.30825\n",
            "Epoch [108/200], Step [56/63] Loss: 0.48738\n",
            "Epoch [108/200], Step [57/63] Loss: 0.32075\n",
            "Epoch [108/200], Step [58/63] Loss: 0.72580\n",
            "Epoch [108/200], Step [59/63] Loss: 0.25039\n",
            "Epoch [108/200], Step [60/63] Loss: 0.31990\n",
            "Epoch [108/200], Step [61/63] Loss: 0.55892\n",
            "Epoch [108/200], Step [62/63] Loss: 0.28343\n",
            "Epoch [108/200], Step [63/63] Loss: 0.55127\n",
            "Valid Accuracy: 57.870370370370374 %\n",
            "Epoch [109/200], Step [1/63] Loss: 0.43345\n",
            "Epoch [109/200], Step [2/63] Loss: 0.52464\n",
            "Epoch [109/200], Step [3/63] Loss: 0.31290\n",
            "Epoch [109/200], Step [4/63] Loss: 0.28485\n",
            "Epoch [109/200], Step [5/63] Loss: 0.31182\n",
            "Epoch [109/200], Step [6/63] Loss: 0.32568\n",
            "Epoch [109/200], Step [7/63] Loss: 0.51679\n",
            "Epoch [109/200], Step [8/63] Loss: 0.32124\n",
            "Epoch [109/200], Step [9/63] Loss: 0.55477\n",
            "Epoch [109/200], Step [10/63] Loss: 0.28226\n",
            "Epoch [109/200], Step [11/63] Loss: 0.49281\n",
            "Epoch [109/200], Step [12/63] Loss: 0.47700\n",
            "Epoch [109/200], Step [13/63] Loss: 0.46886\n",
            "Epoch [109/200], Step [14/63] Loss: 0.29173\n",
            "Epoch [109/200], Step [15/63] Loss: 0.41777\n",
            "Epoch [109/200], Step [16/63] Loss: 0.33105\n",
            "Epoch [109/200], Step [17/63] Loss: 0.53735\n",
            "Epoch [109/200], Step [18/63] Loss: 0.26381\n",
            "Epoch [109/200], Step [19/63] Loss: 0.50494\n",
            "Epoch [109/200], Step [20/63] Loss: 0.82474\n",
            "Epoch [109/200], Step [21/63] Loss: 0.34081\n",
            "Epoch [109/200], Step [22/63] Loss: 0.55545\n",
            "Epoch [109/200], Step [23/63] Loss: 0.42777\n",
            "Epoch [109/200], Step [24/63] Loss: 0.34682\n",
            "Epoch [109/200], Step [25/63] Loss: 0.50392\n",
            "Epoch [109/200], Step [26/63] Loss: 0.67753\n",
            "Epoch [109/200], Step [27/63] Loss: 0.46172\n",
            "Epoch [109/200], Step [28/63] Loss: 0.71388\n",
            "Epoch [109/200], Step [29/63] Loss: 0.31118\n",
            "Epoch [109/200], Step [30/63] Loss: 0.56169\n",
            "Epoch [109/200], Step [31/63] Loss: 0.66885\n",
            "Epoch [109/200], Step [32/63] Loss: 0.41508\n",
            "Epoch [109/200], Step [33/63] Loss: 0.35290\n",
            "Epoch [109/200], Step [34/63] Loss: 0.39520\n",
            "Epoch [109/200], Step [35/63] Loss: 0.37033\n",
            "Epoch [109/200], Step [36/63] Loss: 0.40435\n",
            "Epoch [109/200], Step [37/63] Loss: 0.25229\n",
            "Epoch [109/200], Step [38/63] Loss: 0.32354\n",
            "Epoch [109/200], Step [39/63] Loss: 0.28032\n",
            "Epoch [109/200], Step [40/63] Loss: 0.29364\n",
            "Epoch [109/200], Step [41/63] Loss: 0.48373\n",
            "Epoch [109/200], Step [42/63] Loss: 0.52312\n",
            "Epoch [109/200], Step [43/63] Loss: 0.83622\n",
            "Epoch [109/200], Step [44/63] Loss: 0.40082\n",
            "Epoch [109/200], Step [45/63] Loss: 0.35172\n",
            "Epoch [109/200], Step [46/63] Loss: 0.29847\n",
            "Epoch [109/200], Step [47/63] Loss: 0.35333\n",
            "Epoch [109/200], Step [48/63] Loss: 0.59702\n",
            "Epoch [109/200], Step [49/63] Loss: 0.51156\n",
            "Epoch [109/200], Step [50/63] Loss: 0.35446\n",
            "Epoch [109/200], Step [51/63] Loss: 0.22072\n",
            "Epoch [109/200], Step [52/63] Loss: 0.24668\n",
            "Epoch [109/200], Step [53/63] Loss: 0.31869\n",
            "Epoch [109/200], Step [54/63] Loss: 0.43613\n",
            "Epoch [109/200], Step [55/63] Loss: 0.73207\n",
            "Epoch [109/200], Step [56/63] Loss: 0.45226\n",
            "Epoch [109/200], Step [57/63] Loss: 0.51682\n",
            "Epoch [109/200], Step [58/63] Loss: 0.53256\n",
            "Epoch [109/200], Step [59/63] Loss: 0.50806\n",
            "Epoch [109/200], Step [60/63] Loss: 0.28485\n",
            "Epoch [109/200], Step [61/63] Loss: 0.20627\n",
            "Epoch [109/200], Step [62/63] Loss: 0.56259\n",
            "Epoch [109/200], Step [63/63] Loss: 1.13212\n",
            "Valid Accuracy: 62.96296296296296 %\n",
            "Epoch [110/200], Step [1/63] Loss: 0.29716\n",
            "Epoch [110/200], Step [2/63] Loss: 0.41422\n",
            "Epoch [110/200], Step [3/63] Loss: 0.21186\n",
            "Epoch [110/200], Step [4/63] Loss: 0.35996\n",
            "Epoch [110/200], Step [5/63] Loss: 0.39410\n",
            "Epoch [110/200], Step [6/63] Loss: 0.34452\n",
            "Epoch [110/200], Step [7/63] Loss: 0.35690\n",
            "Epoch [110/200], Step [8/63] Loss: 0.37275\n",
            "Epoch [110/200], Step [9/63] Loss: 0.28331\n",
            "Epoch [110/200], Step [10/63] Loss: 0.23767\n",
            "Epoch [110/200], Step [11/63] Loss: 0.42072\n",
            "Epoch [110/200], Step [12/63] Loss: 0.37681\n",
            "Epoch [110/200], Step [13/63] Loss: 0.43261\n",
            "Epoch [110/200], Step [14/63] Loss: 0.34904\n",
            "Epoch [110/200], Step [15/63] Loss: 0.46954\n",
            "Epoch [110/200], Step [16/63] Loss: 0.39052\n",
            "Epoch [110/200], Step [17/63] Loss: 0.33219\n",
            "Epoch [110/200], Step [18/63] Loss: 0.47755\n",
            "Epoch [110/200], Step [19/63] Loss: 0.56326\n",
            "Epoch [110/200], Step [20/63] Loss: 0.32409\n",
            "Epoch [110/200], Step [21/63] Loss: 0.43158\n",
            "Epoch [110/200], Step [22/63] Loss: 0.34871\n",
            "Epoch [110/200], Step [23/63] Loss: 0.32274\n",
            "Epoch [110/200], Step [24/63] Loss: 0.60303\n",
            "Epoch [110/200], Step [25/63] Loss: 0.43067\n",
            "Epoch [110/200], Step [26/63] Loss: 0.35256\n",
            "Epoch [110/200], Step [27/63] Loss: 0.40893\n",
            "Epoch [110/200], Step [28/63] Loss: 0.37208\n",
            "Epoch [110/200], Step [29/63] Loss: 0.33617\n",
            "Epoch [110/200], Step [30/63] Loss: 0.52202\n",
            "Epoch [110/200], Step [31/63] Loss: 0.21054\n",
            "Epoch [110/200], Step [32/63] Loss: 0.48309\n",
            "Epoch [110/200], Step [33/63] Loss: 0.51281\n",
            "Epoch [110/200], Step [34/63] Loss: 0.62318\n",
            "Epoch [110/200], Step [35/63] Loss: 0.24812\n",
            "Epoch [110/200], Step [36/63] Loss: 0.41248\n",
            "Epoch [110/200], Step [37/63] Loss: 0.33421\n",
            "Epoch [110/200], Step [38/63] Loss: 0.32553\n",
            "Epoch [110/200], Step [39/63] Loss: 0.31742\n",
            "Epoch [110/200], Step [40/63] Loss: 0.87459\n",
            "Epoch [110/200], Step [41/63] Loss: 0.53467\n",
            "Epoch [110/200], Step [42/63] Loss: 0.23244\n",
            "Epoch [110/200], Step [43/63] Loss: 0.37933\n",
            "Epoch [110/200], Step [44/63] Loss: 0.42426\n",
            "Epoch [110/200], Step [45/63] Loss: 0.34271\n",
            "Epoch [110/200], Step [46/63] Loss: 0.37264\n",
            "Epoch [110/200], Step [47/63] Loss: 0.40967\n",
            "Epoch [110/200], Step [48/63] Loss: 0.28880\n",
            "Epoch [110/200], Step [49/63] Loss: 0.28529\n",
            "Epoch [110/200], Step [50/63] Loss: 0.41225\n",
            "Epoch [110/200], Step [51/63] Loss: 0.62879\n",
            "Epoch [110/200], Step [52/63] Loss: 0.42293\n",
            "Epoch [110/200], Step [53/63] Loss: 0.67551\n",
            "Epoch [110/200], Step [54/63] Loss: 0.32083\n",
            "Epoch [110/200], Step [55/63] Loss: 0.33616\n",
            "Epoch [110/200], Step [56/63] Loss: 0.40475\n",
            "Epoch [110/200], Step [57/63] Loss: 0.48321\n",
            "Epoch [110/200], Step [58/63] Loss: 0.48206\n",
            "Epoch [110/200], Step [59/63] Loss: 0.30408\n",
            "Epoch [110/200], Step [60/63] Loss: 0.41143\n",
            "Epoch [110/200], Step [61/63] Loss: 0.19401\n",
            "Epoch [110/200], Step [62/63] Loss: 0.32551\n",
            "Epoch [110/200], Step [63/63] Loss: 0.56108\n",
            "Valid Accuracy: 53.93518518518518 %\n",
            "Epoch [111/200], Step [1/63] Loss: 0.25569\n",
            "Epoch [111/200], Step [2/63] Loss: 0.36551\n",
            "Epoch [111/200], Step [3/63] Loss: 0.41161\n",
            "Epoch [111/200], Step [4/63] Loss: 0.49651\n",
            "Epoch [111/200], Step [5/63] Loss: 0.42426\n",
            "Epoch [111/200], Step [6/63] Loss: 0.21593\n",
            "Epoch [111/200], Step [7/63] Loss: 0.31842\n",
            "Epoch [111/200], Step [8/63] Loss: 0.38337\n",
            "Epoch [111/200], Step [9/63] Loss: 0.27445\n",
            "Epoch [111/200], Step [10/63] Loss: 0.48024\n",
            "Epoch [111/200], Step [11/63] Loss: 0.39542\n",
            "Epoch [111/200], Step [12/63] Loss: 0.51343\n",
            "Epoch [111/200], Step [13/63] Loss: 0.27258\n",
            "Epoch [111/200], Step [14/63] Loss: 0.42077\n",
            "Epoch [111/200], Step [15/63] Loss: 0.27279\n",
            "Epoch [111/200], Step [16/63] Loss: 0.36925\n",
            "Epoch [111/200], Step [17/63] Loss: 0.20168\n",
            "Epoch [111/200], Step [18/63] Loss: 0.11661\n",
            "Epoch [111/200], Step [19/63] Loss: 0.49463\n",
            "Epoch [111/200], Step [20/63] Loss: 0.50412\n",
            "Epoch [111/200], Step [21/63] Loss: 0.28945\n",
            "Epoch [111/200], Step [22/63] Loss: 0.30982\n",
            "Epoch [111/200], Step [23/63] Loss: 0.66061\n",
            "Epoch [111/200], Step [24/63] Loss: 0.16595\n",
            "Epoch [111/200], Step [25/63] Loss: 0.21231\n",
            "Epoch [111/200], Step [26/63] Loss: 0.28070\n",
            "Epoch [111/200], Step [27/63] Loss: 0.31814\n",
            "Epoch [111/200], Step [28/63] Loss: 0.22442\n",
            "Epoch [111/200], Step [29/63] Loss: 0.20229\n",
            "Epoch [111/200], Step [30/63] Loss: 0.25318\n",
            "Epoch [111/200], Step [31/63] Loss: 0.58515\n",
            "Epoch [111/200], Step [32/63] Loss: 0.31819\n",
            "Epoch [111/200], Step [33/63] Loss: 0.24040\n",
            "Epoch [111/200], Step [34/63] Loss: 0.42237\n",
            "Epoch [111/200], Step [35/63] Loss: 0.23564\n",
            "Epoch [111/200], Step [36/63] Loss: 0.38271\n",
            "Epoch [111/200], Step [37/63] Loss: 0.24781\n",
            "Epoch [111/200], Step [38/63] Loss: 0.31319\n",
            "Epoch [111/200], Step [39/63] Loss: 0.28473\n",
            "Epoch [111/200], Step [40/63] Loss: 0.31485\n",
            "Epoch [111/200], Step [41/63] Loss: 0.32777\n",
            "Epoch [111/200], Step [42/63] Loss: 0.23495\n",
            "Epoch [111/200], Step [43/63] Loss: 0.37306\n",
            "Epoch [111/200], Step [44/63] Loss: 0.55124\n",
            "Epoch [111/200], Step [45/63] Loss: 0.31085\n",
            "Epoch [111/200], Step [46/63] Loss: 0.84968\n",
            "Epoch [111/200], Step [47/63] Loss: 0.32778\n",
            "Epoch [111/200], Step [48/63] Loss: 0.34516\n",
            "Epoch [111/200], Step [49/63] Loss: 0.28784\n",
            "Epoch [111/200], Step [50/63] Loss: 0.36654\n",
            "Epoch [111/200], Step [51/63] Loss: 0.33541\n",
            "Epoch [111/200], Step [52/63] Loss: 0.38216\n",
            "Epoch [111/200], Step [53/63] Loss: 0.48622\n",
            "Epoch [111/200], Step [54/63] Loss: 0.43970\n",
            "Epoch [111/200], Step [55/63] Loss: 0.49171\n",
            "Epoch [111/200], Step [56/63] Loss: 0.51292\n",
            "Epoch [111/200], Step [57/63] Loss: 0.25692\n",
            "Epoch [111/200], Step [58/63] Loss: 0.31359\n",
            "Epoch [111/200], Step [59/63] Loss: 0.42230\n",
            "Epoch [111/200], Step [60/63] Loss: 0.70926\n",
            "Epoch [111/200], Step [61/63] Loss: 0.65195\n",
            "Epoch [111/200], Step [62/63] Loss: 0.54398\n",
            "Epoch [111/200], Step [63/63] Loss: 0.20446\n",
            "Valid Accuracy: 56.94444444444444 %\n",
            "Epoch [112/200], Step [1/63] Loss: 0.24349\n",
            "Epoch [112/200], Step [2/63] Loss: 0.37520\n",
            "Epoch [112/200], Step [3/63] Loss: 0.16929\n",
            "Epoch [112/200], Step [4/63] Loss: 0.21485\n",
            "Epoch [112/200], Step [5/63] Loss: 0.30057\n",
            "Epoch [112/200], Step [6/63] Loss: 0.32128\n",
            "Epoch [112/200], Step [7/63] Loss: 0.49378\n",
            "Epoch [112/200], Step [8/63] Loss: 0.45998\n",
            "Epoch [112/200], Step [9/63] Loss: 0.36899\n",
            "Epoch [112/200], Step [10/63] Loss: 0.98673\n",
            "Epoch [112/200], Step [11/63] Loss: 0.58772\n",
            "Epoch [112/200], Step [12/63] Loss: 0.24527\n",
            "Epoch [112/200], Step [13/63] Loss: 0.20592\n",
            "Epoch [112/200], Step [14/63] Loss: 0.25024\n",
            "Epoch [112/200], Step [15/63] Loss: 0.57553\n",
            "Epoch [112/200], Step [16/63] Loss: 0.31142\n",
            "Epoch [112/200], Step [17/63] Loss: 0.45344\n",
            "Epoch [112/200], Step [18/63] Loss: 0.57973\n",
            "Epoch [112/200], Step [19/63] Loss: 0.44823\n",
            "Epoch [112/200], Step [20/63] Loss: 0.57489\n",
            "Epoch [112/200], Step [21/63] Loss: 0.23553\n",
            "Epoch [112/200], Step [22/63] Loss: 0.64791\n",
            "Epoch [112/200], Step [23/63] Loss: 0.41636\n",
            "Epoch [112/200], Step [24/63] Loss: 0.39879\n",
            "Epoch [112/200], Step [25/63] Loss: 0.83431\n",
            "Epoch [112/200], Step [26/63] Loss: 0.57313\n",
            "Epoch [112/200], Step [27/63] Loss: 0.40884\n",
            "Epoch [112/200], Step [28/63] Loss: 0.55323\n",
            "Epoch [112/200], Step [29/63] Loss: 0.41201\n",
            "Epoch [112/200], Step [30/63] Loss: 0.58923\n",
            "Epoch [112/200], Step [31/63] Loss: 0.42061\n",
            "Epoch [112/200], Step [32/63] Loss: 0.71391\n",
            "Epoch [112/200], Step [33/63] Loss: 0.47682\n",
            "Epoch [112/200], Step [34/63] Loss: 0.36643\n",
            "Epoch [112/200], Step [35/63] Loss: 0.59132\n",
            "Epoch [112/200], Step [36/63] Loss: 0.40715\n",
            "Epoch [112/200], Step [37/63] Loss: 0.28310\n",
            "Epoch [112/200], Step [38/63] Loss: 0.31106\n",
            "Epoch [112/200], Step [39/63] Loss: 0.32753\n",
            "Epoch [112/200], Step [40/63] Loss: 0.16106\n",
            "Epoch [112/200], Step [41/63] Loss: 0.57521\n",
            "Epoch [112/200], Step [42/63] Loss: 0.30975\n",
            "Epoch [112/200], Step [43/63] Loss: 0.26765\n",
            "Epoch [112/200], Step [44/63] Loss: 0.93724\n",
            "Epoch [112/200], Step [45/63] Loss: 0.33758\n",
            "Epoch [112/200], Step [46/63] Loss: 0.42754\n",
            "Epoch [112/200], Step [47/63] Loss: 0.49880\n",
            "Epoch [112/200], Step [48/63] Loss: 0.38017\n",
            "Epoch [112/200], Step [49/63] Loss: 0.45343\n",
            "Epoch [112/200], Step [50/63] Loss: 0.20793\n",
            "Epoch [112/200], Step [51/63] Loss: 0.61411\n",
            "Epoch [112/200], Step [52/63] Loss: 0.36700\n",
            "Epoch [112/200], Step [53/63] Loss: 0.45695\n",
            "Epoch [112/200], Step [54/63] Loss: 0.36655\n",
            "Epoch [112/200], Step [55/63] Loss: 0.48209\n",
            "Epoch [112/200], Step [56/63] Loss: 0.53105\n",
            "Epoch [112/200], Step [57/63] Loss: 0.31139\n",
            "Epoch [112/200], Step [58/63] Loss: 0.23389\n",
            "Epoch [112/200], Step [59/63] Loss: 0.25664\n",
            "Epoch [112/200], Step [60/63] Loss: 0.57211\n",
            "Epoch [112/200], Step [61/63] Loss: 0.38416\n",
            "Epoch [112/200], Step [62/63] Loss: 0.56543\n",
            "Epoch [112/200], Step [63/63] Loss: 0.40444\n",
            "Valid Accuracy: 63.888888888888886 %\n",
            "Epoch [113/200], Step [1/63] Loss: 0.30588\n",
            "Epoch [113/200], Step [2/63] Loss: 0.34776\n",
            "Epoch [113/200], Step [3/63] Loss: 0.26030\n",
            "Epoch [113/200], Step [4/63] Loss: 0.57967\n",
            "Epoch [113/200], Step [5/63] Loss: 0.44899\n",
            "Epoch [113/200], Step [6/63] Loss: 0.44773\n",
            "Epoch [113/200], Step [7/63] Loss: 0.49776\n",
            "Epoch [113/200], Step [8/63] Loss: 0.47166\n",
            "Epoch [113/200], Step [9/63] Loss: 0.27425\n",
            "Epoch [113/200], Step [10/63] Loss: 0.31100\n",
            "Epoch [113/200], Step [11/63] Loss: 0.33245\n",
            "Epoch [113/200], Step [12/63] Loss: 0.37065\n",
            "Epoch [113/200], Step [13/63] Loss: 0.23710\n",
            "Epoch [113/200], Step [14/63] Loss: 0.25819\n",
            "Epoch [113/200], Step [15/63] Loss: 0.50484\n",
            "Epoch [113/200], Step [16/63] Loss: 0.32762\n",
            "Epoch [113/200], Step [17/63] Loss: 0.35587\n",
            "Epoch [113/200], Step [18/63] Loss: 0.24470\n",
            "Epoch [113/200], Step [19/63] Loss: 0.36738\n",
            "Epoch [113/200], Step [20/63] Loss: 0.29661\n",
            "Epoch [113/200], Step [21/63] Loss: 0.37668\n",
            "Epoch [113/200], Step [22/63] Loss: 0.32069\n",
            "Epoch [113/200], Step [23/63] Loss: 0.29960\n",
            "Epoch [113/200], Step [24/63] Loss: 0.57333\n",
            "Epoch [113/200], Step [25/63] Loss: 0.53988\n",
            "Epoch [113/200], Step [26/63] Loss: 0.46200\n",
            "Epoch [113/200], Step [27/63] Loss: 0.40135\n",
            "Epoch [113/200], Step [28/63] Loss: 0.38244\n",
            "Epoch [113/200], Step [29/63] Loss: 0.23527\n",
            "Epoch [113/200], Step [30/63] Loss: 0.37900\n",
            "Epoch [113/200], Step [31/63] Loss: 0.48389\n",
            "Epoch [113/200], Step [32/63] Loss: 0.29591\n",
            "Epoch [113/200], Step [33/63] Loss: 0.55627\n",
            "Epoch [113/200], Step [34/63] Loss: 0.28237\n",
            "Epoch [113/200], Step [35/63] Loss: 0.33340\n",
            "Epoch [113/200], Step [36/63] Loss: 0.33812\n",
            "Epoch [113/200], Step [37/63] Loss: 0.33180\n",
            "Epoch [113/200], Step [38/63] Loss: 0.30723\n",
            "Epoch [113/200], Step [39/63] Loss: 0.40931\n",
            "Epoch [113/200], Step [40/63] Loss: 0.22226\n",
            "Epoch [113/200], Step [41/63] Loss: 0.31052\n",
            "Epoch [113/200], Step [42/63] Loss: 0.29129\n",
            "Epoch [113/200], Step [43/63] Loss: 0.23820\n",
            "Epoch [113/200], Step [44/63] Loss: 0.23360\n",
            "Epoch [113/200], Step [45/63] Loss: 0.44143\n",
            "Epoch [113/200], Step [46/63] Loss: 0.37109\n",
            "Epoch [113/200], Step [47/63] Loss: 0.29820\n",
            "Epoch [113/200], Step [48/63] Loss: 0.29054\n",
            "Epoch [113/200], Step [49/63] Loss: 0.59473\n",
            "Epoch [113/200], Step [50/63] Loss: 0.34224\n",
            "Epoch [113/200], Step [51/63] Loss: 0.40964\n",
            "Epoch [113/200], Step [52/63] Loss: 0.38264\n",
            "Epoch [113/200], Step [53/63] Loss: 0.54956\n",
            "Epoch [113/200], Step [54/63] Loss: 0.28956\n",
            "Epoch [113/200], Step [55/63] Loss: 0.25600\n",
            "Epoch [113/200], Step [56/63] Loss: 0.36457\n",
            "Epoch [113/200], Step [57/63] Loss: 0.32507\n",
            "Epoch [113/200], Step [58/63] Loss: 0.46345\n",
            "Epoch [113/200], Step [59/63] Loss: 0.28538\n",
            "Epoch [113/200], Step [60/63] Loss: 0.20263\n",
            "Epoch [113/200], Step [61/63] Loss: 0.36170\n",
            "Epoch [113/200], Step [62/63] Loss: 0.48057\n",
            "Epoch [113/200], Step [63/63] Loss: 1.09448\n",
            "Valid Accuracy: 66.89814814814815 %\n",
            "Epoch [114/200], Step [1/63] Loss: 0.20155\n",
            "Epoch [114/200], Step [2/63] Loss: 0.30790\n",
            "Epoch [114/200], Step [3/63] Loss: 0.41613\n",
            "Epoch [114/200], Step [4/63] Loss: 0.19850\n",
            "Epoch [114/200], Step [5/63] Loss: 0.59702\n",
            "Epoch [114/200], Step [6/63] Loss: 0.29978\n",
            "Epoch [114/200], Step [7/63] Loss: 0.41401\n",
            "Epoch [114/200], Step [8/63] Loss: 0.50638\n",
            "Epoch [114/200], Step [9/63] Loss: 0.47202\n",
            "Epoch [114/200], Step [10/63] Loss: 0.56691\n",
            "Epoch [114/200], Step [11/63] Loss: 0.62685\n",
            "Epoch [114/200], Step [12/63] Loss: 0.39015\n",
            "Epoch [114/200], Step [13/63] Loss: 0.52054\n",
            "Epoch [114/200], Step [14/63] Loss: 0.36929\n",
            "Epoch [114/200], Step [15/63] Loss: 0.29690\n",
            "Epoch [114/200], Step [16/63] Loss: 0.16743\n",
            "Epoch [114/200], Step [17/63] Loss: 0.30448\n",
            "Epoch [114/200], Step [18/63] Loss: 0.76946\n",
            "Epoch [114/200], Step [19/63] Loss: 0.58175\n",
            "Epoch [114/200], Step [20/63] Loss: 0.33736\n",
            "Epoch [114/200], Step [21/63] Loss: 0.40253\n",
            "Epoch [114/200], Step [22/63] Loss: 0.36315\n",
            "Epoch [114/200], Step [23/63] Loss: 0.69496\n",
            "Epoch [114/200], Step [24/63] Loss: 0.27974\n",
            "Epoch [114/200], Step [25/63] Loss: 0.45992\n",
            "Epoch [114/200], Step [26/63] Loss: 0.63948\n",
            "Epoch [114/200], Step [27/63] Loss: 0.41913\n",
            "Epoch [114/200], Step [28/63] Loss: 0.34786\n",
            "Epoch [114/200], Step [29/63] Loss: 0.45536\n",
            "Epoch [114/200], Step [30/63] Loss: 0.32323\n",
            "Epoch [114/200], Step [31/63] Loss: 0.25580\n",
            "Epoch [114/200], Step [32/63] Loss: 0.36703\n",
            "Epoch [114/200], Step [33/63] Loss: 0.55862\n",
            "Epoch [114/200], Step [34/63] Loss: 0.16474\n",
            "Epoch [114/200], Step [35/63] Loss: 0.71766\n",
            "Epoch [114/200], Step [36/63] Loss: 0.50470\n",
            "Epoch [114/200], Step [37/63] Loss: 0.33814\n",
            "Epoch [114/200], Step [38/63] Loss: 0.64749\n",
            "Epoch [114/200], Step [39/63] Loss: 0.37579\n",
            "Epoch [114/200], Step [40/63] Loss: 0.42412\n",
            "Epoch [114/200], Step [41/63] Loss: 0.30038\n",
            "Epoch [114/200], Step [42/63] Loss: 0.29934\n",
            "Epoch [114/200], Step [43/63] Loss: 0.27568\n",
            "Epoch [114/200], Step [44/63] Loss: 0.53228\n",
            "Epoch [114/200], Step [45/63] Loss: 0.39007\n",
            "Epoch [114/200], Step [46/63] Loss: 0.35405\n",
            "Epoch [114/200], Step [47/63] Loss: 0.48986\n",
            "Epoch [114/200], Step [48/63] Loss: 0.29860\n",
            "Epoch [114/200], Step [49/63] Loss: 0.35476\n",
            "Epoch [114/200], Step [50/63] Loss: 0.73727\n",
            "Epoch [114/200], Step [51/63] Loss: 0.18738\n",
            "Epoch [114/200], Step [52/63] Loss: 0.34746\n",
            "Epoch [114/200], Step [53/63] Loss: 0.39075\n",
            "Epoch [114/200], Step [54/63] Loss: 0.52182\n",
            "Epoch [114/200], Step [55/63] Loss: 0.56041\n",
            "Epoch [114/200], Step [56/63] Loss: 0.33454\n",
            "Epoch [114/200], Step [57/63] Loss: 0.45497\n",
            "Epoch [114/200], Step [58/63] Loss: 0.36342\n",
            "Epoch [114/200], Step [59/63] Loss: 0.46478\n",
            "Epoch [114/200], Step [60/63] Loss: 0.40796\n",
            "Epoch [114/200], Step [61/63] Loss: 0.43748\n",
            "Epoch [114/200], Step [62/63] Loss: 0.26021\n",
            "Epoch [114/200], Step [63/63] Loss: 0.57568\n",
            "Valid Accuracy: 67.82407407407408 %\n",
            "Epoch [115/200], Step [1/63] Loss: 0.50457\n",
            "Epoch [115/200], Step [2/63] Loss: 0.39246\n",
            "Epoch [115/200], Step [3/63] Loss: 0.28198\n",
            "Epoch [115/200], Step [4/63] Loss: 0.25102\n",
            "Epoch [115/200], Step [5/63] Loss: 0.39271\n",
            "Epoch [115/200], Step [6/63] Loss: 0.53846\n",
            "Epoch [115/200], Step [7/63] Loss: 0.32856\n",
            "Epoch [115/200], Step [8/63] Loss: 0.60954\n",
            "Epoch [115/200], Step [9/63] Loss: 0.49704\n",
            "Epoch [115/200], Step [10/63] Loss: 0.22164\n",
            "Epoch [115/200], Step [11/63] Loss: 0.28684\n",
            "Epoch [115/200], Step [12/63] Loss: 0.19419\n",
            "Epoch [115/200], Step [13/63] Loss: 0.21918\n",
            "Epoch [115/200], Step [14/63] Loss: 0.27478\n",
            "Epoch [115/200], Step [15/63] Loss: 0.34925\n",
            "Epoch [115/200], Step [16/63] Loss: 0.37611\n",
            "Epoch [115/200], Step [17/63] Loss: 0.52826\n",
            "Epoch [115/200], Step [18/63] Loss: 0.41023\n",
            "Epoch [115/200], Step [19/63] Loss: 0.19566\n",
            "Epoch [115/200], Step [20/63] Loss: 0.14047\n",
            "Epoch [115/200], Step [21/63] Loss: 0.36990\n",
            "Epoch [115/200], Step [22/63] Loss: 0.48742\n",
            "Epoch [115/200], Step [23/63] Loss: 0.37200\n",
            "Epoch [115/200], Step [24/63] Loss: 0.36778\n",
            "Epoch [115/200], Step [25/63] Loss: 0.23158\n",
            "Epoch [115/200], Step [26/63] Loss: 0.16361\n",
            "Epoch [115/200], Step [27/63] Loss: 0.30897\n",
            "Epoch [115/200], Step [28/63] Loss: 0.31230\n",
            "Epoch [115/200], Step [29/63] Loss: 0.43426\n",
            "Epoch [115/200], Step [30/63] Loss: 0.47099\n",
            "Epoch [115/200], Step [31/63] Loss: 0.19261\n",
            "Epoch [115/200], Step [32/63] Loss: 0.17088\n",
            "Epoch [115/200], Step [33/63] Loss: 0.41851\n",
            "Epoch [115/200], Step [34/63] Loss: 0.28331\n",
            "Epoch [115/200], Step [35/63] Loss: 0.79023\n",
            "Epoch [115/200], Step [36/63] Loss: 0.31070\n",
            "Epoch [115/200], Step [37/63] Loss: 0.62302\n",
            "Epoch [115/200], Step [38/63] Loss: 0.34546\n",
            "Epoch [115/200], Step [39/63] Loss: 0.52986\n",
            "Epoch [115/200], Step [40/63] Loss: 0.30582\n",
            "Epoch [115/200], Step [41/63] Loss: 0.20163\n",
            "Epoch [115/200], Step [42/63] Loss: 0.29044\n",
            "Epoch [115/200], Step [43/63] Loss: 0.33128\n",
            "Epoch [115/200], Step [44/63] Loss: 0.33396\n",
            "Epoch [115/200], Step [45/63] Loss: 0.35181\n",
            "Epoch [115/200], Step [46/63] Loss: 0.19394\n",
            "Epoch [115/200], Step [47/63] Loss: 0.40244\n",
            "Epoch [115/200], Step [48/63] Loss: 0.19709\n",
            "Epoch [115/200], Step [49/63] Loss: 0.55666\n",
            "Epoch [115/200], Step [50/63] Loss: 0.22286\n",
            "Epoch [115/200], Step [51/63] Loss: 0.39622\n",
            "Epoch [115/200], Step [52/63] Loss: 0.28269\n",
            "Epoch [115/200], Step [53/63] Loss: 0.25518\n",
            "Epoch [115/200], Step [54/63] Loss: 0.39969\n",
            "Epoch [115/200], Step [55/63] Loss: 0.30318\n",
            "Epoch [115/200], Step [56/63] Loss: 0.32592\n",
            "Epoch [115/200], Step [57/63] Loss: 0.56050\n",
            "Epoch [115/200], Step [58/63] Loss: 0.34498\n",
            "Epoch [115/200], Step [59/63] Loss: 0.32858\n",
            "Epoch [115/200], Step [60/63] Loss: 0.48368\n",
            "Epoch [115/200], Step [61/63] Loss: 0.35984\n",
            "Epoch [115/200], Step [62/63] Loss: 0.32793\n",
            "Epoch [115/200], Step [63/63] Loss: 0.30399\n",
            "Valid Accuracy: 56.71296296296296 %\n",
            "Epoch [116/200], Step [1/63] Loss: 0.25398\n",
            "Epoch [116/200], Step [2/63] Loss: 0.41205\n",
            "Epoch [116/200], Step [3/63] Loss: 0.17681\n",
            "Epoch [116/200], Step [4/63] Loss: 0.24686\n",
            "Epoch [116/200], Step [5/63] Loss: 0.26546\n",
            "Epoch [116/200], Step [6/63] Loss: 0.35590\n",
            "Epoch [116/200], Step [7/63] Loss: 0.45031\n",
            "Epoch [116/200], Step [8/63] Loss: 0.24823\n",
            "Epoch [116/200], Step [9/63] Loss: 0.19886\n",
            "Epoch [116/200], Step [10/63] Loss: 0.18946\n",
            "Epoch [116/200], Step [11/63] Loss: 0.19361\n",
            "Epoch [116/200], Step [12/63] Loss: 0.39166\n",
            "Epoch [116/200], Step [13/63] Loss: 0.35699\n",
            "Epoch [116/200], Step [14/63] Loss: 0.24275\n",
            "Epoch [116/200], Step [15/63] Loss: 0.45395\n",
            "Epoch [116/200], Step [16/63] Loss: 0.39110\n",
            "Epoch [116/200], Step [17/63] Loss: 0.25899\n",
            "Epoch [116/200], Step [18/63] Loss: 0.62045\n",
            "Epoch [116/200], Step [19/63] Loss: 0.30116\n",
            "Epoch [116/200], Step [20/63] Loss: 0.23942\n",
            "Epoch [116/200], Step [21/63] Loss: 0.45251\n",
            "Epoch [116/200], Step [22/63] Loss: 0.39758\n",
            "Epoch [116/200], Step [23/63] Loss: 0.34272\n",
            "Epoch [116/200], Step [24/63] Loss: 0.23332\n",
            "Epoch [116/200], Step [25/63] Loss: 0.12748\n",
            "Epoch [116/200], Step [26/63] Loss: 0.30119\n",
            "Epoch [116/200], Step [27/63] Loss: 0.51352\n",
            "Epoch [116/200], Step [28/63] Loss: 0.43405\n",
            "Epoch [116/200], Step [29/63] Loss: 0.27902\n",
            "Epoch [116/200], Step [30/63] Loss: 0.29522\n",
            "Epoch [116/200], Step [31/63] Loss: 0.18228\n",
            "Epoch [116/200], Step [32/63] Loss: 0.18236\n",
            "Epoch [116/200], Step [33/63] Loss: 0.33523\n",
            "Epoch [116/200], Step [34/63] Loss: 0.29058\n",
            "Epoch [116/200], Step [35/63] Loss: 0.54882\n",
            "Epoch [116/200], Step [36/63] Loss: 0.07738\n",
            "Epoch [116/200], Step [37/63] Loss: 0.29043\n",
            "Epoch [116/200], Step [38/63] Loss: 0.28219\n",
            "Epoch [116/200], Step [39/63] Loss: 0.23804\n",
            "Epoch [116/200], Step [40/63] Loss: 0.31764\n",
            "Epoch [116/200], Step [41/63] Loss: 0.39709\n",
            "Epoch [116/200], Step [42/63] Loss: 0.21958\n",
            "Epoch [116/200], Step [43/63] Loss: 0.13354\n",
            "Epoch [116/200], Step [44/63] Loss: 0.21315\n",
            "Epoch [116/200], Step [45/63] Loss: 0.42062\n",
            "Epoch [116/200], Step [46/63] Loss: 0.57618\n",
            "Epoch [116/200], Step [47/63] Loss: 0.19030\n",
            "Epoch [116/200], Step [48/63] Loss: 0.32415\n",
            "Epoch [116/200], Step [49/63] Loss: 0.45481\n",
            "Epoch [116/200], Step [50/63] Loss: 0.39741\n",
            "Epoch [116/200], Step [51/63] Loss: 0.35815\n",
            "Epoch [116/200], Step [52/63] Loss: 0.41507\n",
            "Epoch [116/200], Step [53/63] Loss: 0.36228\n",
            "Epoch [116/200], Step [54/63] Loss: 0.31329\n",
            "Epoch [116/200], Step [55/63] Loss: 0.40796\n",
            "Epoch [116/200], Step [56/63] Loss: 0.29846\n",
            "Epoch [116/200], Step [57/63] Loss: 0.23954\n",
            "Epoch [116/200], Step [58/63] Loss: 0.28018\n",
            "Epoch [116/200], Step [59/63] Loss: 0.18566\n",
            "Epoch [116/200], Step [60/63] Loss: 0.24509\n",
            "Epoch [116/200], Step [61/63] Loss: 0.44811\n",
            "Epoch [116/200], Step [62/63] Loss: 0.32244\n",
            "Epoch [116/200], Step [63/63] Loss: 0.22427\n",
            "Valid Accuracy: 58.333333333333336 %\n",
            "Epoch [117/200], Step [1/63] Loss: 0.24102\n",
            "Epoch [117/200], Step [2/63] Loss: 0.28242\n",
            "Epoch [117/200], Step [3/63] Loss: 0.27483\n",
            "Epoch [117/200], Step [4/63] Loss: 0.10984\n",
            "Epoch [117/200], Step [5/63] Loss: 0.58923\n",
            "Epoch [117/200], Step [6/63] Loss: 0.10546\n",
            "Epoch [117/200], Step [7/63] Loss: 0.37030\n",
            "Epoch [117/200], Step [8/63] Loss: 0.20040\n",
            "Epoch [117/200], Step [9/63] Loss: 0.33482\n",
            "Epoch [117/200], Step [10/63] Loss: 0.27354\n",
            "Epoch [117/200], Step [11/63] Loss: 0.19072\n",
            "Epoch [117/200], Step [12/63] Loss: 0.36967\n",
            "Epoch [117/200], Step [13/63] Loss: 0.43146\n",
            "Epoch [117/200], Step [14/63] Loss: 0.47865\n",
            "Epoch [117/200], Step [15/63] Loss: 0.32126\n",
            "Epoch [117/200], Step [16/63] Loss: 0.19741\n",
            "Epoch [117/200], Step [17/63] Loss: 0.19456\n",
            "Epoch [117/200], Step [18/63] Loss: 0.24243\n",
            "Epoch [117/200], Step [19/63] Loss: 0.20281\n",
            "Epoch [117/200], Step [20/63] Loss: 0.19882\n",
            "Epoch [117/200], Step [21/63] Loss: 0.30469\n",
            "Epoch [117/200], Step [22/63] Loss: 0.30127\n",
            "Epoch [117/200], Step [23/63] Loss: 0.29385\n",
            "Epoch [117/200], Step [24/63] Loss: 0.33710\n",
            "Epoch [117/200], Step [25/63] Loss: 0.20903\n",
            "Epoch [117/200], Step [26/63] Loss: 0.23064\n",
            "Epoch [117/200], Step [27/63] Loss: 0.14162\n",
            "Epoch [117/200], Step [28/63] Loss: 0.36499\n",
            "Epoch [117/200], Step [29/63] Loss: 0.46536\n",
            "Epoch [117/200], Step [30/63] Loss: 0.34015\n",
            "Epoch [117/200], Step [31/63] Loss: 0.40954\n",
            "Epoch [117/200], Step [32/63] Loss: 0.28943\n",
            "Epoch [117/200], Step [33/63] Loss: 0.25444\n",
            "Epoch [117/200], Step [34/63] Loss: 0.37972\n",
            "Epoch [117/200], Step [35/63] Loss: 0.21003\n",
            "Epoch [117/200], Step [36/63] Loss: 0.22071\n",
            "Epoch [117/200], Step [37/63] Loss: 0.18008\n",
            "Epoch [117/200], Step [38/63] Loss: 0.42161\n",
            "Epoch [117/200], Step [39/63] Loss: 0.45008\n",
            "Epoch [117/200], Step [40/63] Loss: 0.16440\n",
            "Epoch [117/200], Step [41/63] Loss: 0.34834\n",
            "Epoch [117/200], Step [42/63] Loss: 0.14512\n",
            "Epoch [117/200], Step [43/63] Loss: 0.32378\n",
            "Epoch [117/200], Step [44/63] Loss: 0.72141\n",
            "Epoch [117/200], Step [45/63] Loss: 0.32098\n",
            "Epoch [117/200], Step [46/63] Loss: 0.23238\n",
            "Epoch [117/200], Step [47/63] Loss: 0.21322\n",
            "Epoch [117/200], Step [48/63] Loss: 0.29215\n",
            "Epoch [117/200], Step [49/63] Loss: 0.15813\n",
            "Epoch [117/200], Step [50/63] Loss: 0.25892\n",
            "Epoch [117/200], Step [51/63] Loss: 0.38226\n",
            "Epoch [117/200], Step [52/63] Loss: 0.25933\n",
            "Epoch [117/200], Step [53/63] Loss: 0.40164\n",
            "Epoch [117/200], Step [54/63] Loss: 0.45644\n",
            "Epoch [117/200], Step [55/63] Loss: 0.49505\n",
            "Epoch [117/200], Step [56/63] Loss: 0.37634\n",
            "Epoch [117/200], Step [57/63] Loss: 0.39278\n",
            "Epoch [117/200], Step [58/63] Loss: 0.39116\n",
            "Epoch [117/200], Step [59/63] Loss: 0.25967\n",
            "Epoch [117/200], Step [60/63] Loss: 0.41916\n",
            "Epoch [117/200], Step [61/63] Loss: 0.24917\n",
            "Epoch [117/200], Step [62/63] Loss: 0.32215\n",
            "Epoch [117/200], Step [63/63] Loss: 0.48314\n",
            "Valid Accuracy: 49.76851851851852 %\n",
            "Epoch [118/200], Step [1/63] Loss: 0.29152\n",
            "Epoch [118/200], Step [2/63] Loss: 0.34059\n",
            "Epoch [118/200], Step [3/63] Loss: 0.27610\n",
            "Epoch [118/200], Step [4/63] Loss: 0.19803\n",
            "Epoch [118/200], Step [5/63] Loss: 0.15172\n",
            "Epoch [118/200], Step [6/63] Loss: 0.32430\n",
            "Epoch [118/200], Step [7/63] Loss: 0.23661\n",
            "Epoch [118/200], Step [8/63] Loss: 0.11624\n",
            "Epoch [118/200], Step [9/63] Loss: 0.28427\n",
            "Epoch [118/200], Step [10/63] Loss: 0.17431\n",
            "Epoch [118/200], Step [11/63] Loss: 0.35158\n",
            "Epoch [118/200], Step [12/63] Loss: 0.35115\n",
            "Epoch [118/200], Step [13/63] Loss: 0.51661\n",
            "Epoch [118/200], Step [14/63] Loss: 0.10952\n",
            "Epoch [118/200], Step [15/63] Loss: 0.41620\n",
            "Epoch [118/200], Step [16/63] Loss: 0.13590\n",
            "Epoch [118/200], Step [17/63] Loss: 0.17278\n",
            "Epoch [118/200], Step [18/63] Loss: 0.17868\n",
            "Epoch [118/200], Step [19/63] Loss: 0.33287\n",
            "Epoch [118/200], Step [20/63] Loss: 0.17733\n",
            "Epoch [118/200], Step [21/63] Loss: 0.24057\n",
            "Epoch [118/200], Step [22/63] Loss: 0.18757\n",
            "Epoch [118/200], Step [23/63] Loss: 0.53789\n",
            "Epoch [118/200], Step [24/63] Loss: 0.17217\n",
            "Epoch [118/200], Step [25/63] Loss: 0.30844\n",
            "Epoch [118/200], Step [26/63] Loss: 0.47060\n",
            "Epoch [118/200], Step [27/63] Loss: 0.31237\n",
            "Epoch [118/200], Step [28/63] Loss: 0.19558\n",
            "Epoch [118/200], Step [29/63] Loss: 0.34231\n",
            "Epoch [118/200], Step [30/63] Loss: 0.24567\n",
            "Epoch [118/200], Step [31/63] Loss: 0.41538\n",
            "Epoch [118/200], Step [32/63] Loss: 0.22091\n",
            "Epoch [118/200], Step [33/63] Loss: 0.20167\n",
            "Epoch [118/200], Step [34/63] Loss: 0.31679\n",
            "Epoch [118/200], Step [35/63] Loss: 0.30535\n",
            "Epoch [118/200], Step [36/63] Loss: 0.24521\n",
            "Epoch [118/200], Step [37/63] Loss: 0.29285\n",
            "Epoch [118/200], Step [38/63] Loss: 0.18744\n",
            "Epoch [118/200], Step [39/63] Loss: 0.37822\n",
            "Epoch [118/200], Step [40/63] Loss: 0.29109\n",
            "Epoch [118/200], Step [41/63] Loss: 0.28403\n",
            "Epoch [118/200], Step [42/63] Loss: 0.19937\n",
            "Epoch [118/200], Step [43/63] Loss: 0.26377\n",
            "Epoch [118/200], Step [44/63] Loss: 0.13070\n",
            "Epoch [118/200], Step [45/63] Loss: 0.31763\n",
            "Epoch [118/200], Step [46/63] Loss: 0.20159\n",
            "Epoch [118/200], Step [47/63] Loss: 0.40722\n",
            "Epoch [118/200], Step [48/63] Loss: 0.13384\n",
            "Epoch [118/200], Step [49/63] Loss: 0.44726\n",
            "Epoch [118/200], Step [50/63] Loss: 0.18866\n",
            "Epoch [118/200], Step [51/63] Loss: 0.17007\n",
            "Epoch [118/200], Step [52/63] Loss: 0.39983\n",
            "Epoch [118/200], Step [53/63] Loss: 0.25069\n",
            "Epoch [118/200], Step [54/63] Loss: 0.41360\n",
            "Epoch [118/200], Step [55/63] Loss: 0.13529\n",
            "Epoch [118/200], Step [56/63] Loss: 0.24027\n",
            "Epoch [118/200], Step [57/63] Loss: 0.20291\n",
            "Epoch [118/200], Step [58/63] Loss: 0.58162\n",
            "Epoch [118/200], Step [59/63] Loss: 0.24658\n",
            "Epoch [118/200], Step [60/63] Loss: 0.16657\n",
            "Epoch [118/200], Step [61/63] Loss: 0.49234\n",
            "Epoch [118/200], Step [62/63] Loss: 0.12263\n",
            "Epoch [118/200], Step [63/63] Loss: 0.51111\n",
            "Valid Accuracy: 64.58333333333333 %\n",
            "Epoch [119/200], Step [1/63] Loss: 0.26195\n",
            "Epoch [119/200], Step [2/63] Loss: 0.27449\n",
            "Epoch [119/200], Step [3/63] Loss: 0.26795\n",
            "Epoch [119/200], Step [4/63] Loss: 0.18728\n",
            "Epoch [119/200], Step [5/63] Loss: 0.24267\n",
            "Epoch [119/200], Step [6/63] Loss: 0.27027\n",
            "Epoch [119/200], Step [7/63] Loss: 0.37211\n",
            "Epoch [119/200], Step [8/63] Loss: 0.19790\n",
            "Epoch [119/200], Step [9/63] Loss: 0.17390\n",
            "Epoch [119/200], Step [10/63] Loss: 0.31844\n",
            "Epoch [119/200], Step [11/63] Loss: 0.20008\n",
            "Epoch [119/200], Step [12/63] Loss: 0.21829\n",
            "Epoch [119/200], Step [13/63] Loss: 0.32231\n",
            "Epoch [119/200], Step [14/63] Loss: 0.23861\n",
            "Epoch [119/200], Step [15/63] Loss: 0.11835\n",
            "Epoch [119/200], Step [16/63] Loss: 0.17930\n",
            "Epoch [119/200], Step [17/63] Loss: 0.20646\n",
            "Epoch [119/200], Step [18/63] Loss: 0.18201\n",
            "Epoch [119/200], Step [19/63] Loss: 0.28580\n",
            "Epoch [119/200], Step [20/63] Loss: 0.21211\n",
            "Epoch [119/200], Step [21/63] Loss: 0.24969\n",
            "Epoch [119/200], Step [22/63] Loss: 0.74752\n",
            "Epoch [119/200], Step [23/63] Loss: 0.17303\n",
            "Epoch [119/200], Step [24/63] Loss: 0.23854\n",
            "Epoch [119/200], Step [25/63] Loss: 0.19308\n",
            "Epoch [119/200], Step [26/63] Loss: 0.22727\n",
            "Epoch [119/200], Step [27/63] Loss: 0.22525\n",
            "Epoch [119/200], Step [28/63] Loss: 0.33025\n",
            "Epoch [119/200], Step [29/63] Loss: 0.08816\n",
            "Epoch [119/200], Step [30/63] Loss: 0.21537\n",
            "Epoch [119/200], Step [31/63] Loss: 0.25908\n",
            "Epoch [119/200], Step [32/63] Loss: 0.23592\n",
            "Epoch [119/200], Step [33/63] Loss: 0.46226\n",
            "Epoch [119/200], Step [34/63] Loss: 0.28667\n",
            "Epoch [119/200], Step [35/63] Loss: 0.16909\n",
            "Epoch [119/200], Step [36/63] Loss: 0.20972\n",
            "Epoch [119/200], Step [37/63] Loss: 0.19908\n",
            "Epoch [119/200], Step [38/63] Loss: 0.17742\n",
            "Epoch [119/200], Step [39/63] Loss: 0.19814\n",
            "Epoch [119/200], Step [40/63] Loss: 0.27809\n",
            "Epoch [119/200], Step [41/63] Loss: 0.16365\n",
            "Epoch [119/200], Step [42/63] Loss: 0.41457\n",
            "Epoch [119/200], Step [43/63] Loss: 0.52002\n",
            "Epoch [119/200], Step [44/63] Loss: 0.23884\n",
            "Epoch [119/200], Step [45/63] Loss: 0.12253\n",
            "Epoch [119/200], Step [46/63] Loss: 0.20864\n",
            "Epoch [119/200], Step [47/63] Loss: 0.28976\n",
            "Epoch [119/200], Step [48/63] Loss: 0.14212\n",
            "Epoch [119/200], Step [49/63] Loss: 0.24702\n",
            "Epoch [119/200], Step [50/63] Loss: 0.38544\n",
            "Epoch [119/200], Step [51/63] Loss: 0.16294\n",
            "Epoch [119/200], Step [52/63] Loss: 0.25511\n",
            "Epoch [119/200], Step [53/63] Loss: 0.24348\n",
            "Epoch [119/200], Step [54/63] Loss: 0.23109\n",
            "Epoch [119/200], Step [55/63] Loss: 0.45546\n",
            "Epoch [119/200], Step [56/63] Loss: 0.19373\n",
            "Epoch [119/200], Step [57/63] Loss: 0.19967\n",
            "Epoch [119/200], Step [58/63] Loss: 0.36016\n",
            "Epoch [119/200], Step [59/63] Loss: 0.26943\n",
            "Epoch [119/200], Step [60/63] Loss: 0.24678\n",
            "Epoch [119/200], Step [61/63] Loss: 0.33484\n",
            "Epoch [119/200], Step [62/63] Loss: 0.20667\n",
            "Epoch [119/200], Step [63/63] Loss: 0.37614\n",
            "Valid Accuracy: 65.04629629629629 %\n",
            "Epoch [120/200], Step [1/63] Loss: 0.26220\n",
            "Epoch [120/200], Step [2/63] Loss: 0.23270\n",
            "Epoch [120/200], Step [3/63] Loss: 0.62474\n",
            "Epoch [120/200], Step [4/63] Loss: 0.11954\n",
            "Epoch [120/200], Step [5/63] Loss: 0.31051\n",
            "Epoch [120/200], Step [6/63] Loss: 0.15193\n",
            "Epoch [120/200], Step [7/63] Loss: 0.61356\n",
            "Epoch [120/200], Step [8/63] Loss: 0.25944\n",
            "Epoch [120/200], Step [9/63] Loss: 0.20168\n",
            "Epoch [120/200], Step [10/63] Loss: 0.32423\n",
            "Epoch [120/200], Step [11/63] Loss: 0.38921\n",
            "Epoch [120/200], Step [12/63] Loss: 0.26672\n",
            "Epoch [120/200], Step [13/63] Loss: 0.31103\n",
            "Epoch [120/200], Step [14/63] Loss: 0.11344\n",
            "Epoch [120/200], Step [15/63] Loss: 0.19853\n",
            "Epoch [120/200], Step [16/63] Loss: 0.31135\n",
            "Epoch [120/200], Step [17/63] Loss: 0.19653\n",
            "Epoch [120/200], Step [18/63] Loss: 0.18910\n",
            "Epoch [120/200], Step [19/63] Loss: 0.43367\n",
            "Epoch [120/200], Step [20/63] Loss: 0.35539\n",
            "Epoch [120/200], Step [21/63] Loss: 0.29914\n",
            "Epoch [120/200], Step [22/63] Loss: 0.10421\n",
            "Epoch [120/200], Step [23/63] Loss: 0.25324\n",
            "Epoch [120/200], Step [24/63] Loss: 0.20502\n",
            "Epoch [120/200], Step [25/63] Loss: 0.12532\n",
            "Epoch [120/200], Step [26/63] Loss: 0.33082\n",
            "Epoch [120/200], Step [27/63] Loss: 0.32167\n",
            "Epoch [120/200], Step [28/63] Loss: 0.20657\n",
            "Epoch [120/200], Step [29/63] Loss: 0.24078\n",
            "Epoch [120/200], Step [30/63] Loss: 0.21203\n",
            "Epoch [120/200], Step [31/63] Loss: 0.22789\n",
            "Epoch [120/200], Step [32/63] Loss: 0.11431\n",
            "Epoch [120/200], Step [33/63] Loss: 0.21430\n",
            "Epoch [120/200], Step [34/63] Loss: 0.22301\n",
            "Epoch [120/200], Step [35/63] Loss: 0.34600\n",
            "Epoch [120/200], Step [36/63] Loss: 0.36147\n",
            "Epoch [120/200], Step [37/63] Loss: 0.32446\n",
            "Epoch [120/200], Step [38/63] Loss: 0.36361\n",
            "Epoch [120/200], Step [39/63] Loss: 0.21663\n",
            "Epoch [120/200], Step [40/63] Loss: 0.25055\n",
            "Epoch [120/200], Step [41/63] Loss: 0.24238\n",
            "Epoch [120/200], Step [42/63] Loss: 0.15523\n",
            "Epoch [120/200], Step [43/63] Loss: 0.31464\n",
            "Epoch [120/200], Step [44/63] Loss: 0.35229\n",
            "Epoch [120/200], Step [45/63] Loss: 0.70484\n",
            "Epoch [120/200], Step [46/63] Loss: 0.17803\n",
            "Epoch [120/200], Step [47/63] Loss: 0.29956\n",
            "Epoch [120/200], Step [48/63] Loss: 0.15001\n",
            "Epoch [120/200], Step [49/63] Loss: 0.30223\n",
            "Epoch [120/200], Step [50/63] Loss: 0.22572\n",
            "Epoch [120/200], Step [51/63] Loss: 0.24064\n",
            "Epoch [120/200], Step [52/63] Loss: 0.26740\n",
            "Epoch [120/200], Step [53/63] Loss: 0.27563\n",
            "Epoch [120/200], Step [54/63] Loss: 0.20000\n",
            "Epoch [120/200], Step [55/63] Loss: 0.28581\n",
            "Epoch [120/200], Step [56/63] Loss: 0.17555\n",
            "Epoch [120/200], Step [57/63] Loss: 0.36499\n",
            "Epoch [120/200], Step [58/63] Loss: 0.24825\n",
            "Epoch [120/200], Step [59/63] Loss: 0.10219\n",
            "Epoch [120/200], Step [60/63] Loss: 0.21510\n",
            "Epoch [120/200], Step [61/63] Loss: 0.44443\n",
            "Epoch [120/200], Step [62/63] Loss: 0.18184\n",
            "Epoch [120/200], Step [63/63] Loss: 0.36347\n",
            "Valid Accuracy: 67.82407407407408 %\n",
            "Epoch [121/200], Step [1/63] Loss: 0.27519\n",
            "Epoch [121/200], Step [2/63] Loss: 0.12895\n",
            "Epoch [121/200], Step [3/63] Loss: 0.27632\n",
            "Epoch [121/200], Step [4/63] Loss: 0.21686\n",
            "Epoch [121/200], Step [5/63] Loss: 0.18297\n",
            "Epoch [121/200], Step [6/63] Loss: 0.10189\n",
            "Epoch [121/200], Step [7/63] Loss: 0.15671\n",
            "Epoch [121/200], Step [8/63] Loss: 0.19277\n",
            "Epoch [121/200], Step [9/63] Loss: 0.29868\n",
            "Epoch [121/200], Step [10/63] Loss: 0.12301\n",
            "Epoch [121/200], Step [11/63] Loss: 0.15964\n",
            "Epoch [121/200], Step [12/63] Loss: 0.44522\n",
            "Epoch [121/200], Step [13/63] Loss: 0.18829\n",
            "Epoch [121/200], Step [14/63] Loss: 0.30913\n",
            "Epoch [121/200], Step [15/63] Loss: 0.62025\n",
            "Epoch [121/200], Step [16/63] Loss: 0.09827\n",
            "Epoch [121/200], Step [17/63] Loss: 0.15621\n",
            "Epoch [121/200], Step [18/63] Loss: 0.11101\n",
            "Epoch [121/200], Step [19/63] Loss: 0.29216\n",
            "Epoch [121/200], Step [20/63] Loss: 0.25764\n",
            "Epoch [121/200], Step [21/63] Loss: 0.21876\n",
            "Epoch [121/200], Step [22/63] Loss: 0.06917\n",
            "Epoch [121/200], Step [23/63] Loss: 0.21634\n",
            "Epoch [121/200], Step [24/63] Loss: 0.47190\n",
            "Epoch [121/200], Step [25/63] Loss: 0.50304\n",
            "Epoch [121/200], Step [26/63] Loss: 0.58762\n",
            "Epoch [121/200], Step [27/63] Loss: 0.18677\n",
            "Epoch [121/200], Step [28/63] Loss: 0.16220\n",
            "Epoch [121/200], Step [29/63] Loss: 0.11493\n",
            "Epoch [121/200], Step [30/63] Loss: 0.47198\n",
            "Epoch [121/200], Step [31/63] Loss: 0.14898\n",
            "Epoch [121/200], Step [32/63] Loss: 0.43848\n",
            "Epoch [121/200], Step [33/63] Loss: 0.45610\n",
            "Epoch [121/200], Step [34/63] Loss: 0.22467\n",
            "Epoch [121/200], Step [35/63] Loss: 0.35149\n",
            "Epoch [121/200], Step [36/63] Loss: 0.23655\n",
            "Epoch [121/200], Step [37/63] Loss: 0.39642\n",
            "Epoch [121/200], Step [38/63] Loss: 0.46607\n",
            "Epoch [121/200], Step [39/63] Loss: 0.39717\n",
            "Epoch [121/200], Step [40/63] Loss: 0.36299\n",
            "Epoch [121/200], Step [41/63] Loss: 0.40061\n",
            "Epoch [121/200], Step [42/63] Loss: 0.30827\n",
            "Epoch [121/200], Step [43/63] Loss: 0.27773\n",
            "Epoch [121/200], Step [44/63] Loss: 0.30645\n",
            "Epoch [121/200], Step [45/63] Loss: 0.44718\n",
            "Epoch [121/200], Step [46/63] Loss: 0.29525\n",
            "Epoch [121/200], Step [47/63] Loss: 0.29980\n",
            "Epoch [121/200], Step [48/63] Loss: 0.30831\n",
            "Epoch [121/200], Step [49/63] Loss: 0.58331\n",
            "Epoch [121/200], Step [50/63] Loss: 0.39543\n",
            "Epoch [121/200], Step [51/63] Loss: 0.24872\n",
            "Epoch [121/200], Step [52/63] Loss: 0.28613\n",
            "Epoch [121/200], Step [53/63] Loss: 0.42364\n",
            "Epoch [121/200], Step [54/63] Loss: 0.40931\n",
            "Epoch [121/200], Step [55/63] Loss: 0.22174\n",
            "Epoch [121/200], Step [56/63] Loss: 0.46136\n",
            "Epoch [121/200], Step [57/63] Loss: 0.28118\n",
            "Epoch [121/200], Step [58/63] Loss: 0.23944\n",
            "Epoch [121/200], Step [59/63] Loss: 0.16468\n",
            "Epoch [121/200], Step [60/63] Loss: 0.37760\n",
            "Epoch [121/200], Step [61/63] Loss: 0.41587\n",
            "Epoch [121/200], Step [62/63] Loss: 0.38172\n",
            "Epoch [121/200], Step [63/63] Loss: 0.19668\n",
            "Valid Accuracy: 60.879629629629626 %\n",
            "Epoch [122/200], Step [1/63] Loss: 0.17793\n",
            "Epoch [122/200], Step [2/63] Loss: 0.26351\n",
            "Epoch [122/200], Step [3/63] Loss: 0.35031\n",
            "Epoch [122/200], Step [4/63] Loss: 0.11880\n",
            "Epoch [122/200], Step [5/63] Loss: 0.19271\n",
            "Epoch [122/200], Step [6/63] Loss: 0.52509\n",
            "Epoch [122/200], Step [7/63] Loss: 0.18007\n",
            "Epoch [122/200], Step [8/63] Loss: 0.23708\n",
            "Epoch [122/200], Step [9/63] Loss: 0.56147\n",
            "Epoch [122/200], Step [10/63] Loss: 0.29889\n",
            "Epoch [122/200], Step [11/63] Loss: 0.23555\n",
            "Epoch [122/200], Step [12/63] Loss: 0.22460\n",
            "Epoch [122/200], Step [13/63] Loss: 0.20892\n",
            "Epoch [122/200], Step [14/63] Loss: 0.23943\n",
            "Epoch [122/200], Step [15/63] Loss: 0.21610\n",
            "Epoch [122/200], Step [16/63] Loss: 0.29433\n",
            "Epoch [122/200], Step [17/63] Loss: 0.31016\n",
            "Epoch [122/200], Step [18/63] Loss: 0.35400\n",
            "Epoch [122/200], Step [19/63] Loss: 0.40242\n",
            "Epoch [122/200], Step [20/63] Loss: 0.25008\n",
            "Epoch [122/200], Step [21/63] Loss: 0.19575\n",
            "Epoch [122/200], Step [22/63] Loss: 0.18787\n",
            "Epoch [122/200], Step [23/63] Loss: 0.13159\n",
            "Epoch [122/200], Step [24/63] Loss: 0.19806\n",
            "Epoch [122/200], Step [25/63] Loss: 0.21315\n",
            "Epoch [122/200], Step [26/63] Loss: 0.28200\n",
            "Epoch [122/200], Step [27/63] Loss: 0.16943\n",
            "Epoch [122/200], Step [28/63] Loss: 0.25829\n",
            "Epoch [122/200], Step [29/63] Loss: 0.15427\n",
            "Epoch [122/200], Step [30/63] Loss: 0.17155\n",
            "Epoch [122/200], Step [31/63] Loss: 0.29859\n",
            "Epoch [122/200], Step [32/63] Loss: 0.21298\n",
            "Epoch [122/200], Step [33/63] Loss: 0.22535\n",
            "Epoch [122/200], Step [34/63] Loss: 0.24127\n",
            "Epoch [122/200], Step [35/63] Loss: 0.17881\n",
            "Epoch [122/200], Step [36/63] Loss: 0.18999\n",
            "Epoch [122/200], Step [37/63] Loss: 0.12795\n",
            "Epoch [122/200], Step [38/63] Loss: 0.20991\n",
            "Epoch [122/200], Step [39/63] Loss: 0.30595\n",
            "Epoch [122/200], Step [40/63] Loss: 0.36689\n",
            "Epoch [122/200], Step [41/63] Loss: 0.32559\n",
            "Epoch [122/200], Step [42/63] Loss: 0.10803\n",
            "Epoch [122/200], Step [43/63] Loss: 0.29863\n",
            "Epoch [122/200], Step [44/63] Loss: 0.40577\n",
            "Epoch [122/200], Step [45/63] Loss: 0.16880\n",
            "Epoch [122/200], Step [46/63] Loss: 0.20994\n",
            "Epoch [122/200], Step [47/63] Loss: 0.25485\n",
            "Epoch [122/200], Step [48/63] Loss: 0.08100\n",
            "Epoch [122/200], Step [49/63] Loss: 0.15089\n",
            "Epoch [122/200], Step [50/63] Loss: 0.18815\n",
            "Epoch [122/200], Step [51/63] Loss: 0.08987\n",
            "Epoch [122/200], Step [52/63] Loss: 0.23598\n",
            "Epoch [122/200], Step [53/63] Loss: 0.27861\n",
            "Epoch [122/200], Step [54/63] Loss: 0.30089\n",
            "Epoch [122/200], Step [55/63] Loss: 0.22054\n",
            "Epoch [122/200], Step [56/63] Loss: 0.25610\n",
            "Epoch [122/200], Step [57/63] Loss: 0.21405\n",
            "Epoch [122/200], Step [58/63] Loss: 0.16412\n",
            "Epoch [122/200], Step [59/63] Loss: 0.25944\n",
            "Epoch [122/200], Step [60/63] Loss: 0.26748\n",
            "Epoch [122/200], Step [61/63] Loss: 0.22740\n",
            "Epoch [122/200], Step [62/63] Loss: 0.12139\n",
            "Epoch [122/200], Step [63/63] Loss: 0.37778\n",
            "Valid Accuracy: 61.574074074074076 %\n",
            "Epoch [123/200], Step [1/63] Loss: 0.12787\n",
            "Epoch [123/200], Step [2/63] Loss: 0.17548\n",
            "Epoch [123/200], Step [3/63] Loss: 0.06002\n",
            "Epoch [123/200], Step [4/63] Loss: 0.11841\n",
            "Epoch [123/200], Step [5/63] Loss: 0.17968\n",
            "Epoch [123/200], Step [6/63] Loss: 0.27524\n",
            "Epoch [123/200], Step [7/63] Loss: 0.10069\n",
            "Epoch [123/200], Step [8/63] Loss: 0.25749\n",
            "Epoch [123/200], Step [9/63] Loss: 0.13594\n",
            "Epoch [123/200], Step [10/63] Loss: 0.34060\n",
            "Epoch [123/200], Step [11/63] Loss: 0.42400\n",
            "Epoch [123/200], Step [12/63] Loss: 0.27925\n",
            "Epoch [123/200], Step [13/63] Loss: 0.11403\n",
            "Epoch [123/200], Step [14/63] Loss: 0.14670\n",
            "Epoch [123/200], Step [15/63] Loss: 0.12651\n",
            "Epoch [123/200], Step [16/63] Loss: 0.17893\n",
            "Epoch [123/200], Step [17/63] Loss: 0.23380\n",
            "Epoch [123/200], Step [18/63] Loss: 0.18041\n",
            "Epoch [123/200], Step [19/63] Loss: 0.23043\n",
            "Epoch [123/200], Step [20/63] Loss: 0.19860\n",
            "Epoch [123/200], Step [21/63] Loss: 0.07421\n",
            "Epoch [123/200], Step [22/63] Loss: 0.12208\n",
            "Epoch [123/200], Step [23/63] Loss: 0.34328\n",
            "Epoch [123/200], Step [24/63] Loss: 0.19755\n",
            "Epoch [123/200], Step [25/63] Loss: 0.16194\n",
            "Epoch [123/200], Step [26/63] Loss: 0.22657\n",
            "Epoch [123/200], Step [27/63] Loss: 0.32452\n",
            "Epoch [123/200], Step [28/63] Loss: 0.12751\n",
            "Epoch [123/200], Step [29/63] Loss: 0.12144\n",
            "Epoch [123/200], Step [30/63] Loss: 0.12364\n",
            "Epoch [123/200], Step [31/63] Loss: 0.08154\n",
            "Epoch [123/200], Step [32/63] Loss: 0.14699\n",
            "Epoch [123/200], Step [33/63] Loss: 0.17835\n",
            "Epoch [123/200], Step [34/63] Loss: 0.18959\n",
            "Epoch [123/200], Step [35/63] Loss: 0.09840\n",
            "Epoch [123/200], Step [36/63] Loss: 0.17066\n",
            "Epoch [123/200], Step [37/63] Loss: 0.12719\n",
            "Epoch [123/200], Step [38/63] Loss: 0.31702\n",
            "Epoch [123/200], Step [39/63] Loss: 0.11796\n",
            "Epoch [123/200], Step [40/63] Loss: 0.14177\n",
            "Epoch [123/200], Step [41/63] Loss: 0.12532\n",
            "Epoch [123/200], Step [42/63] Loss: 0.30954\n",
            "Epoch [123/200], Step [43/63] Loss: 0.06717\n",
            "Epoch [123/200], Step [44/63] Loss: 0.12285\n",
            "Epoch [123/200], Step [45/63] Loss: 0.12087\n",
            "Epoch [123/200], Step [46/63] Loss: 0.13206\n",
            "Epoch [123/200], Step [47/63] Loss: 0.49255\n",
            "Epoch [123/200], Step [48/63] Loss: 0.25013\n",
            "Epoch [123/200], Step [49/63] Loss: 0.16062\n",
            "Epoch [123/200], Step [50/63] Loss: 0.17486\n",
            "Epoch [123/200], Step [51/63] Loss: 0.29818\n",
            "Epoch [123/200], Step [52/63] Loss: 0.18703\n",
            "Epoch [123/200], Step [53/63] Loss: 0.26582\n",
            "Epoch [123/200], Step [54/63] Loss: 0.17437\n",
            "Epoch [123/200], Step [55/63] Loss: 0.16929\n",
            "Epoch [123/200], Step [56/63] Loss: 0.15525\n",
            "Epoch [123/200], Step [57/63] Loss: 0.38081\n",
            "Epoch [123/200], Step [58/63] Loss: 0.20443\n",
            "Epoch [123/200], Step [59/63] Loss: 0.14959\n",
            "Epoch [123/200], Step [60/63] Loss: 0.19606\n",
            "Epoch [123/200], Step [61/63] Loss: 0.62471\n",
            "Epoch [123/200], Step [62/63] Loss: 0.24697\n",
            "Epoch [123/200], Step [63/63] Loss: 0.20123\n",
            "Valid Accuracy: 65.97222222222223 %\n",
            "Epoch [124/200], Step [1/63] Loss: 0.08722\n",
            "Epoch [124/200], Step [2/63] Loss: 0.12978\n",
            "Epoch [124/200], Step [3/63] Loss: 0.16469\n",
            "Epoch [124/200], Step [4/63] Loss: 0.18822\n",
            "Epoch [124/200], Step [5/63] Loss: 0.20528\n",
            "Epoch [124/200], Step [6/63] Loss: 0.32799\n",
            "Epoch [124/200], Step [7/63] Loss: 0.16495\n",
            "Epoch [124/200], Step [8/63] Loss: 0.12088\n",
            "Epoch [124/200], Step [9/63] Loss: 0.17933\n",
            "Epoch [124/200], Step [10/63] Loss: 0.11748\n",
            "Epoch [124/200], Step [11/63] Loss: 0.11824\n",
            "Epoch [124/200], Step [12/63] Loss: 0.21080\n",
            "Epoch [124/200], Step [13/63] Loss: 0.34631\n",
            "Epoch [124/200], Step [14/63] Loss: 0.07520\n",
            "Epoch [124/200], Step [15/63] Loss: 0.15986\n",
            "Epoch [124/200], Step [16/63] Loss: 0.16484\n",
            "Epoch [124/200], Step [17/63] Loss: 0.22484\n",
            "Epoch [124/200], Step [18/63] Loss: 0.11322\n",
            "Epoch [124/200], Step [19/63] Loss: 0.13663\n",
            "Epoch [124/200], Step [20/63] Loss: 0.14666\n",
            "Epoch [124/200], Step [21/63] Loss: 0.11020\n",
            "Epoch [124/200], Step [22/63] Loss: 0.11201\n",
            "Epoch [124/200], Step [23/63] Loss: 0.20105\n",
            "Epoch [124/200], Step [24/63] Loss: 0.09303\n",
            "Epoch [124/200], Step [25/63] Loss: 0.14353\n",
            "Epoch [124/200], Step [26/63] Loss: 0.09253\n",
            "Epoch [124/200], Step [27/63] Loss: 0.18717\n",
            "Epoch [124/200], Step [28/63] Loss: 0.11704\n",
            "Epoch [124/200], Step [29/63] Loss: 0.15100\n",
            "Epoch [124/200], Step [30/63] Loss: 0.14902\n",
            "Epoch [124/200], Step [31/63] Loss: 0.24824\n",
            "Epoch [124/200], Step [32/63] Loss: 0.31676\n",
            "Epoch [124/200], Step [33/63] Loss: 0.13368\n",
            "Epoch [124/200], Step [34/63] Loss: 0.04071\n",
            "Epoch [124/200], Step [35/63] Loss: 0.05467\n",
            "Epoch [124/200], Step [36/63] Loss: 0.13999\n",
            "Epoch [124/200], Step [37/63] Loss: 0.10411\n",
            "Epoch [124/200], Step [38/63] Loss: 0.43454\n",
            "Epoch [124/200], Step [39/63] Loss: 0.22564\n",
            "Epoch [124/200], Step [40/63] Loss: 0.19366\n",
            "Epoch [124/200], Step [41/63] Loss: 0.12915\n",
            "Epoch [124/200], Step [42/63] Loss: 0.12715\n",
            "Epoch [124/200], Step [43/63] Loss: 0.42751\n",
            "Epoch [124/200], Step [44/63] Loss: 0.30398\n",
            "Epoch [124/200], Step [45/63] Loss: 0.32841\n",
            "Epoch [124/200], Step [46/63] Loss: 0.09797\n",
            "Epoch [124/200], Step [47/63] Loss: 0.48014\n",
            "Epoch [124/200], Step [48/63] Loss: 0.35678\n",
            "Epoch [124/200], Step [49/63] Loss: 0.28422\n",
            "Epoch [124/200], Step [50/63] Loss: 0.21289\n",
            "Epoch [124/200], Step [51/63] Loss: 0.23990\n",
            "Epoch [124/200], Step [52/63] Loss: 0.17259\n",
            "Epoch [124/200], Step [53/63] Loss: 0.26200\n",
            "Epoch [124/200], Step [54/63] Loss: 0.13336\n",
            "Epoch [124/200], Step [55/63] Loss: 0.12133\n",
            "Epoch [124/200], Step [56/63] Loss: 0.19550\n",
            "Epoch [124/200], Step [57/63] Loss: 0.18892\n",
            "Epoch [124/200], Step [58/63] Loss: 0.41776\n",
            "Epoch [124/200], Step [59/63] Loss: 0.30700\n",
            "Epoch [124/200], Step [60/63] Loss: 0.18162\n",
            "Epoch [124/200], Step [61/63] Loss: 0.12186\n",
            "Epoch [124/200], Step [62/63] Loss: 0.27404\n",
            "Epoch [124/200], Step [63/63] Loss: 0.31394\n",
            "Valid Accuracy: 65.04629629629629 %\n",
            "Epoch [125/200], Step [1/63] Loss: 0.23255\n",
            "Epoch [125/200], Step [2/63] Loss: 0.19014\n",
            "Epoch [125/200], Step [3/63] Loss: 0.13974\n",
            "Epoch [125/200], Step [4/63] Loss: 0.13298\n",
            "Epoch [125/200], Step [5/63] Loss: 0.13103\n",
            "Epoch [125/200], Step [6/63] Loss: 0.15273\n",
            "Epoch [125/200], Step [7/63] Loss: 0.20484\n",
            "Epoch [125/200], Step [8/63] Loss: 0.12958\n",
            "Epoch [125/200], Step [9/63] Loss: 0.20485\n",
            "Epoch [125/200], Step [10/63] Loss: 0.13242\n",
            "Epoch [125/200], Step [11/63] Loss: 0.15372\n",
            "Epoch [125/200], Step [12/63] Loss: 0.19546\n",
            "Epoch [125/200], Step [13/63] Loss: 0.27867\n",
            "Epoch [125/200], Step [14/63] Loss: 0.17981\n",
            "Epoch [125/200], Step [15/63] Loss: 0.44129\n",
            "Epoch [125/200], Step [16/63] Loss: 0.23951\n",
            "Epoch [125/200], Step [17/63] Loss: 0.18270\n",
            "Epoch [125/200], Step [18/63] Loss: 0.25706\n",
            "Epoch [125/200], Step [19/63] Loss: 0.23497\n",
            "Epoch [125/200], Step [20/63] Loss: 0.15037\n",
            "Epoch [125/200], Step [21/63] Loss: 0.27269\n",
            "Epoch [125/200], Step [22/63] Loss: 0.22947\n",
            "Epoch [125/200], Step [23/63] Loss: 0.14178\n",
            "Epoch [125/200], Step [24/63] Loss: 0.29944\n",
            "Epoch [125/200], Step [25/63] Loss: 0.27494\n",
            "Epoch [125/200], Step [26/63] Loss: 0.15164\n",
            "Epoch [125/200], Step [27/63] Loss: 0.22794\n",
            "Epoch [125/200], Step [28/63] Loss: 0.13967\n",
            "Epoch [125/200], Step [29/63] Loss: 0.17913\n",
            "Epoch [125/200], Step [30/63] Loss: 0.30254\n",
            "Epoch [125/200], Step [31/63] Loss: 0.12318\n",
            "Epoch [125/200], Step [32/63] Loss: 0.11025\n",
            "Epoch [125/200], Step [33/63] Loss: 0.15400\n",
            "Epoch [125/200], Step [34/63] Loss: 0.24620\n",
            "Epoch [125/200], Step [35/63] Loss: 0.35182\n",
            "Epoch [125/200], Step [36/63] Loss: 0.13614\n",
            "Epoch [125/200], Step [37/63] Loss: 0.24288\n",
            "Epoch [125/200], Step [38/63] Loss: 0.13536\n",
            "Epoch [125/200], Step [39/63] Loss: 0.09405\n",
            "Epoch [125/200], Step [40/63] Loss: 0.23393\n",
            "Epoch [125/200], Step [41/63] Loss: 0.12199\n",
            "Epoch [125/200], Step [42/63] Loss: 0.10098\n",
            "Epoch [125/200], Step [43/63] Loss: 0.23197\n",
            "Epoch [125/200], Step [44/63] Loss: 0.23116\n",
            "Epoch [125/200], Step [45/63] Loss: 0.19910\n",
            "Epoch [125/200], Step [46/63] Loss: 0.43023\n",
            "Epoch [125/200], Step [47/63] Loss: 0.20331\n",
            "Epoch [125/200], Step [48/63] Loss: 0.21044\n",
            "Epoch [125/200], Step [49/63] Loss: 0.13411\n",
            "Epoch [125/200], Step [50/63] Loss: 0.14446\n",
            "Epoch [125/200], Step [51/63] Loss: 0.34303\n",
            "Epoch [125/200], Step [52/63] Loss: 0.23768\n",
            "Epoch [125/200], Step [53/63] Loss: 0.24958\n",
            "Epoch [125/200], Step [54/63] Loss: 0.12102\n",
            "Epoch [125/200], Step [55/63] Loss: 0.24811\n",
            "Epoch [125/200], Step [56/63] Loss: 0.38349\n",
            "Epoch [125/200], Step [57/63] Loss: 0.19739\n",
            "Epoch [125/200], Step [58/63] Loss: 0.32869\n",
            "Epoch [125/200], Step [59/63] Loss: 0.51601\n",
            "Epoch [125/200], Step [60/63] Loss: 0.10345\n",
            "Epoch [125/200], Step [61/63] Loss: 0.21570\n",
            "Epoch [125/200], Step [62/63] Loss: 0.13493\n",
            "Epoch [125/200], Step [63/63] Loss: 0.10281\n",
            "Valid Accuracy: 62.5 %\n",
            "Epoch [126/200], Step [1/63] Loss: 0.19904\n",
            "Epoch [126/200], Step [2/63] Loss: 0.32099\n",
            "Epoch [126/200], Step [3/63] Loss: 0.29228\n",
            "Epoch [126/200], Step [4/63] Loss: 0.34404\n",
            "Epoch [126/200], Step [5/63] Loss: 0.45337\n",
            "Epoch [126/200], Step [6/63] Loss: 0.09137\n",
            "Epoch [126/200], Step [7/63] Loss: 0.30219\n",
            "Epoch [126/200], Step [8/63] Loss: 0.18638\n",
            "Epoch [126/200], Step [9/63] Loss: 0.16254\n",
            "Epoch [126/200], Step [10/63] Loss: 0.29751\n",
            "Epoch [126/200], Step [11/63] Loss: 0.13094\n",
            "Epoch [126/200], Step [12/63] Loss: 0.14590\n",
            "Epoch [126/200], Step [13/63] Loss: 0.20734\n",
            "Epoch [126/200], Step [14/63] Loss: 0.17995\n",
            "Epoch [126/200], Step [15/63] Loss: 0.24284\n",
            "Epoch [126/200], Step [16/63] Loss: 0.20676\n",
            "Epoch [126/200], Step [17/63] Loss: 0.21862\n",
            "Epoch [126/200], Step [18/63] Loss: 0.14416\n",
            "Epoch [126/200], Step [19/63] Loss: 0.15696\n",
            "Epoch [126/200], Step [20/63] Loss: 0.16309\n",
            "Epoch [126/200], Step [21/63] Loss: 0.20552\n",
            "Epoch [126/200], Step [22/63] Loss: 0.15502\n",
            "Epoch [126/200], Step [23/63] Loss: 0.17663\n",
            "Epoch [126/200], Step [24/63] Loss: 0.38912\n",
            "Epoch [126/200], Step [25/63] Loss: 0.14926\n",
            "Epoch [126/200], Step [26/63] Loss: 0.15707\n",
            "Epoch [126/200], Step [27/63] Loss: 0.13159\n",
            "Epoch [126/200], Step [28/63] Loss: 0.18407\n",
            "Epoch [126/200], Step [29/63] Loss: 0.25917\n",
            "Epoch [126/200], Step [30/63] Loss: 0.44530\n",
            "Epoch [126/200], Step [31/63] Loss: 0.34586\n",
            "Epoch [126/200], Step [32/63] Loss: 0.18468\n",
            "Epoch [126/200], Step [33/63] Loss: 0.22701\n",
            "Epoch [126/200], Step [34/63] Loss: 0.14909\n",
            "Epoch [126/200], Step [35/63] Loss: 0.46525\n",
            "Epoch [126/200], Step [36/63] Loss: 0.15776\n",
            "Epoch [126/200], Step [37/63] Loss: 0.38678\n",
            "Epoch [126/200], Step [38/63] Loss: 0.31126\n",
            "Epoch [126/200], Step [39/63] Loss: 0.41404\n",
            "Epoch [126/200], Step [40/63] Loss: 0.29077\n",
            "Epoch [126/200], Step [41/63] Loss: 0.13641\n",
            "Epoch [126/200], Step [42/63] Loss: 0.11464\n",
            "Epoch [126/200], Step [43/63] Loss: 0.37554\n",
            "Epoch [126/200], Step [44/63] Loss: 0.22749\n",
            "Epoch [126/200], Step [45/63] Loss: 0.10895\n",
            "Epoch [126/200], Step [46/63] Loss: 0.25737\n",
            "Epoch [126/200], Step [47/63] Loss: 0.12823\n",
            "Epoch [126/200], Step [48/63] Loss: 0.20176\n",
            "Epoch [126/200], Step [49/63] Loss: 0.35127\n",
            "Epoch [126/200], Step [50/63] Loss: 0.25418\n",
            "Epoch [126/200], Step [51/63] Loss: 0.24275\n",
            "Epoch [126/200], Step [52/63] Loss: 0.14165\n",
            "Epoch [126/200], Step [53/63] Loss: 0.19268\n",
            "Epoch [126/200], Step [54/63] Loss: 0.38197\n",
            "Epoch [126/200], Step [55/63] Loss: 0.14501\n",
            "Epoch [126/200], Step [56/63] Loss: 0.11141\n",
            "Epoch [126/200], Step [57/63] Loss: 0.08295\n",
            "Epoch [126/200], Step [58/63] Loss: 0.23737\n",
            "Epoch [126/200], Step [59/63] Loss: 0.21709\n",
            "Epoch [126/200], Step [60/63] Loss: 0.33009\n",
            "Epoch [126/200], Step [61/63] Loss: 0.45452\n",
            "Epoch [126/200], Step [62/63] Loss: 0.20817\n",
            "Epoch [126/200], Step [63/63] Loss: 0.12588\n",
            "Valid Accuracy: 65.74074074074075 %\n",
            "Epoch [127/200], Step [1/63] Loss: 0.15863\n",
            "Epoch [127/200], Step [2/63] Loss: 0.19649\n",
            "Epoch [127/200], Step [3/63] Loss: 0.33544\n",
            "Epoch [127/200], Step [4/63] Loss: 0.13386\n",
            "Epoch [127/200], Step [5/63] Loss: 0.21431\n",
            "Epoch [127/200], Step [6/63] Loss: 0.14942\n",
            "Epoch [127/200], Step [7/63] Loss: 0.24683\n",
            "Epoch [127/200], Step [8/63] Loss: 0.11687\n",
            "Epoch [127/200], Step [9/63] Loss: 0.07047\n",
            "Epoch [127/200], Step [10/63] Loss: 0.19870\n",
            "Epoch [127/200], Step [11/63] Loss: 0.15599\n",
            "Epoch [127/200], Step [12/63] Loss: 0.26011\n",
            "Epoch [127/200], Step [13/63] Loss: 0.06234\n",
            "Epoch [127/200], Step [14/63] Loss: 0.21556\n",
            "Epoch [127/200], Step [15/63] Loss: 0.14523\n",
            "Epoch [127/200], Step [16/63] Loss: 0.27971\n",
            "Epoch [127/200], Step [17/63] Loss: 0.18802\n",
            "Epoch [127/200], Step [18/63] Loss: 0.14378\n",
            "Epoch [127/200], Step [19/63] Loss: 0.12523\n",
            "Epoch [127/200], Step [20/63] Loss: 0.15411\n",
            "Epoch [127/200], Step [21/63] Loss: 0.19106\n",
            "Epoch [127/200], Step [22/63] Loss: 0.24120\n",
            "Epoch [127/200], Step [23/63] Loss: 0.19004\n",
            "Epoch [127/200], Step [24/63] Loss: 0.11656\n",
            "Epoch [127/200], Step [25/63] Loss: 0.22188\n",
            "Epoch [127/200], Step [26/63] Loss: 0.10863\n",
            "Epoch [127/200], Step [27/63] Loss: 0.26335\n",
            "Epoch [127/200], Step [28/63] Loss: 0.14634\n",
            "Epoch [127/200], Step [29/63] Loss: 0.08021\n",
            "Epoch [127/200], Step [30/63] Loss: 0.13472\n",
            "Epoch [127/200], Step [31/63] Loss: 0.15880\n",
            "Epoch [127/200], Step [32/63] Loss: 0.06746\n",
            "Epoch [127/200], Step [33/63] Loss: 0.27275\n",
            "Epoch [127/200], Step [34/63] Loss: 0.21005\n",
            "Epoch [127/200], Step [35/63] Loss: 0.07266\n",
            "Epoch [127/200], Step [36/63] Loss: 0.16524\n",
            "Epoch [127/200], Step [37/63] Loss: 0.16458\n",
            "Epoch [127/200], Step [38/63] Loss: 0.42142\n",
            "Epoch [127/200], Step [39/63] Loss: 0.18309\n",
            "Epoch [127/200], Step [40/63] Loss: 0.09505\n",
            "Epoch [127/200], Step [41/63] Loss: 0.08856\n",
            "Epoch [127/200], Step [42/63] Loss: 0.24902\n",
            "Epoch [127/200], Step [43/63] Loss: 0.32176\n",
            "Epoch [127/200], Step [44/63] Loss: 0.31987\n",
            "Epoch [127/200], Step [45/63] Loss: 0.10127\n",
            "Epoch [127/200], Step [46/63] Loss: 0.21537\n",
            "Epoch [127/200], Step [47/63] Loss: 0.04985\n",
            "Epoch [127/200], Step [48/63] Loss: 0.15166\n",
            "Epoch [127/200], Step [49/63] Loss: 0.11814\n",
            "Epoch [127/200], Step [50/63] Loss: 0.11950\n",
            "Epoch [127/200], Step [51/63] Loss: 0.18978\n",
            "Epoch [127/200], Step [52/63] Loss: 0.19212\n",
            "Epoch [127/200], Step [53/63] Loss: 0.32319\n",
            "Epoch [127/200], Step [54/63] Loss: 0.11586\n",
            "Epoch [127/200], Step [55/63] Loss: 0.05879\n",
            "Epoch [127/200], Step [56/63] Loss: 0.18886\n",
            "Epoch [127/200], Step [57/63] Loss: 0.28886\n",
            "Epoch [127/200], Step [58/63] Loss: 0.14823\n",
            "Epoch [127/200], Step [59/63] Loss: 0.23272\n",
            "Epoch [127/200], Step [60/63] Loss: 0.19335\n",
            "Epoch [127/200], Step [61/63] Loss: 0.09223\n",
            "Epoch [127/200], Step [62/63] Loss: 0.56557\n",
            "Epoch [127/200], Step [63/63] Loss: 0.07312\n",
            "Valid Accuracy: 62.03703703703704 %\n",
            "Epoch [128/200], Step [1/63] Loss: 0.20948\n",
            "Epoch [128/200], Step [2/63] Loss: 0.29088\n",
            "Epoch [128/200], Step [3/63] Loss: 0.18260\n",
            "Epoch [128/200], Step [4/63] Loss: 0.12047\n",
            "Epoch [128/200], Step [5/63] Loss: 0.23909\n",
            "Epoch [128/200], Step [6/63] Loss: 0.07624\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "##################################################################################################################################################################\n",
        "##################################################################################################################################################################\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=6e-4)\n",
        "p=[]\n",
        "l=[]\n",
        "total_step = len(train_loader)\n",
        "scheduler=torch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr=learning_rate,steps_per_epoch=total_step, epochs=num_epochs)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "print(\"> Training\")\n",
        "start = time.time()  # time generation\n",
        "for epoch in range(num_epochs):\n",
        "    br = None\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):  # load a batch\n",
        "        #print(images)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        #################### We go forward and calculate the weights\n",
        "        with autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "          ## autocast serve as context managers that allow regions of your script to run in mixed precision.\n",
        "          ## In these regions, CUDA ops run in a dtype chosen by autocast to improve performance while maintaining accuracy.\n",
        "            #print(images.shape)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        ###################### We now go back and optimise the values\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        br = loss.item()\n",
        "        #if (i + 1) % 100 == 0:\n",
        "        if total_step == 63: l.append(loss.item())\n",
        "        print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.5f}\".format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "        scheduler.step()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        p.append(100 * correct / total)\n",
        "        print('Valid Accuracy: {} %'.format(100 * correct / total))\n",
        "end = time.time()\n",
        "elapsed = end - start\n",
        "print(\"Training took \" + str(elapsed) + \" secs or \" + str(elapsed / 60) + \" mins in total\")\n",
        "###############################################################################################################################################################\n",
        "# Test the model\n",
        "print(\"> Testing\")\n",
        "start = time.time() #time generation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))\n",
        "end = time.time()\n",
        "elapsed = end - start\n",
        "print(\"Testing took \" + str(elapsed) + \" secs or \" + str(elapsed/60) + \" mins in total\")\n",
        "\n",
        "print('END')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(p)\n",
        "plt.show()\n",
        "print(len(p))"
      ],
      "metadata": {
        "id": "Ymq3sk3ZaAyA"
      },
      "id": "Ymq3sk3ZaAyA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e3f5463b",
      "metadata": {
        "id": "e3f5463b"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12803aea",
      "metadata": {
        "id": "12803aea"
      },
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "# X.\n",
        "# X_train = X_train[:, :, :, np.newaxis]\n",
        "# X_test = X_test[:, :, :, np.newaxis]\n",
        "# print(\"X_train shape:\", X_train.shape)\n",
        "# X_train = tf.cast(X_train, tf.float32)\n",
        "# X_test = tf.cast(X_test, tf.float32)\n",
        "# #categorical one hot\n",
        "# y_train_cat = to_categorical(y_train, categories)\n",
        "# y_test_cat = to_categorical(y_test, categories)\n",
        "# print(X_train.shape)\n",
        "# print(X_test.shape)\n",
        "# print(y_train_cat.shape)\n",
        "# print(y_test_cat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2a7b08",
      "metadata": {
        "id": "ce2a7b08"
      },
      "outputs": [],
      "source": [
        "# model.fit(X_train, y_train_cat,\n",
        "#                 epochs=epochs,\n",
        "#                 batch_size=batch_size,\n",
        "#                 shuffle=True,\n",
        "#                 validation_data=(X_test, y_test_cat))\n",
        "\n",
        "# # classify the digits using the trained model\n",
        "# # note that we take them from the *test* set\n",
        "# predictions = model.predict(X_test)\n",
        "\n",
        "# #reverse one-hot encoding\n",
        "# predictions = np.argmax(predictions, axis=1)\n",
        "# ########################################################################################\n",
        "# correct = predictions==y_test.numpy()\n",
        "# total_test = len(X_test_transformed)\n",
        "# #print(\"Gnd Truth:\", y_test)\n",
        "# print(\"Total Testing\", total_test)\n",
        "# print(\"Predictions\", predictions)\n",
        "# print(\"Which Correct:\",correct)\n",
        "# print(\"Total Correct:\",np.sum(correct))\n",
        "# print(\"Accuracy:\",np.sum(correct)/total_test)\n",
        "# print(classification_report(y_test, predictions, target_names=target_names))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}